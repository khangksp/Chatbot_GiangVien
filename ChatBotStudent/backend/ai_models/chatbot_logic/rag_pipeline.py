import time
import re
import random
import logging
logger = logging.getLogger(__name__)

import jwt # Th√™m import n√†y

# Import c√°c module v·ª´a t√°ch
from .retriever import ChatbotAI
from .reranker import SemanticReRanker
from .decision_engine import PureSemanticDecisionEngine

# Import c√°c service kh√°c
from ..phobert_service import retriever_service # S·ª≠ d·ª•ng .. ƒë·ªÉ ƒëi ra kh·ªèi th∆∞ m·ª•c chatbot_logic
from ..interaction_logger_service import interaction_logger
from qa_management.services import drive_service          # <--- ƒê∆∞·ªùng d·∫´n ƒë√∫ng cho drive_service
from ..external_api_service import external_api_service # <--- ƒê∆∞·ªùng d·∫´n n√†y v·∫´n ƒë√∫ng cho external_api_service

class PureSemanticChatbotAI:
    def __init__(self, shared_response_generator):
        print("--- CHECKPOINT 2: PureSemanticChatbotAI __init__ started ---")
        from ..phobert_service import retriever_service        
        self.sbert_retriever = ChatbotAI(shared_response_generator=shared_response_generator)
        self.retriever_service = retriever_service
        self.response_generator = shared_response_generator
        self.decision_engine = PureSemanticDecisionEngine()        
        self.semantic_reranker = SemanticReRanker(retriever_service=self.retriever_service)        
        self.conversation_memory = {}        
        logger.info("üéØ ENHANCED PureSemanticChatbotAI initialized")
        logger.info("   üõ°Ô∏è Smart penalty systm√¨nh enabled")
        logger.info("   üß† Confidence-aware decision making")
        logger.info("   üéØ High-quality answer preservation")
        logger.info("   üî¨ Top-5 smart candidate selection")
    
    def _check_direct_entity_query(self, query: str, session_id: str):
        """üîß IMPROVED: Better detection of entity queries"""
        session_memory = self.get_conversation_context(session_id)
        if not session_memory or len(session_memory) == 0:
            return False, None, None

        query_lower = query.lower().strip()
        
        # üÜï EXPANDED: More patterns to catch entity questions
        direct_patterns = [
            r'\b(v·∫≠y|th·∫ø)\s+([A-Z√Ä-·ª∏][a-z√†-·ªπ]+(?:\s+[A-Z√Ä-·ª∏][a-z√†-·ªπ]+)*)\s+l√†\s+(ai|g√¨)\b',  # "v·∫≠y X l√† ai"
            r'\b(v·∫≠y|th·∫ø)\s+(th·∫ßy|c√¥|√¥ng|b√†|anh|ch·ªã)\s+([A-Z√Ä-·ª∏][a-z√†-·ªπ]+(?:\s+[A-Z√Ä-·ª∏][a-z√†-·ªπ]+)*)\b',  # "v·∫≠y th·∫ßy X"
            r'\b(c√≤n|v√†)\s+([A-Z√Ä-·ª∏][a-z√†-·ªπ]+(?:\s+[A-Z√Ä-·ª∏][a-z√†-·ªπ]+)*)\s+(th√¨ sao|nh∆∞ th·∫ø n√†o|l√† ai)\b',  # "c√≤n X th√¨ sao"
            r'\b([A-Z√Ä-·ª∏][a-z√†-·ªπ]+(?:\s+[A-Z√Ä-·ª∏][a-z√†-·ªπ]+)*)\s+l√†\s+(ai|g√¨)\b',  # "X l√† ai"
            r'\b(?:√¥ng|b√†|th·∫ßy|c√¥|anh|ch·ªã)\s+([A-Z√Ä-·ª∏][a-z√†-·ªπ]+(?:\s+[A-Z√Ä-·ª∏][a-z√†-·ªπ]+)*)\s*$'  # "√¥ng X", "b√† Y"
        ]
        
        # Traditional direct references
        direct_pronouns = ['√¥ng ·∫•y', 'b√† ·∫•y', 'ng∆∞·ªùi ƒë√≥', 'th·∫ßy ·∫•y', 'c√¥ ·∫•y', 'anh ·∫•y', 'ch·ªã ·∫•y']
        has_direct_pronoun = any(pronoun in query_lower for pronoun in direct_pronouns)
        
        # Check for name patterns
        extracted_name = None
        has_direct_pattern = False
        
        for pattern in direct_patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                has_direct_pattern = True
                # Extract name from different groups based on pattern
                groups = match.groups()
                for group in groups:
                    if group and len(group.split()) >= 1 and group[0].isupper():
                        # Handle single name or full name
                        if len(group.split()) == 1:
                            # Single word - check if it could be part of a longer name
                            extracted_name = group
                        else:
                            # Multiple words - likely full name
                            extracted_name = group
                        break
                if extracted_name:
                    break
        
        # ONLY proceed if there's a clear direct reference
        if not (has_direct_pronoun or has_direct_pattern):
            return False, None, None

        # Check last 3 interactions instead of just 1
        recent_interactions = session_memory[-3:] if len(session_memory) >= 3 else session_memory
        
        for interaction in reversed(recent_interactions):
            last_entities_info = interaction.get('semantic_info', {}).get('extracted_entities', {})
            last_person_entities = last_entities_info.get('person_name', [])
            
            if not last_person_entities:
                continue
            
            # If we extracted a specific name, try to match it
            if extracted_name:
                for entity in last_person_entities:
                    if self._names_match_flexible(extracted_name, entity):
                        logger.info(f"üéØ Direct entity match: '{extracted_name}' ‚Üí '{entity}'")
                        return True, entity, interaction
            
            # For pronouns, use the most recent entity
            if has_direct_pronoun:
                main_entity = last_person_entities[0]
                logger.info(f"üéØ Direct pronoun reference: '{main_entity}'")
                return True, main_entity, interaction
        
        return False, None, None

    def _names_match_flexible(self, name1: str, name2: str) -> bool:
        """üÜï NEW: Flexible name matching"""
        if not name1 or not name2:
            return False
        
        # Normalize both names
        norm1 = name1.lower().strip()
        norm2 = name2.lower().strip()
        
        # Remove Vietnamese particles
        particles = ['d·∫°', '·∫°', '∆°i', 'nh√©', 'v·∫≠y', 'th√¨', 'l√†', 'c·ªßa', 'v√†', 'v·ªõi']
        for particle in particles:
            norm1 = norm1.replace(particle, ' ')
            norm2 = norm2.replace(particle, ' ')
        
        # Clean up spaces
        norm1 = ' '.join(norm1.split())
        norm2 = ' '.join(norm2.split())
        
        # Direct match
        if norm1 == norm2:
            return True
        
        # Word-level matching
        words1 = set(norm1.split())
        words2 = set(norm2.split())
        
        # If both have multiple words, check overlap
        if len(words1) >= 2 and len(words2) >= 2:
            overlap = len(words1.intersection(words2))
            total_words = min(len(words1), len(words2))  # Use smaller set as denominator
            overlap_ratio = overlap / total_words if total_words > 0 else 0
            
            # 60% overlap is good enough for names
            return overlap_ratio >= 0.6
        
        # Single word vs multi-word (e.g., "c∆∞·ªùng" vs "l√™ vƒÉn c∆∞·ªùng")
        if len(words1) == 1 and len(words2) >= 2:
            single_word = list(words1)[0]
            return single_word in words2 and len(single_word) > 2
        elif len(words2) == 1 and len(words1) >= 2:
            single_word = list(words2)[0]
            return single_word in words1 and len(single_word) > 2
        
        # Single word matching with partial
        if len(words1) == 1 and len(words2) == 1:
            word1, word2 = list(words1)[0], list(words2)[0]
            if len(word1) >= 3 and len(word2) >= 3:
                # Check if one contains the other
                return word1 in word2 or word2 in word1
        
        return False

    def _create_response_from_memory(self, query, entity_name, last_interaction, session_id):
        """Create response using information from memory"""
        last_response = last_interaction.get('response', '')
        last_query = last_interaction.get('query', '')
        
        logger.info(f"üìù Creating response from memory about: '{entity_name}'")
        logger.info(f"   Previous query: '{last_query}'")
        
        # Use generator to create natural response
        context = {
            'response': last_response,
            'entity_name': entity_name,
            'previous_query': last_query,
            'memory_based': True
        }
        
        response = self.response_generator.generate_response(
            query=query,
            context=context,
            intent_info=None,
            entities={'person_name': [entity_name]},
            session_id=session_id
        )
        
        response_text = response.get('response', '') if response else ''
        
        if not response_text or len(response_text.strip()) < 10:
            # Fallback to simple response
            response_text = f"Ch√†o b·∫°n, v·ªÅ {entity_name}, nh∆∞ m√¨nh ƒë√£ th·∫£o lu·∫≠n tr∆∞·ªõc ƒë√≥: {last_response[:200]}..."
        
        return {
            'response': response_text,
            'confidence': 0.85,
            'method': 'memory_direct_hit',
            'decision_type': 'memory_context',
            'entity_referenced': entity_name,
            'processing_time': 0.1,
            'context_aware_rag': True
        }

    def _smart_entity_fallback_search(self, query, session_id):
        """Smart fallback for entity queries when main search fails"""
        if not session_id:
            return []
        
        session_memory = self.get_conversation_context(session_id)
        if not session_memory:
            return []
        
        # Get recent entities from memory
        recent_entities = []
        for interaction in reversed(session_memory[-5:]):  # Check last 5 interactions
            entities_info = interaction.get('semantic_info', {}).get('extracted_entities', {})
            if 'person_name' in entities_info:
                recent_entities.extend(entities_info['person_name'])
        
        if not recent_entities:
            return []
        
        # Build enhanced query with entity context
        recent_entities = list(set(recent_entities))[:3]  # Top 3 unique entities
        entity_context = " ".join(recent_entities)
        enhanced_query = f"{query} {entity_context}"
        
        logger.info(f"üîç Smart fallback: Enhanced query with entities: {recent_entities}")
        
        # Search with enhanced query
        candidates = self.sbert_retriever.semantic_search_top_k(
            enhanced_query,
            top_k=self.semantic_reranker.config['stage1_top_k']
        )
        
        return candidates

    def _calculate_semantic_relevance(self, query, document_text):
        """Calculate semantic relevance between query and document"""
        try:
            from sentence_transformers import util
            
            # Get embeddings
            query_embedding = self.sbert_retriever.model.encode(query, convert_to_tensor=True)
            doc_embedding = self.sbert_retriever.model.encode(document_text[:500], convert_to_tensor=True)
            
            # Calculate cosine similarity
            similarity = util.cos_sim(query_embedding, doc_embedding).item()
            
            return similarity
            
        except Exception as e:
            logger.error(f"‚ùå Error calculating semantic relevance: {str(e)}")
            return 0.5  # Default to medium relevance
    
    def process_query(self, query, session_id=None, jwt_token=None, document_text=None):
        start_time = time.time()
        logger.info(f"üéØ IMPROVED Context-Aware Semantic RAG Processing: '{query}' (session: {session_id})")
        
        # ‚úÖ FIX #1: LOAD PROFILE FROM JWT_TOKEN
        student_profile = None
        if jwt_token:
            try:
                logger.info(f"üîë Loading student profile from JWT token...")
                profile_result = external_api_service.get_student_profile(jwt_token)
                if profile_result:
                    student_profile = {
                        "full_name": getattr(profile_result, "full_name", ""),
                        "mssv": getattr(profile_result, "mssv", ""),
                        "class_name": getattr(profile_result, "class_name", ""),
                        "faculty": getattr(profile_result, "faculty", ""),
                        "major": getattr(profile_result, "major", ""),
                        "email": getattr(profile_result, "email", ""),
                    }
                    logger.info(f"‚úÖ Profile loaded: {student_profile.get('full_name')} ({student_profile.get('mssv')}), l·ªõp {student_profile.get('class_name')}")
                    
                    # ‚úÖ SET PROFILE INTO GENERATOR CONTEXT
                    if self.response_generator and hasattr(self.response_generator, '_user_context_cache'):
                        if session_id:
                            self.response_generator._user_context_cache[session_id] = student_profile
                            logger.info(f"‚úÖ Profile set into generator context for session: {session_id}")
                else:
                    logger.warning("‚ö†Ô∏è get_student_profile returned None")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Could not load student profile from JWT: {e}")
        else:
            logger.debug("‚ÑπÔ∏è No JWT token provided, skipping profile load")
        
        try:
            normalized_query = query.lower().strip()
        
            ACKNOWLEDGEMENT_PHRASES = [
                'ƒë√∫ng r·ªìi', 'ch√≠nh x√°c', 'ch√≠nh x√°c r·ªìi ƒë√≥', 'c·∫≠u n√≥i ƒë√∫ng r·ªìi', 'c·∫≠u n√≥i ƒë√∫ng',
                'ok b·∫°n', 'c·∫£m ∆°n c·∫≠u', 'c·∫£m ∆°n', 'c·∫£m ∆°n b·∫°n', 'c·∫£m ∆°n nh√©', 'c·∫£m ∆°n nha',
                'oke', 'okela', 'okee', 'ok', 'ukm', '·ª´m', 'uhm', 'uh', '·ª´'
            ]
            if normalized_query in ACKNOWLEDGEMENT_PHRASES:
                ACKNOWLEDGEMENT_RESPONSES = [
                    "D·∫°, m√¨nh ƒë√¢y. C·∫≠u c·∫ßn m√¨nh h·ªó tr·ª£ th√™m g√¨ kh√¥ng?", "Okie, n·∫øu c·∫≠u c·∫ßn g√¨ c·ª© h·ªèi nh√©!",
                    "Kh√¥ng c√≥ g√¨ ƒë√¢u c·∫≠u. C·∫ßn g√¨ c·ª© n√≥i m√¨nh nha.", "M√¨nh hi·ªÉu r·ªìi. C·∫≠u c√≥ c√¢u h·ªèi n√†o kh√°c kh√¥ng?",
                    "R·∫•t vui v√¨ ƒë√£ gi√∫p ƒë∆∞·ª£c c·∫≠u!"
                ]
                response_text = random.choice(ACKNOWLEDGEMENT_RESPONSES)
                logger.info(f"üí¨ Conversational filter triggered for '{query}'. Responding naturally.")
                
                return {
                    'response': response_text, 'confidence': 0.98, 'method': 'conversational_filter',
                    'decision_type': 'acknowledgement', 'processing_time': time.time() - start_time, 'context_aware_rag': True,
                }
            
            query = self._clean_query(query)
            if not query:
                return self._get_empty_query_response()

            session_memory = self.get_conversation_context(session_id) if session_id else []
            logger.info(f"üß† Session memory: {len(session_memory)} interactions")

            is_document_context_active = False
            final_document_text = document_text # ∆Øu ti√™n t√†i li·ªáu m·ªõi t·∫£i l√™n

            if not final_document_text and session_memory:
                previous_document = None
                # T√¨m ki·∫øm ng∆∞·ª£c t·ª´ cu·ªëi l·ªãch s·ª≠ h·ªôi tho·∫°i
                for interaction in reversed(session_memory):
                    doc_in_memory = interaction.get('document_text')
                    if doc_in_memory and doc_in_memory.strip():
                        # T√¨m th·∫•y t√†i li·ªáu g·∫ßn nh·∫•t, l·∫•y n√≥ v√† d·ª´ng t√¨m ki·∫øm
                        previous_document = doc_in_memory
                        break
                
                if previous_document:
                    # T·∫ßng 1: T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a (logic n√†y gi·ªØ nguy√™n)
                    relevance_score = self._calculate_semantic_relevance(query, previous_document)
                    logger.info(f"üìä Semantic relevance to previous document: {relevance_score:.2f}")

                    # T·∫ßng 2: Ra quy·∫øt ƒë·ªãnh cho c√°c tr∆∞·ªùng h·ª£p r√µ r√†ng
                    if relevance_score > 0.7: # Ng∆∞·ª°ng R·∫•t Cao
                        logger.info("‚úîÔ∏è Relevance check: PASSED (High similarity). Reusing document.")
                        final_document_text = previous_document
                    elif relevance_score < 0.4: # Ng∆∞·ª°ng R·∫•t Th·∫•p
                        logger.info("‚ùå Relevance check: FAILED (Low similarity). Topic shift.")
                        final_document_text = None
                    else:
                        # T·∫ßng 3: Tr∆∞·ªùng h·ª£p nh·∫≠p nh·∫±ng -> Hi·ªán t·∫°i, ƒë·ªÉ ƒë∆°n gi·∫£n, ch√∫ng ta s·∫Ω coi l√† li√™n quan
                        # (Trong t∆∞∆°ng lai, c√≥ th·ªÉ th√™m 1 l·ªánh g·ªçi LLM ·ªü ƒë√¢y ƒë·ªÉ x√°c nh·∫≠n)
                        logger.info("‚ö†Ô∏è Relevance check: AMBIGUOUS. Assuming continuation for better user experience.")
                        final_document_text = previous_document
            
            if final_document_text and final_document_text.strip():
                is_document_context_active = True

            if is_document_context_active:
                logger.info(f"üìÑ Document context is ACTIVE with {len(final_document_text.strip())} chars. Prioritizing document processing.")
                decision_type, context, should_respond = self.decision_engine.make_decision(
                    query, [], session_memory, jwt_token, final_document_text
                )
                
                # ‚úÖ ADD PROFILE TO CONTEXT
                if student_profile:
                    context['profile'] = student_profile
                
                if should_respond and decision_type == 'use_document_context':
                    response_text = self._execute_fixed_semantic_decision(
                        decision_type, query, context, session_id
                    )
                    final_score = context.get('confidence', 0.95)
                    
                    if session_id:
                        self._update_semantic_memory(
                            session_id, query, final_score, decision_type, 
                            True, context, final_document_text
                        )

                    return {
                        'response': response_text, 'confidence': final_score, 'method': 'document_context_priority',
                        'decision_type': decision_type, 'semantic_info': context, 'sources': [],
                        'processing_time': time.time() - start_time, 'document_context_used': True,
                        'document_context_priority': True, 'context_aware_rag': True,
                    }
            
            logger.info("üìö No active document context. Proceeding with standard RAG pipeline.")
            
            is_direct_hit, entity_name, last_interaction = self._check_direct_entity_query(query, session_id)
            if is_direct_hit:
                response_data = self._create_response_from_memory(query, entity_name, last_interaction, session_id)
                if session_id:
                    self._update_semantic_memory(
                        session_id, query, response_data['confidence'], response_data['decision_type'], 
                        True, response_data, None
                    )
                return response_data
            
            candidates = []
            search_method = 'normal'
            context_info = {}
            if session_id and hasattr(self.response_generator, 'memory'):
                context_info = self.response_generator.memory.get_context_for_query(session_id, query)
                logger.info(f"üîç Context analysis: should_use={context_info.get('should_use_context', False)}, strength={context_info.get('context_strength', 0)}")
            
            should_try_context = (
                context_info.get('should_use_context', False) and 
                context_info.get('related_entities') and
                context_info.get('context_strength', 0) >= 1.5
            )
            is_entity_query = any(pattern in query.lower() for pattern in [
                'l√† ai', 'ai l√†', '√¥ng ', 'b√† ', 'th·∫ßy ', 'c√¥ ',
                'v·∫≠y ', 'th·∫ø ', 'c√≤n ', 'v√† ', 'gs.ts', 'ti·∫øn sƒ©'
            ])

            if should_try_context:
                logger.info("üîÑ Trying DUAL search (context + normal) for comparison")
                context_keywords = context_info.get('context_keywords', [])
                candidates, search_method = self.sbert_retriever.dual_semantic_search(
                    query, 
                    context_keywords, 
                    top_k=self.semantic_reranker.config['stage1_top_k']
                )
                logger.info(f"üîÑ Dual search result: method={search_method}, candidates={len(candidates)}")
            elif is_entity_query:
                logger.info("üîç Entity query detected - trying smart fallback search first")
                fallback_candidates = self._smart_entity_fallback_search(query, session_id)
                
                if fallback_candidates:
                    logger.info(f"‚úÖ Smart fallback found {len(fallback_candidates)} candidates")
                    candidates = fallback_candidates
                    search_method = 'entity_fallback'
                else:
                    logger.info("‚ö†Ô∏è Smart fallback found no candidates, using normal search")
                    candidates = self.sbert_retriever.semantic_search_top_k(
                        query, 
                        top_k=self.semantic_reranker.config['stage1_top_k']
                    )
                    search_method = 'normal'
            else:
                logger.info("üîç Using NORMAL search (non-entity query)")
                candidates = self.sbert_retriever.semantic_search_top_k(
                    query, 
                    top_k=self.semantic_reranker.config['stage1_top_k']
                )
                search_method = 'normal'

            if not candidates or len(candidates) == 0:
                logger.warning("‚ö†Ô∏è No candidates found in semantic search")
                return self._get_no_match_response()
            
            reranked_candidates = self.semantic_reranker.rerank(
                query=query, 
                candidates=candidates, 
                context_keywords=context_info.get('context_keywords', [])  # N·∫øu c·∫ßn, thay v√¨ context_info
            )
            
            if not reranked_candidates:
                logger.warning("‚ö†Ô∏è No candidates after re-ranking")
                return self._get_no_match_response()
            
            logger.info(f"üìä Top candidate analysis:")
            top = reranked_candidates[0]
            context_quality = self._analyze_context_quality(query, top, session_memory)
            logger.info(f"   Score: {top.get('final_score', 0):.3f} | Context quality: {context_quality:.3f}")
            logger.info(f"   Method: {search_method}")
            
            decision_type, context, should_respond = self.decision_engine.make_decision(
                query, reranked_candidates, session_memory, jwt_token, None
            )
            
            # ‚úÖ ADD PROFILE TO CONTEXT
            if student_profile:
                context['profile'] = student_profile
                logger.info(f"‚úÖ Added profile to context: {student_profile.get('full_name')}")
            
            if not should_respond:
                logger.info(f"üö´ Decision engine says NO RESPOND for type: {decision_type}")
                return self._get_no_match_response()
            
            was_education = self._is_education_related(query)
            user_type = 'student'
            
            response_text = self._execute_fixed_semantic_decision(
                decision_type, query, context, session_id
            )
            
            final_score = context.get('confidence', 0.5)
            
            if session_id:
                self._update_semantic_memory(
                    session_id, query, final_score, decision_type, 
                    was_education, context, None
                )
            
            return {
                'response': response_text,
                'confidence': final_score,
                'method': f'semantic_rag_{search_method}',
                'decision_type': decision_type,
                'semantic_info': context,
                'sources': self._format_sources(reranked_candidates[:3]),
                'processing_time': time.time() - start_time,
                'was_education_related': was_education,
                'user_type': user_type,
                'context_aware_rag': True,
                'context_quality': context_quality
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error in process_query: {str(e)}", exc_info=True)
            return {
                'response': self._get_error_response(session_id),
                'confidence': 0.0,
                'method': 'error',
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    def _analyze_context_quality(self, query, top_candidate, session_memory):
        """Analyze the quality of context from memory"""
        try:
            quality_score = 0.0
            
            # Base score from candidate
            if top_candidate:
                quality_score += min(0.3, top_candidate.get('final_score', 0))
            
            # Boost from memory
            if session_memory and len(session_memory) > 0:
                quality_score += 0.2
            
            # Boost from entity continuity
            query_lower = query.lower()
            if any(kw in query_lower for kw in ['√¥ng ·∫•y', 'b√† ·∫•y', 'th·∫ßy ·∫•y', 'c√¥ ·∫•y', 'v·∫≠y', 'th·∫ø', 'c√≤n']):
                quality_score += 0.3
            
            # Boost from clear question patterns
            if any(kw in query_lower for kw in ['l√† ai', 'ai l√†', 'l√†m g√¨', '·ªü ƒë√¢u', 'nh∆∞ th·∫ø n√†o']):
                quality_score += 0.2
            
            return min(1.0, quality_score)
            
        except Exception as e:
            logger.error(f"‚ùå Error analyzing context quality: {str(e)}")
            return 0.0
        
    def _execute_fixed_semantic_decision(self, decision_type, query, context, session_id):
        logger.info(f"üéØ Executing FIXED semantic decision: {decision_type}")
        gemini_available = self._check_gemini_availability()
        if not gemini_available:
            logger.warning("‚ö†Ô∏è Gemini API not available - using FIXED graceful degradation")
            return self._create_fixed_semantic_fallback_response(decision_type, query, context, session_id)        
        try:
            if decision_type == 'use_document_context':
                response = self.response_generator.generate_response(
                    query=query, context=context, intent_info=None, entities={}, session_id=session_id
                )
                response_text = response.get('response', '') if response else ''                
                if not response_text or len(response_text.strip()) < 10:
                    logger.warning("‚ö†Ô∏è Empty/invalid response from Gemini - using fallback")
                    return self._get_document_fallback(session_id)
                
                return response_text
            elif decision_type == 'use_external_api':
                return self._handle_external_api_decision(query, context, session_id)            
            elif decision_type == 'require_authentication':
                return self._handle_authentication_required(session_id)            
            elif decision_type in ['use_db_direct', 'enhance_db_answer']:
                response = self.response_generator.generate_response(
                    query=query, context=context, intent_info=None, entities={}, session_id=session_id
                )
                response_text = response.get('response', '') if response else ''                
                if not response_text or len(response_text.strip()) < 10:
                    logger.warning("‚ö†Ô∏è Empty/invalid response from Gemini - using FIXED semantic fallback")
                    return self._create_fixed_semantic_fallback_response(decision_type, query, context, session_id)                
                return response_text
            
            elif decision_type == 'ask_clarification':
                if context and context.get('smart_clarification', False):
                    logger.info("ü§î Creating FIXED smart clarification response")
                    mismatch_issues = context.get('mismatch_issues', [])
                    return self.decision_engine._create_smart_clarification_response(
                        query, mismatch_issues, session_id
                    )
                else:
                    response = self.response_generator.generate_response(
                        query=query, context=context, intent_info=None, entities={}, session_id=session_id
                    )
                    response_text = response.get('response', '') if response else ''                    
                    if not response_text or len(response_text.strip()) < 10:
                        return self._get_clarification_fallback(session_id)                    
                    return response_text
            elif decision_type == 'say_dont_know':
                response = self.response_generator.generate_response(
                    query=query, context=context, intent_info=None, entities={}, session_id=session_id
                )
                response_text = response.get('response', '') if response else ''                
                if not response_text or len(response_text.strip()) < 10:
                    return self._get_dont_know_fallback(session_id)                
                return response_text            
            else:
                return self._get_out_of_scope_response(session_id)
                
        except Exception as e:
            logger.error(f"‚ùå Error in _execute_fixed_semantic_decision: {str(e)}")
            return self._create_fixed_semantic_fallback_response(decision_type, query, context, session_id)
    
    def _create_fixed_semantic_fallback_response(self, decision_type, query, context, session_id):
        logger.info(f"üõ°Ô∏è Creating FIXED semantic fallback for decision: {decision_type}")
        personal_address = self._get_personal_address(session_id)
        raw_answer = context.get('response', '') if context else ''
        mismatch_issues = context.get('mismatch_issues', []) if context else []
        confidence_preserved = context.get('confidence_preserved', False) if context else False        
        if mismatch_issues and decision_type in ['use_db_direct', 'enhance_db_answer', 'ask_clarification']:
            logger.info("ü§î FIXED fallback: Using smart clarification due to detected mismatches")
            return self.decision_engine._create_smart_clarification_response(
                query, mismatch_issues, session_id
            )
        if decision_type in ['use_db_direct', 'enhance_db_answer']:
            if raw_answer and raw_answer.strip():
                logger.info(f"üîç DEBUG - Raw database answer: '{raw_answer[:300]}...'")                
                clean_answer = raw_answer.strip()                
                clean_answer = re.sub(r'^(d·∫°\s+(th·∫ßy|c√¥|sinh vi√™n)[^,]*,?\s*)', '', clean_answer, flags=re.IGNORECASE)
                clean_answer = re.sub(r'^(xin ch√†o|ch√†o)[^.!?]*[.!?]\s*', '', clean_answer, flags=re.IGNORECASE)                
                if clean_answer and not clean_answer[0].isupper():
                    clean_answer = clean_answer[0].upper() + clean_answer[1:]                
                personalized_response = f"Ch√†o c·∫≠u, {clean_answer}"                
                if not personalized_response.strip().endswith(('?', '!', '.')):
                    personalized_response += '.'                
                if confidence_preserved:
                    personalized_response += f' {personal_address.title()} c√≥ c·∫ßn t·ªõ h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?'
                else:
                    personalized_response += f' {personal_address.title()} c·∫ßn t·ªõ l√†m r√µ th√™m g√¨ kh√¥ng ·∫°?'                
                logger.info(f"üõ°Ô∏è FIXED SEMANTIC FALLBACK: Formatted raw answer for {personal_address}")
                return personalized_response
            else:
                return f"Ch√†o c·∫≠u, t·ªõ ch∆∞a c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y. {personal_address.title()} c√≥ th·ªÉ li√™n h·ªá ph√≤ng ban li√™n quan ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ chi ti·∫øt."
        return f"Ch√†o c·∫≠u, t·ªõ s·∫µn s√†ng h·ªó tr·ª£ {personal_address} v·ªÅ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn BDU. {personal_address.title()} c√≥ th·ªÉ chia s·∫ª c·ª• th·ªÉ h∆°n v·ªÅ ƒëi·ªÅu c·∫ßn h·ªó tr·ª£ kh√¥ng ·∫°?"
    def _check_gemini_availability(self):
        try:
            if not self.response_generator:
                return False            
            if not hasattr(self.response_generator, 'key_manager') or not self.response_generator.key_manager.keys:
                return False            
            test_key = self.response_generator.key_manager.get_key()
            if not test_key:
                return False            
            return True            
        except Exception as e:
            logger.error(f"‚ùå Error checking Gemini availability: {str(e)}")
            return False
    def _validate_answer_relevance(self, query, answer):
        try:
            query_lower = query.lower()
            answer_lower = answer.lower()            
            concept_patterns = {
                'b√°o c√°o kh·ªëi l∆∞·ª£ng': ['b√°o c√°o', 'kh·ªëi l∆∞·ª£ng', 'c√¥ng vi·ªác'],
                'k√™ khai nhi·ªám v·ª•': ['k√™ khai', 'nhi·ªám v·ª•'],
                't·ªët nghi·ªáp': ['t·ªët nghi·ªáp', 'graduation'],
                't·∫°p ch√≠': ['t·∫°p ch√≠', 'journal', 'b√†i vi·∫øt'],
                'l·ªãch gi·∫£ng': ['l·ªãch', 'gi·∫£ng d·∫°y', 'schedule'],
                'h·∫°n n·ªôp': ['h·∫°n', 'deadline', 'ch·∫≠m nh·∫•t']
            }            
            main_concept = None
            for concept, keywords in concept_patterns.items():
                if any(kw in query_lower for kw in keywords):
                    main_concept = concept
                    break            
            if not main_concept:
                return True
            concept_keywords = concept_patterns[main_concept]
            answer_has_concept = any(kw in answer_lower for kw in concept_keywords)            
            relevance_issues = []
            if 'b√°o c√°o kh·ªëi l∆∞·ª£ng' in query_lower and 'kh·ªëi l∆∞·ª£ng h·ªçc t·∫≠p' in answer_lower:
                relevance_issues.append("Query v·ªÅ 'b√°o c√°o kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác' nh∆∞ng answer v·ªÅ 'kh·ªëi l∆∞·ª£ng h·ªçc t·∫≠p sinh vi√™n'")
            if 'k√™ khai nhi·ªám v·ª•' in query_lower and 'k√™ khai' not in answer_lower:
                relevance_issues.append("Query v·ªÅ 'k√™ khai nhi·ªám v·ª•' nh∆∞ng answer kh√¥ng ch·ª©a 'k√™ khai'")
            if relevance_issues:
                logger.warning(f"üîç ANSWER RELEVANCE WARNING:")
                for issue in relevance_issues:
                    logger.warning(f"   ‚ö†Ô∏è {issue}")
                return False            
            return answer_has_concept            
        except Exception as e:
            logger.error(f"‚ùå Error in answer relevance validation: {str(e)}")
            return True
    def _clean_query(self, query):
        if not query:
            return ""
        query = re.sub(r'\s+', ' ', query.strip())
        query = re.sub(r'[?]{2,}', '?', query)
        query = re.sub(r'[!]{2,}', '!', query)
        return query
    
    def _update_semantic_memory(self, session_id, query, confidence, decision_type, was_education, semantic_info_context, document_text=None):
        if session_id not in self.conversation_memory:
            self.conversation_memory[session_id] = []
        
        user_type = 'student'
        
        interaction = {
            'query': query,
            'response': semantic_info_context.get('response', '') if isinstance(semantic_info_context, dict) else '',
            'confidence': confidence,
            'semantic_info': {
                'method': semantic_info_context.get('method', 'unknown') if isinstance(semantic_info_context, dict) else 'unknown',
                'top_score': semantic_info_context.get('final_score', confidence) if isinstance(semantic_info_context, dict) else confidence,
                'extracted_entities': semantic_info_context.get('extracted_entities', {}) if isinstance(semantic_info_context, dict) else {},
                'confidence_preserved': semantic_info_context.get('confidence_preserved', False),
                'smart_penalty': semantic_info_context.get('smart_penalty', 0),
                'mismatch_issues': semantic_info_context.get('mismatch_issues', []),
                'semantic_decision': True
            },
            'timestamp': time.time(),
            
            # S·ª¨ D·ª§NG GI√Å TR·ªä ƒê√öNG
            'user_type': user_type,
            
            'decision_type': decision_type,
            'was_education_related': was_education,
            'fixed_semantic_processed': True,
            'document_text': document_text,
            'document_context_priority': decision_type == 'use_document_context',
            'external_api_used': decision_type == 'use_external_api',
            'query_length': len(query.split()),
            'architecture': 'fixed_semantic_rag'
        }
        self.conversation_memory[session_id].append(interaction)
        
        self.conversation_memory[session_id] = self.conversation_memory[session_id][-30:]
        
        logger.info(f"üß† FIXED semantic memory updated for session {session_id} (user_type: {user_type}): {len(self.conversation_memory[session_id])} interactions")
        
    def _get_personal_address(self, session_id):
        try:
            if hasattr(self.response_generator, '_get_personal_address'):
                return self.response_generator._get_personal_address(session_id)
            return "b·∫°n"
        except Exception as e:
            logger.error(f"‚ùå Error getting personal address: {str(e)}")
            return "b·∫°n"
    def _get_empty_query_response(self):
        return {
            'response': "Ch√†o b·∫°n! M√¨nh c√≥ th·ªÉ h·ªó tr·ª£ g√¨ cho b·∫°n v·ªÅ c√°c v·∫•n ƒë·ªÅ t·∫°i BDU kh√¥ng?",
            'confidence': 0.9,
            'method': 'empty_query',
            'processing_time': 0.01,
            'fixed_semantic_rag': True
        }
    def _get_no_match_response(self):
        return {
            'response': "M√¨nh ch∆∞a c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y. B·∫°n c√≥ th·ªÉ li√™n h·ªá ph√≤ng ban li√™n quan ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ chi ti·∫øt nh√©.",
            'confidence': 0.1,
            'method': 'no_match_semantic',
            'decision_type': 'say_dont_know',
            'processing_time': 0.01,
            'fixed_semantic_rag': True
        }
    def _get_out_of_scope_response(self, session_id):
        personal_address = self._get_personal_address(session_id)
        return f"Ch√†o c·∫≠u, t·ªõ ch·ªâ h·ªó tr·ª£ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn c√¥ng vi·ªác sinh vi√™n t·∫°i BDU th√¥i ·∫°!"
    def _get_error_response(self, session_id):
        personal_address = self._get_personal_address(session_id)
        return f"Ch√†o c·∫≠u, t·ªõ g·∫∑p kh√≥ khƒÉn k·ªπ thu·∫≠t. {personal_address.title()} c√≥ th·ªÉ li√™n h·ªá b·ªô ph·∫≠n IT qua email it@bdu.edu.vn ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£."
    def _get_clarification_fallback(self, session_id):
        personal_address = self._get_personal_address(session_id)
        return f"Ch√†o b·∫°n, ƒë·ªÉ t·ªõ h·ªó tr·ª£ ch√≠nh x√°c nh·∫•t, {personal_address} c√≥ th·ªÉ n√≥i r√µ h∆°n v·ªÅ v·∫•n ƒë·ªÅ c·∫ßn h·ªó tr·ª£ kh√¥ng ·∫°?"
    def _get_dont_know_fallback(self, session_id):
        personal_address = self._get_personal_address(session_id)
        return f"Ch√†o b·∫°n, t·ªõ ch∆∞a c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y. {personal_address.title()} c√≥ th·ªÉ li√™n h·ªá ph√≤ng ban li√™n quan ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ chi ti·∫øt."
    def _get_document_fallback(self, session_id):
        personal_address = self._get_personal_address(session_id)
        return f"Ch√†o b·∫°n, t·ªõ ƒë√£ x√©t t√†i li·ªáu nh∆∞ng g·∫∑p kh√≥ khƒÉn trong vi·ªác tr·∫£ l·ªùi. {personal_address.title()} c√≥ th·ªÉ ƒë·∫∑t c√¢u h·ªèi c·ª• th·ªÉ h∆°n kh√¥ng ·∫°?"
    def _handle_external_api_decision(self, query, context, session_id):
        """
        Quy·∫øt ƒë·ªãnh g·ªçi external API v√† t·∫°o c√¢u tr·∫£ l·ªùi t·ª± nhi√™n.
        H·ªó tr·ª£ sinh vi√™n. C√≥ fallback khi LLM ho·∫∑c API thi·∫øu d·ªØ li·ªáu.
        """
        from .student_api_handler import handle_external_api_student
        from ..external_api_service import ExternalAPIService  # tr√°nh import v√≤ng
        import jwt

        svc = ExternalAPIService()
        jwt_token = (context or {}).get('jwt_token') or ''
        role_hint = (context or {}).get('role')  # n·∫øu upstream c√≥ truy·ªÅn
        lower_q = (query or '').lower()

        # 1) X√°c ƒë·ªãnh vai tr√≤ t·ª´ JWT (∆∞u ti√™n) ho·∫∑c hint
        role = None
        try:
            # PyJWT decode kh√¥ng c·∫ßn verify khi ch·ªâ ƒë·ªçc claim, SSO n·ªôi b·ªô th√¨ verify ·ªü middleware r·ªìi
            payload = jwt.decode(jwt_token, options={"verify_signature": False})
            role = payload.get('role') or payload.get('roles') or role_hint
        except Exception:
            role = role_hint

        # 2) Router: student only
        is_student = (role == 'sinhvien' or 'sinh vi√™n' in str(role or '').lower())

        try:
            if is_student:
                # S·ª≠ d·ª•ng function handle_external_api_student m·ªõi
                result = handle_external_api_student(jwt_token, query)
                if result.get("status") == "success":
                    return result.get("response", "ƒê√£ x·ª≠ l√Ω y√™u c·∫ßu th√†nh c√¥ng.")
                else:
                    return self._get_api_error_response(result, session_id)

            else:
                # Ch·ªâ h·ªó tr·ª£ sinh vi√™n
                return "Xin l·ªói, h·ªá th·ªëng hi·ªán t·∫°i ch·ªâ h·ªó tr·ª£ sinh vi√™n. Vui l√≤ng li√™n h·ªá ph√≤ng ƒë√†o t·∫°o ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£."

        except Exception as e:
            logger.error(f"‚ùå Error handling external API: {str(e)}")
            return self._get_api_error_fallback(session_id)
    def _handle_authentication_required(self, session_id):
        personal_address = self._get_personal_address(session_id)
        return f"Ch√†o b·∫°n, ƒë·ªÉ t·ªõ c√≥ th·ªÉ cung c·∫•p th√¥ng tin c√° nh√¢n nh∆∞ l·ªãch h·ªçc, {personal_address} c·∫ßn ƒëƒÉng nh·∫≠p v√†o ·ª©ng d·ª•ng tr∆∞·ªõc . üîê"
    def _get_api_fallback(self, api_result, session_id):
        personal_address = self._get_personal_address(session_id)
        return f"Ch√†o b·∫°n, t·ªõ ƒë√£ t√¨m th·∫•y th√¥ng tin l·ªãch h·ªçc nh∆∞ng g·∫∑p kh√≥ khƒÉn trong vi·ªác tr√¨nh b√†y chi ti·∫øt. {personal_address.title()} c√≥ th·ªÉ truy c·∫≠p h·ªá th·ªëng qu·∫£n l√Ω ƒë√†o t·∫°o ƒë·ªÉ xem th√¥ng tin ƒë·∫ßy ƒë·ªß."
    def _get_api_error_response(self, api_result, session_id):
        personal_address = self._get_personal_address(session_id)
        return f"Ch√†o b·∫°n, t·ªõ g·∫∑p kh√≥ khƒÉn khi truy xu·∫•t th√¥ng tin c√° nh√¢n. {personal_address.title()} c√≥ th·ªÉ th·ª≠ l·∫°i sau ho·∫∑c li√™n h·ªá b·ªô ph·∫≠n IT ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£."
    def _get_api_error_fallback(self, session_id):
        personal_address = self._get_personal_address(session_id)
        return f"Ch√†o b·∫°n, m√¨nh g·∫∑p kh√≥ khƒÉn k·ªπ thu·∫≠t khi truy xu·∫•t th√¥ng tin c√° nh√¢n. {personal_address.title()} c√≥ th·ªÉ th·ª≠ l·∫°i sau."
    def _format_sources(self, results):
        sources = []
        for result in results:
            if result and result.get('final_score', 0) > 0.2:
                sources.append({
                    'question': result['question'],
                    'category': result.get('category', 'sinh vi√™n'),
                    'final_score': result.get('final_score', 0),
                    'original_semantic_score': result.get('semantic_score', 0),
                    'smart_penalty': result.get('smart_penalty', 0),
                    'stage1_score': result.get('stage1_score', 0),
                    'stage2_score': result.get('stage2_score', 0),
                    'mismatch_issues': result.get('mismatch_issues', []),
                    'fixed_semantic_reranking': result.get('fixed_semantic_reranking', False)
                })
        return sources

    def get_conversation_context(self, session_id):
        return self.conversation_memory.get(session_id, [])
    def get_conversation_memory(self, session_id):
        return self.response_generator.get_conversation_memory(session_id)
    def clear_conversation_memory(self, session_id=None):
        if session_id:
            self.response_generator.clear_conversation_memory(session_id)
            if session_id in self.conversation_memory:
                del self.conversation_memory[session_id]
        else:
            self.response_generator.clear_conversation_memory()
            self.conversation_memory.clear()
    def reload_after_qa_update(self):
        logger.info("üîÑ Reloading FIXED semantic knowledge base...")
        if hasattr(self.sbert_retriever, 'cached_data'):
            self.sbert_retriever.cached_data = None
            self.sbert_retriever.cache_timestamp = 0
        self.sbert_retriever.load_knowledge_base()
        if self.sbert_retriever.model and self.sbert_retriever.knowledge_data:
            self.sbert_retriever.build_faiss_index()
    def _is_education_related(self, query):
        education_keywords = [
            'tr∆∞·ªùng', 'h·ªçc', 'sinh vi√™n', 'tuy·ªÉn sinh', 'h·ªçc ph√≠', 'ng√†nh',
            'ƒë·∫°i h·ªçc', 'bdu', 'ƒëƒÉng k√Ω', 'm√¥n h·ªçc', 't√≠n ch·ªâ', 
            'l·ªãch thi', 'k·ª≥ thi', 'ƒëi·ªÉm', 'ƒëi·ªÉm danh', 'v·∫Øng',
            'th·ªùi kh√≥a bi·ªÉu', 'l·ªãch h·ªçc', 'ph√≤ng h·ªçc', 'ti·∫øt h·ªçc',
            'h·ªçc l·∫°i', 'c·∫£i thi·ªán ƒëi·ªÉm', 'thi l·∫°i', 'n√¢ng ƒëi·ªÉm',
            'ƒëi·ªÉm trung b√¨nh', 'trung b√¨nh', 't√≠nh ƒëi·ªÉm', 'ƒëi·ªÉm qu√° tr√¨nh',
            'ƒëi·ªÉm thi', 'ƒëi·ªÉm cu·ªëi k·ª≥', 'ƒëi·ªÉm gi·ªØa k·ª≥',
            'kh·ªëi l∆∞·ª£ng', 't·ªëi thi·ªÉu', 'ch∆∞∆°ng tr√¨nh', 'h·ªçc k·ª≥', 'nƒÉm h·ªçc',
            't·ªët nghi·ªáp', 'l·ªÖ t·ªët nghi·ªáp', 'x√©t t·ªët nghi·ªáp', 'b·∫±ng c·∫•p',
            'vƒÉn b·∫±ng', 'c·ª≠ nh√¢n', 'c·∫•p b·∫±ng', 'nh·∫≠n b·∫±ng',
            'k·ª∑ lu·∫≠t', 'danh s√°ch', 'theo quy ƒë·ªãnh', 'quy ƒë·ªãnh v·ªÅ', 
            'th·ªß t·ª•c', 'ƒëi·ªÅu ki·ªán', 'y√™u c·∫ßu', 'm·ªü l·ªõp',
            'nh∆∞ th·∫ø n√†o', 'bao nhi√™u', 'l√† ai', 'ai l√†', 'l√†m g√¨', '·ªü ƒë√¢u',
            'khi n√†o', 'c√≥ ƒë∆∞·ª£c', 'c·∫ßn g√¨', 'ph·∫£i l√†m'
        ]
        if not query:
            return False        
        query_lower = query.lower()
        return any(kw in query_lower for kw in education_keywords)