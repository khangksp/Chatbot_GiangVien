import re
import logging
import time

logger = logging.getLogger(__name__)

class SimpleEntityExtractor:
    """
    Tr√≠ch xu·∫•t th·ª±c th·ªÉ ƒë∆°n gi·∫£n (NER) b·∫±ng Regex c√≥ m·ª•c ti√™u (Targeted Regex).
    Ch·ªâ tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ r√µ r√†ng ƒë·ªÉ tr√°nh l√†m √¥ nhi·ªÖm context memory.
    """
    
    def __init__(self):
        # --- S·ª¨A L·ªñI: ƒê·ªãnh nghƒ©a l·∫°i ho√†n to√†n c√°c pattern ---
        self.entity_patterns = {
            'person_name': [
                # Ch·ªâ b·∫Øt t√™n (1-3 t·ª´) SAU KHI c√≥ c√°c ch·ª©c danh
                r'(?:th·∫ßy|c√¥|√¥ng|b√†|GS\.TS\.|TS\.|GS\.|ti·∫øn sƒ©|gi√°o s∆∞)\s+([A-Z√Ä-·ª∏][a-z√†-·ªπ]+(?:\s+[A-Z√Ä-·ª∏][a-z√†-·ªπ]+){0,2})\b',
                # B·∫Øt t√™n ƒë·∫ßy ƒë·ªß (2-4 t·ª´) vi·∫øt hoa
                r'\b([A-Z√Ä-·ª∏][a-z√†-·ªπ]+(?:\s+[A-Z√Ä-·ª∏][a-z√†-·ªπ]+){1,3})\b'
            ],
            'position': [
                r'\b(hi·ªáu tr∆∞·ªüng|ph√≥ hi·ªáu tr∆∞·ªüng|tr∆∞·ªüng ph√≤ng|ph√≥ tr∆∞·ªüng ph√≤ng|tr∆∞·ªüng khoa|ph√≥ tr∆∞·ªüng khoa)\b'
            ],
            'department': [
                r'\b(khoa [A-Z√Ä-·ª∏][A-Za-z√†-·ªπ\s]+|ph√≤ng [A-Z√Ä-·ª∏][A-Za-z√†-·ªπ\s]+|ban [A-Z√Ä-·ª∏][A-Za-z√†-·ªπ\s]+)\b'
            ],
            'dates': [
                r'(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
                r'(h·ªçc k·ª≥ \d+|nƒÉm h·ªçc \d{4}-\d{4})'
            ]
        }
        
        # ‚úÖ FIX #3.1: EXPANDED BLACKLIST - Lo·∫°i b·ªè c√°c t·ª´ b·ªã nh·∫≠n nh·∫ßm l√† T√äN
        self.person_name_blacklist = {
            # Ng√†y trong tu·∫ßn
            'th·ª© hai', 'th·ª© ba', 'th·ª© t∆∞', 'th·ª© nƒÉm', 'th·ª© s√°u', 'th·ª© b·∫£y', 'ch·ªß nh·∫≠t',
            'th·ª© 2', 'th·ª© 3', 'th·ª© 4', 'th·ª© 5', 'th·ª© 6', 'th·ª© 7', 'th·ª© 8',
            'h√¥m nay', 'h√¥m qua', 'ng√†y mai', 'tu·∫ßn n√†y', 'tu·∫ßn sau',
            
            # Greeting phrases
            'ch√†o b·∫°n', 'xin ch√†o', 'ch√∫c b·∫°n', 'k√≠nh ch√†o', 'ch√†o m·ª´ng',
            'ch√†o t√†i', 'ch√†o hi·ªáp', 'ch√†o m·ªçi ng∆∞·ªùi',
            
            # Common phrases
            'ch√∫c b·∫°n h·ªçc t·ªët', 'b·∫°n vui l√≤ng cho', 'm√¨nh bi·∫øt tu·∫ßn n√†y', 
            'v√≠ d·ª•', 'h·ªèi n√†o kh√°c', 'vui l√≤ng cho',
            
            # Academic terms
            'c√¥ng ngh·ªá th√¥ng tin', 'ƒë·∫°i h·ªçc b√¨nh d∆∞∆°ng', 'l·∫≠p tr√¨nh web',
            'th·ªùi kh√≥a bi·ªÉu', 'li·ªáu th·ªùi kh√≥a bi·ªÉu', 'h·ªçc ph√≠', 'h·ªçc k·ª≥', 
            'nƒÉm h·ªçc', 't√≠n ch·ªâ', 'm√¥n h·ªçc', 'l·ªõp h·ªçc', 'kh√≥a h·ªçc',
            
            # Organizational terms
            'tr√¢n tr·ªçng', 'k√≠nh g·ª≠i', 'th√¢n g·ª≠i', 'trung t√¢m', 
            'ph√≤ng ƒë√†o t·∫°o', 'khoa', 'b·ªô m√¥n', 'ban', 'ph√≤ng ban',
            
            # Questions/Requests
            'cho bi·∫øt', 'cho t√¥i', 'gi√∫p t√¥i', 'vui l√≤ng',
        }
        
        # ‚úÖ FIX #3.2: QUESTION WORDS ƒë·ªÉ filter department/position
        self.question_words = {
            'g√¨', 'n√†o', 'ƒë√¢u', 'sao', 'th·∫ø n√†o', 'nh∆∞ th·∫ø n√†o', 
            'nh√©', '·∫°', 'h·∫£', '√†', 'h·∫£', 'kh√¥ng'
        }
        
        # ‚úÖ FIX #3.3: TIME INDICATORS ƒë·ªÉ tr√°nh extract ng√†y l√†m entity
        self.time_indicators = {
            'th·ª©', 'h√¥m', 'ng√†y', 'th√°ng', 'nƒÉm', 'tu·∫ßn', 't·ªëi', 's√°ng', 'chi·ªÅu', 'tr∆∞a'
        }
        
        logger.info("‚úÖ Targeted SimpleEntityExtractor initialized with expanded filters.")

    def extract_entities(self, text, query_context=""):
        if not text:
            return {}
            
        entities = {}
        text_cleaned = re.sub(r'\s+', ' ', text.strip())
        
        for entity_type, patterns in self.entity_patterns.items():
            found_entities = []
            for pattern in patterns:
                # D√πng re.IGNORECASE cho c√°c ch·ª©c danh (position, department)
                flags = re.IGNORECASE if entity_type != 'person_name' else 0
                matches = re.finditer(pattern, text_cleaned, flags)
                
                for match in matches:
                    # Lu√¥n l·∫•y group(1) n·∫øu c√≥, v√¨ group(0) s·∫Ω ch·ª©a c·∫£ ch·ª©c danh
                    entity_value = match.group(1) if match.groups() else match.group(0)
                    entity_value = entity_value.strip().rstrip('.,')
                    
                    if self._is_valid_entity(entity_value, entity_type, text_cleaned):
                        normalized_value = self._normalize_entity(entity_value, entity_type)
                        if normalized_value not in found_entities:
                            found_entities.append(normalized_value)
            
            if found_entities:
                entities[entity_type] = found_entities
        
        logger.debug(f"üîç Entity extraction result: {entities}")
        return entities

    def _is_valid_entity(self, entity_value, entity_type, full_text=""):
        """
        ‚úÖ FIX #3.4: ENHANCED VALIDATION v·ªõi nhi·ªÅu check h∆°n
        """
        if not entity_value or len(entity_value.strip()) < 3:
            return False
            
        entity_lower = entity_value.lower().strip()
        
        # ‚úÖ CHECK 1: Blacklist cho person_name
        if entity_type == 'person_name':
            if entity_lower in self.person_name_blacklist:
                logger.debug(f"üö´ Rejected by person blacklist: '{entity_value}'")
                return False
            
            # Check xem c√≥ ph·∫£i c√¢u ho√†n ch·ªânh kh√¥ng
            sentence_starters = ['ch√∫c', 'h·ªèi', 'v√≠ d·ª•', 'li·ªáu', 'b·∫°n', 'm√¨nh', 'cho', 'gi√∫p', 'vui l√≤ng']
            if entity_lower.startswith(tuple(sentence_starters)):
                logger.debug(f"üö´ Rejected sentence-like entity: '{entity_value}'")
                return False
            
            # ‚úÖ CHECK 2: Time indicators (tr√°nh "Th·ª© Ba", "H√¥m Nay")
            words = entity_lower.split()
            if any(word in self.time_indicators for word in words):
                logger.debug(f"üö´ Rejected time indicator in name: '{entity_value}'")
                return False

        # ‚úÖ CHECK 3: Question words cho department/position
        if entity_type in ['department', 'position']:
            words = entity_lower.split()
            if any(word in self.question_words for word in words):
                logger.debug(f"üö´ Rejected question word in {entity_type}: '{entity_value}'")
                return False
            
            # Tr√°nh pattern "khoa g√¨", "ph√≤ng n√†o", etc.
            if re.search(r'(khoa|ph√≤ng|ban)\s+(g√¨|n√†o|ƒë√¢u|nh√©|·∫°)', entity_lower):
                logger.debug(f"üö´ Rejected question pattern in {entity_type}: '{entity_value}'")
                return False

        # ‚úÖ CHECK 4: C√°c t·ª´ r√°c chung
        noise_words = {'l√†', 'c·ªßa', 'v√†', 'ƒë·ªÉ', 'trong', 'c√≥', 'kh√¥ng', 'ƒë∆∞·ª£c', 'th√¨', 'n√†y', 'ƒë√≥'}
        entity_words = set(entity_lower.split())
        if len(entity_words.intersection(noise_words)) > 0:
            logger.debug(f"üö´ Rejected noise word in entity: '{entity_value}'")
            return False

        # ‚úÖ CHECK 5: Too short sau khi remove prefix
        clean_words = [w for w in entity_lower.split() if w not in {'khoa', 'ph√≤ng', 'ban'}]
        if entity_type in ['department', 'position'] and len(' '.join(clean_words)) < 3:
            logger.debug(f"üö´ Rejected too short {entity_type}: '{entity_value}'")
            return False

        logger.debug(f"‚úÖ Valid entity: '{entity_value}' (type: {entity_type})")
        return True

    def _normalize_entity(self, name, entity_type):
        """Chu·∫©n h√≥a entity"""
        if entity_type == 'person_name':
            # Vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu m·ªói t·ª´
            words = name.lower().split()
            normalized_words = [word[0].upper() + word[1:] for word in words if word]
            return ' '.join(normalized_words)
        
        return name.lower() # Gi·ªØ nguy√™n case cho c√°c lo·∫°i kh√°c n·∫øu c·∫ßn, ho·∫∑c lower()

    def build_entity_relationships(self, query, answer, entities):
        """Build relationships between entities"""
        relationships = []
        if 'person_name' in entities and 'position' in entities:
            for person in entities['person_name']:
                for position in entities['position']:
                    relationships.append({
                        'type': 'person_position',
                        'entity1': person,
                        'entity2': position,
                        'relation': 'has_position',
                        'confidence': 0.8,
                        'source': 'context'
                    })
        return relationships

    def get_context_keywords(self, entities, relationships):
        """Get context keywords from entities"""
        context_keywords = []
        if 'person_name' in entities:
            context_keywords.extend(entities['person_name'][:2])
        if 'position' in entities:
            context_keywords.extend(entities['position'][:1])
        context_keywords = list(set(context_keywords))
        return [kw for kw in context_keywords if len(kw.strip()) > 2][:3]