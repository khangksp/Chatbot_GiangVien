import logging
import time
import requests
import re
import random
import json
from typing import Dict, Any, Optional, List
from unidecode import unidecode
import difflib
import pandas as pd
import os
from ..ner_service import SimpleEntityExtractor
from bs4 import BeautifulSoup
from .key_manager import GeminiApiKeyManager
from .memory import ConversationMemory
from .restorer import SimpleVietnameseRestorer
from .token_manager import SmartTokenManager
from .confidence import AdvancedConfidenceManager
from . import prompts

logger = logging.getLogger(__name__)

class GeminiResponseGenerator:    
    def __init__(self):
        self.key_manager = GeminiApiKeyManager()
        self.model_name = "gemini-2.5-flash" 
        self.base_url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model_name}:generateContent"
        self.memory = ConversationMemory(max_history=30)
        self.vietnamese_restorer = SimpleVietnameseRestorer(self.key_manager)
        self.token_manager = SmartTokenManager()
        self.confidence_manager = AdvancedConfidenceManager()
        self._user_context_cache = {}
        
        self.default_generation_config = {
            "temperature": 0.55,
            "topP": 0.85
        }
        
        self.role_consistency_rules = {
            'identity': 'AI assistant cá»§a Äáº¡i há»c BÃ¬nh DÆ°Æ¡ng (BDU) há»— trá»£ sinh viÃªn',
            'personality': 'lá»‹ch sá»±, chuyÃªn nghiá»‡p, tÃ´n trá»ng',
            'knowledge_scope': 'chuyÃªn vá» thÃ´ng tin BDU vÃ  há»— trá»£ sinh viÃªn',
            'addressing': 'xÆ°ng hÃ´ theo pronoun_style Ä‘Ã£ cáº¥u hÃ¬nh (vÃ­ dá»¥ ngÆ°á»i dÃ¹ng="báº¡n", bot="tÃ´i"/"tá»›"/"mÃ¬nh")',
            'prohibited_roles': [
                'sinh viÃªn', 'há»c sinh', 'phá»¥ huynh', 'ngÆ°á»i ngoÃ i trÆ°á»ng'
            ]
        }
    def _get_dynamic_pronouns(self, session_id: str) -> Dict[str, str]:
        user_context = self._user_context_cache.get(session_id, {})
        preferences = user_context.get('preferences', {})
        pronoun_style = preferences.get('pronoun_style', 'default')
        style = prompts.PERSONAL_PRONOUNS.get(pronoun_style, prompts.PERSONAL_PRONOUNS['default'])
        user_options = style['user']
        bot_options = style['bot']
        bot_pronoun = random.choice(bot_options)
        user_pronoun = "báº¡n"
        first_name = None
        full_name = user_context.get('full_name')
        if full_name and isinstance(full_name, str):
            name_parts = full_name.split()
            first_name = name_parts[-1] if name_parts else full_name
        available_user_options = []
        for option in user_options:
            if option == '{first_name}':
                if first_name:
                    available_user_options.extend([first_name, first_name]) 
            else:
                available_user_options.append(option)
                
        if not available_user_options:
            available_user_options = ['báº¡n', 'cáº­u']

        user_pronoun = random.choice(available_user_options)

        return {'user': user_pronoun, 'bot': bot_pronoun}
        
    def _should_strip_greeting(self, session_id: str) -> bool:
        try:
            hist = self.memory.conversations.get(session_id, {}).get('history', [])
            return len(hist) >= 1
        except Exception:
            return False
    def _strip_greeting_and_closing(self, text: str, personal_address: str) -> str:
        if not text:
            return text
        s = text.strip()
        greet_pat = rf'^(Dáº¡[\s,]+)?((Xin\s+)?[Cc]hÃ o\s+[^,!:]{{0,50}}[,!:]\s*)'
        s = re.sub(greet_pat, '', s)
        pa = re.escape(personal_address)
        s = re.sub(rf'^(Dáº¡[\s,]+)?{pa}\s*[,!:]\s*', '', s)
        pa_title = re.escape(personal_address.title())
        closing_variants = [
            r'cÃ³\s+thá»ƒ\s+em\s+há»—\s*trá»£\s+thÃªm\s+gÃ¬\s+khÃ´ng\s+áº¡\??',
            rf'{pa_title}\s+cÃ³\s+cáº§n\s+em\s+há»—\s*trá»£\s+thÃªm\s+gÃ¬\s+khÃ´ng\s+áº¡\??',
            r'em\s+cÃ³\s+thá»ƒ\s+giÃºp\s+gÃ¬\s+thÃªm\s+khÃ´ng\s+áº¡\??'
        ]
        s = re.sub('|'.join(closing_variants), '', s, flags=re.IGNORECASE).strip()
        s = re.sub(r'\s*([,;:])\s*$', '', s).strip()
        return s

    def _generate_external_api_response(self, query, context, session_id=None):
        api_data = context.get('api_data', None) # Láº¥y api_data (cÃ³ thá»ƒ lÃ  dict hoáº·c list hoáº·c None)
        profile_data = context.get('profile', {}) # Láº¥y profile (luÃ´n lÃ  dict)
        data_type = context.get('data_type', 'general') # Láº¥y data_type
        
        logger.debug(f"--- DEBUG: _generate_external_api_response ---")
        logger.debug(f"Data type: {data_type}")
        logger.debug(f"api_data type: {type(api_data)}")
        logger.debug(f"profile_data: {json.dumps(profile_data, ensure_ascii=False)}")
        student_name = profile_data.get('name', profile_data.get('full_name', 'báº¡n'))
        display_name = student_name.split()[-1] if student_name and student_name != 'báº¡n' else 'báº¡n' # Láº¥y tÃªn riÃªng
        mssv = profile_data.get('mssv', 'N/A')
        class_name = profile_data.get('class', profile_data.get('class_name', 'N/A'))
        faculty = profile_data.get('faculty', 'N/A')
        system_prompt_header = f"""Báº¡n lÃ  ChatBDU, trá»£ lÃ½ AI thÃ¢n thiá»‡n cá»§a Äáº¡i há»c BÃ¬nh DÆ°Æ¡ng, Ä‘ang nÃ³i chuyá»‡n vá»›i sinh viÃªn tÃªn lÃ  {display_name}.

ğŸ‘¤ THÃ”NG TIN Cá»¦A SINH VIÃŠN (Báº N):
- TÃªn: {student_name}
- MSSV: {mssv}
- Lá»›p: {class_name}
- Khoa: {faculty}

â“ CÃ‚U Há»I Cá»¦A SINH VIÃŠN: "{query}"

ğŸ“ Dá»® LIá»†U Tá»ª Há»† THá»NG:
"""
        data_section = ""
        instruction_section = ""
        if api_data is None:
             logger.warning(f"âš ï¸ api_data is None for data_type '{data_type}'. Cannot build data section.")
             data_section = "(KhÃ´ng cÃ³ dá»¯ liá»‡u tá»« há»‡ thá»‘ng)"
             instruction_section = f"""
HÆ¯á»šNG DáºªN (KhÃ´ng cÃ³ dá»¯ liá»‡u):
- ThÃ´ng bÃ¡o cho {display_name} ráº±ng khÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u liÃªn quan Ä‘áº¿n cÃ¢u há»i.
- CÃ³ thá»ƒ gá»£i Ã½ há»i láº¡i hoáº·c kiá»ƒm tra thÃ´ng tin khÃ¡c.
- XÆ°ng "mÃ¬nh" vÃ  gá»i sinh viÃªn lÃ  "{display_name}" hoáº·c "báº¡n".
"""
        elif data_type == "profile":
            _student_info_data = {}
            if isinstance(api_data, dict):
                 _student_info_data = api_data.get('student_info', api_data)
            else:
                 _student_info_data = profile_data
                 logger.warning(f"âš ï¸ Expected dict for profile api_data, got {type(api_data)}. Using profile_data instead.")

            data_section = f"""```json
{json.dumps(_student_info_data, ensure_ascii=False, indent=2)}
```"""
            instruction_section = f"""
HÆ¯á»šNG DáºªN (Profile):
- PhÃ¢n tÃ­ch cÃ¢u há»i Ä‘á»ƒ xÃ¡c Ä‘á»‹nh thÃ´ng tin sinh viÃªn Ä‘ang há»i vá» CHÃNH Há»Œ dá»±a trÃªn dá»¯ liá»‡u trÃªn.
- Chá»‰ tráº£ lá»i pháº§n thÃ´ng tin Ä‘Æ°á»£c há»i, ngáº¯n gá»n, tá»± nhiÃªn.
- XÆ°ng "mÃ¬nh" vÃ  gá»i sinh viÃªn lÃ  "{display_name}" hoáº·c "báº¡n".
- TUYá»†T Äá»I KHÃ”NG nÃ³i "MÃ¬nh há»c lá»›p..." mÃ  pháº£i nÃ³i "Báº¡n há»c lá»›p...".
"""
        elif data_type == "schedule" and isinstance(api_data, list):
            date_range_info = profile_data.get("date_range", {}) # Láº¥y tá»« profile context
            start_date = date_range_info.get("start_date")
            end_date = date_range_info.get("end_date")
            date_context = ""
            if start_date and end_date:
                date_context = f"cho khoáº£ng thá»i gian tá»« {start_date} Ä‘áº¿n {end_date}"
                if start_date == end_date:
                    date_context = f"cho ngÃ y {start_date}"

            data_section = f"""(Lá»‹ch há»c {date_context})
```json
{json.dumps(api_data, ensure_ascii=False, indent=2)}
```"""
            instruction_section = f"""
HÆ¯á»šNG DáºªN (Lá»‹ch há»c):
- Dá»±a vÃ o danh sÃ¡ch cÃ¡c buá»•i há»c {date_context}, hÃ£y tÃ³m táº¯t lá»‹ch há»c cho {display_name}.
- TrÃ¬nh bÃ y rÃµ rÃ ng theo ngÃ y, mÃ´n há»c, thá»i gian (tiáº¿t), phÃ²ng há»c, giáº£ng viÃªn.
- Náº¿u khÃ´ng cÃ³ lá»‹ch há»c trong danh sÃ¡ch, hÃ£y bÃ¡o rÃµ lÃ  khÃ´ng cÃ³ cho khoáº£ng thá»i gian Ä‘Ã³.
- XÆ°ng "mÃ¬nh" vÃ  gá»i sinh viÃªn lÃ  "{display_name}" hoáº·c "báº¡n".
- Náº¿u dá»¯ liá»‡u lÃ  cá»§a "2 tuáº§n tá»›i" (theo date_context), hÃ£y dÃ¹ng Ä‘Ãºng cá»¥m tá»« Ä‘Ã³.
"""
        elif data_type == "tuition" and isinstance(api_data, list):
            data_section = f"""(Danh sÃ¡ch cÃ¡c khoáº£n thu há»c phÃ­ vÃ  BHYT)
```json
{json.dumps(api_data, ensure_ascii=False, indent=2)}
```"""
            instruction_section = f"""
HÆ¯á»šNG DáºªN (Há»c phÃ­):
- Dá»±a vÃ o danh sÃ¡ch cÃ¡c khoáº£n thu (bao gá»“m trÆ°á»ng `tong_tien_phai_thu`, `tong_tien_da_thu`, `tong_tien_con_lai`, `status`, `nkhk`), hÃ£y phÃ¢n tÃ­ch vÃ  tráº£ lá»i cÃ¢u há»i cá»§a {display_name}.
- TÃ­nh toÃ¡n tá»•ng sá»‘ tiá»n cÃ²n láº¡i náº¿u Ä‘Æ°á»£c há»i "cÃ²n bao nhiÃªu" hoáº·c "chÆ°a Ä‘Ã³ng".
- Liá»‡t kÃª chi tiáº¿t cÃ¡c khoáº£n theo há»c ká»³ hoáº·c nÄƒm há»c (`nkhk`) náº¿u Ä‘Æ°á»£c há»i "cÃ¡c ká»³" hoáº·c "nÄƒm X".
- Tráº£ lá»i vá» tráº¡ng thÃ¡i ("Ä‘Ã£ Ä‘Ã³ng", "chÆ°a Ä‘Ã³ng") náº¿u Ä‘Æ°á»£c há»i.
- Náº¿u há»i "tá»•ng há»c phÃ­ cÃ¡c ká»³", hÃ£y tÃ­nh tá»•ng cá»™ng pháº£i Ä‘Ã³ng, Ä‘Ã£ Ä‘Ã³ng, cÃ²n láº¡i cá»§a Táº¤T Cáº¢ cÃ¡c khoáº£n trong danh sÃ¡ch.
- XÆ°ng "mÃ¬nh" vÃ  gá»i sinh viÃªn lÃ  "{display_name}" hoáº·c "báº¡n".
"""

        # *** KHá»I LOGIC Má»šI CHO CURRICULUM ***
        elif data_type == "curriculum" and isinstance(api_data, dict):
            curriculum_tree = api_data.get("curriculum_tree", [])
            credit_summary = api_data.get("credit_summary", {})
            
            data_section = f"""1. Dá»® LIá»†U TÃN CHá»ˆ Tá»”NG QUAN:
```json
{json.dumps(credit_summary, ensure_ascii=False, indent=2)}
```"""
            instruction_section = f"""
HÆ¯á»šNG DáºªN (Chung):
- Dá»±a vÃ o dá»¯ liá»‡u trÃªn, cá»‘ gáº¯ng tráº£ lá»i cÃ¢u há»i cá»§a {display_name} má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  tá»± nhiÃªn nháº¥t cÃ³ thá»ƒ.
- XÆ°ng "mÃ¬nh" vÃ  gá»i sinh viÃªn lÃ  "{display_name}" hoáº·c "báº¡n".
"""
        prompt = system_prompt_header + data_section + instruction_section + "\nTráº£ lá»i:"
        optimal_tokens = self.token_manager.calculate_optimal_tokens(
            len(prompt),
            'external_api_processing'
        )
        
        # *** TÄ‚NG TOKEN CHO CURRICULUM ***
        if data_type == "curriculum":
            optimal_tokens = max(optimal_tokens, 4096) # Äáº£m báº£o Ä‘á»§ token Ä‘á»ƒ phÃ¢n tÃ­ch JSON
            logger.info(f"ğŸŒ Processing external API data ({data_type}) with BOOSTED {optimal_tokens} tokens")
        else:
            logger.info(f"ğŸŒ Processing external API data ({data_type}) with {optimal_tokens} tokens")

        response = self._call_gemini_api_with_smart_tokens(
            prompt, 'external_api_processing', optimal_tokens, session_id
        )
        if response:
             response = response.strip()
        if not response:
            logger.warning(f"âš ï¸ Gemini failed or returned empty for external API ({data_type}). Using basic fallback.")
            if data_type == "profile":
                 s_name = profile_data.get('name', student_name)
                 s_mssv = profile_data.get('mssv', mssv)
                 s_class = profile_data.get('class', class_name)
                 s_faculty = profile_data.get('faculty', faculty)
                 response = f"ThÃ´ng tin cá»§a báº¡n: TÃªn {s_name}, MSSV {s_mssv}, Lá»›p {s_class}, Khoa {s_faculty}."
            elif data_type == "schedule" and isinstance(api_data, list):
                 if not api_data:
                     response = f"ChÃ o {display_name}, báº¡n khÃ´ng cÃ³ lá»‹ch há»c nÃ o trong khoáº£ng thá»i gian Ä‘Æ°á»£c yÃªu cáº§u."
                 else:
                     response = f"ÄÃ¢y lÃ  lá»‹ch há»c cá»§a báº¡n, {display_name}:\n"
                     for session in api_data[:2]:
                         response += f"- {session.get('ten_mon_hoc', 'N/A')} vÃ o ngÃ y {session.get('ngay_hoc', '?')}\n"
                     if len(api_data) > 2: response += "... (vÃ  cÃ¡c mÃ´n khÃ¡c)"
            elif data_type == "tuition" and isinstance(api_data, list):
                 try:
                     from ..chatbot_logic.student_api_handler import _format_tuition_response # Import náº¿u cáº§n
                     response = _format_tuition_response(api_data, "overview", query)
                 except ImportError:
                     logger.error("Fallback function _format_tuition_response not found.")
                     response = f"MÃ¬nh gáº·p khÃ³ khÄƒn khi xá»­ lÃ½ thÃ´ng tin há»c phÃ­ cá»§a {display_name}."
                 except Exception as fmt_err:
                     logger.error(f"Error in fallback _format_tuition_response: {fmt_err}")
                     response = f"MÃ¬nh gáº·p khÃ³ khÄƒn khi tÃ³m táº¯t há»c phÃ­ cá»§a {display_name}."
            
            # *** FALLBACK CHO CURRICULUM ***
            elif data_type == "curriculum":
                 response = f"ChÃ o {display_name}, mÃ¬nh Ä‘Ã£ táº£i Ä‘Æ°á»£c chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o cá»§a cáº­u nhÆ°ng gáº·p lá»—i khi phÃ¢n tÃ­ch chi tiáº¿t. Cáº­u cÃ³ thá»ƒ kiá»ƒm tra trá»±c tiáº¿p trÃªn cá»•ng thÃ´ng tin sinh viÃªn nhÃ©."
            
            else:
                 response = f"MÃ¬nh Ä‘Ã£ nháº­n Ä‘Æ°á»£c dá»¯ liá»‡u nhÆ°ng gáº·p khÃ³ khÄƒn khi diá»…n giáº£i cho {display_name}. Báº¡n cÃ³ thá»ƒ há»i cá»¥ thá»ƒ hÆ¡n khÃ´ng?"
        logger.debug(f"--- DEBUG END: _generate_external_api_response ---")
        return response
    
    def _generate_student_profile_response(self, query, student_info, session_id=None):
        try:
            student_name = student_info.get('student_name', '')
            mssv = student_info.get('mssv', '')
            class_name = student_info.get('class', '')
            faculty = student_info.get('faculty', '')
            display_name = student_name.split()[-1] if student_name else 'báº¡n'
            prompt = f"""Báº¡n lÃ  ChatBDU, trá»£ lÃ½ AI thÃ¢n thiá»‡n cá»§a Äáº¡i há»c BÃ¬nh DÆ°Æ¡ng, Ä‘ang nÃ³i chuyá»‡n vá»›i má»™t sinh viÃªn.

ğŸ‘¤ THÃ”NG TIN Cá»¦A SINH VIÃŠN (Báº N):
- TÃªn: {student_name}
- MSSV: {mssv}
- Lá»›p: {class_name}
- Khoa: {faculty}

â“ CÃ‚U Há»I Cá»¦A SINH VIÃŠN: {query}

HÆ¯á»šNG DáºªN:
- PhÃ¢n tÃ­ch cÃ¢u há»i Ä‘á»ƒ xÃ¡c Ä‘á»‹nh thÃ´ng tin sinh viÃªn Ä‘ang há»i vá» CHÃNH Há»Œ.
- Chá»‰ tráº£ lá»i pháº§n thÃ´ng tin Ä‘Æ°á»£c há»i, ngáº¯n gá»n, tá»± nhiÃªn.
- VÃ­ dá»¥:
  - "tÃ´i lÃ  ai" â†’ Tráº£ Ä‘áº§y Ä‘á»§ tÃªn + MSSV + lá»›p + khoa
  - "mssv" â†’ Chá»‰ tráº£ MSSV
  - "lá»›p cá»§a tÃ´i" â†’ Chá»‰ tráº£ lá»›p
  - "khoa" â†’ Chá»‰ tráº£ khoa
  - "tÃªn tÃ´i" â†’ Chá»‰ tráº£ tÃªn
- **QUAN TRá»ŒNG Vá»€ XÆ¯NG HÃ”:** LuÃ´n xÆ°ng "mÃ¬nh" (hoáº·c "tá»›") vÃ  gá»i sinh viÃªn lÃ  "báº¡n" (hoáº·c tÃªn {display_name}).
- **TUYá»†T Äá»I KHÃ”NG:** KhÃ´ng bao giá» nÃ³i "MÃ¬nh há»c lá»›p..." (I am in class...). Pháº£i nÃ³i "Báº¡n há»c lá»›p..." (You are in class...) hoáº·c "Lá»›p cá»§a báº¡n lÃ ...".
- HÃ£y tráº£ lá»i trá»±c tiáº¿p vÃ o thÃ´ng tin, khÃ´ng cáº§n chÃ o "ChÃ o báº¡n..." náº¿u khÃ´ng cáº§n thiáº¿t.

Tráº£ lá»i:"""
            
            optimal_tokens = self.token_manager.calculate_optimal_tokens(
                len(prompt), 
                'student_profile_processing'
            )
            logger.info(f"ğŸ“ Processing student profile with {optimal_tokens} tokens")
            
            response = self._call_gemini_api_with_smart_tokens(
                prompt, 'student_profile_processing', optimal_tokens, session_id
            )
            
            if not response:
                return f"ThÃ´ng tin cá»§a báº¡n: TÃªn {student_name}, MSSV {mssv}, Lá»›p {class_name}, Khoa {faculty}."
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ Error generating student profile response: {e}")
            return f"ThÃ´ng tin cá»§a báº¡n: TÃªn {student_info.get('student_name', 'N/A')}, MSSV {student_info.get('mssv', 'N/A')}, Lá»›p {student_info.get('class', 'N/A')}, Khoa {student_info.get('faculty', 'N/A')}."
    
    def _format_schedule_for_prompt(self, daily_schedule):
        if not daily_schedule:
            return "Hiá»‡n táº¡i khÃ´ng cÃ³ lá»‹ch giáº£ng dáº¡y trong khoáº£ng thá»i gian nÃ y."
        formatted_lines = []
        sorted_dates = sorted(daily_schedule.keys())
        for date_str in sorted_dates:
            classes = daily_schedule[date_str]
            try:
                from datetime import datetime
                date_obj = datetime.strptime(date_str, '%d-%m-%Y')
                weekdays = ['Thá»© Hai', 'Thá»© Ba', 'Thá»© TÆ°', 'Thá»© NÄƒm', 'Thá»© SÃ¡u', 'Thá»© Báº£y', 'Chá»§ Nháº­t']
                weekday = weekdays[date_obj.weekday()]
                formatted_date = f"{weekday}, {date_str}"
            except:
                formatted_date = date_str
            formatted_lines.append(f"\nğŸ“… {formatted_date}:")
            sorted_classes = sorted(classes, key=lambda x: x.get('tiet_bat_dau', 0))
            for class_info in sorted_classes:
                ma_mon_hoc = class_info.get('ma_mon_hoc', '')
                ten_mon_hoc = class_info.get('ten_mon_hoc', '')
                ma_lop = class_info.get('ma_lop', '')
                ma_phong = class_info.get('ma_phong', '')
                tiet_bat_dau = class_info.get('tiet_bat_dau', '')
                so_tiet = class_info.get('so_tiet', '')
                so_luong_sv = class_info.get('so_luong_sv', '')
                class_line = f"   â€¢ {ten_mon_hoc} ({ma_mon_hoc})"
                class_line += f" - Lá»›p {ma_lop}"
                class_line += f" - PhÃ²ng {ma_phong}"
                class_line += f" - Tiáº¿t {tiet_bat_dau}"
                if so_tiet:
                    class_line += f" ({so_tiet} tiáº¿t)"
                if so_luong_sv:
                    class_line += f" - {so_luong_sv} SV"
                formatted_lines.append(class_line)
        
        return '\n'.join(formatted_lines) if formatted_lines else "KhÃ´ng cÃ³ lá»‹ch giáº£ng dáº¡y."
    
    def _get_personalized_system_prompt_for_external_api(self, student_info):        
        base_prompt = """Báº¡n lÃ  AI assistant cá»§a Äáº¡i há»c BÃ¬nh DÆ°Æ¡ng (BDU), chuyÃªn há»— trá»£ sinh viÃªn.

ğŸ¯ QUY Táº®C QUAN TRá»ŒNG (external API):
- Giá»¯ cÃ¡ch xÆ°ng hÃ´ theo pronoun_style hiá»‡n táº¡i (vÃ­ dá»¥: gá»i ngÆ°á»i dÃ¹ng lÃ  "báº¡n", tá»± xÆ°ng "tÃ´i"/"tá»›"/"mÃ¬nh").
- Tráº£ lá»i chÃ­nh xÃ¡c theo dá»¯ liá»‡u tá»« há»‡ thá»‘ng; khÃ´ng bá»‹a.
- TrÃ¬nh bÃ y tá»± nhiÃªn, dá»… Ä‘á»c; khÃ´ng Ã©p máº«u "Dáº¡ ...", khÃ´ng Ã©p cÃ¢u káº¿t thÃºc khuÃ´n máº«u.
"""
        return base_prompt
    def _get_personal_address_from_api_data(self, student_info, session_id): return 'báº¡n'
    def _post_process_external_api_response(self, response, student_info, query, session_id):
        if not response:
            return response
        response = re.sub(r'\*\*\d+\.\s*', '', response)
        response = re.sub(r'^\s*\d+\.\s*', '', response, flags=re.MULTILINE)
        response = re.sub(r'^\s*[â€¢\-\*]\s*', '', response, flags=re.MULTILINE)
        response = re.sub(r'\*\*(.*?)\*\*', r'\1', response)
        return response.strip()
    def _get_external_api_fallback_response(self, api_data, personal_address):
        student_info = api_data.get('student_info', {})
        ten_sinh_vien = student_info.get('student_name', personal_address)
        return f"""ChÃ o {personal_address}, mÃ¬nh tÃ¬m tháº¥y thÃ´ng tin tá»« há»‡ thá»‘ng cá»§a trÆ°á»ng:
ğŸ‘¤ ThÃ´ng tin cá»§a {ten_sinh_vien}:
- MSSV: {student_info.get('mssv', 'KhÃ´ng xÃ¡c Ä‘á»‹nh')}
- Lá»›p: {student_info.get('class', 'KhÃ´ng xÃ¡c Ä‘á»‹nh')}
- Khoa: {student_info.get('faculty', 'KhÃ´ng xÃ¡c Ä‘á»‹nh')}

Tuy nhiÃªn, mÃ¬nh gáº·p khÃ³ khÄƒn khi xá»­ lÃ½ cÃ¢u há»i cá»¥ thá»ƒ cá»§a báº¡n. Báº¡n cÃ³ thá»ƒ há»i láº¡i má»™t cÃ¡ch khÃ¡c khÃ´ng?"""
    def set_user_context(self, session_id: str, user_context: dict):
        print("\n" + "="*20 + " DEBUG: set_user_context " + "="*20)
        print(f"ğŸ•µï¸â€â™‚ï¸ [set_user_context] Äang cÃ i Ä‘áº·t context cho session: {session_id}")
        print(f"ğŸ•µï¸â€â™‚ï¸ [set_user_context] Dá»¯ liá»‡u context nháº­n Ä‘Æ°á»£c: {user_context}")
        if 'gender' in user_context:
            print(f"âœ… [set_user_context] TÃŒM THáº¤Y 'gender' trong context: '{user_context['gender']}'")
        else:
            print(f"âŒ [set_user_context] KHÃ”NG TÃŒM THáº¤Y 'gender' trong context!")
        print("="*60 + "\n")
        self._user_context_cache[session_id] = user_context
        logger.info(f"âœ… Set user context for session {session_id}: {user_context.get('faculty_code', 'Unknown')}")

    def _get_personalized_system_prompt(self, session_id: str = None, context: Optional[Dict] = None):
        try:
            user_context = self._user_context_cache.get(session_id, {})
            user_memory_prompt = user_context.get('preferences', {}).get('user_memory_prompt', '')
            profile = None
            if context and isinstance(context, dict):
                profile = context.get('profile')
            if session_id:
                pronouns = self._get_dynamic_pronouns(session_id)
                user_address = pronouns['user']
                bot_pronoun = pronouns['bot']
            else:
                style = prompts.PERSONAL_PRONOUNS['default']
                user_address = random.choice(style['user']).replace('{first_name}', 'báº¡n')
                bot_pronoun = random.choice(style['bot'])
            return prompts.build_personalized_system_prompt(
                user_memory_prompt, 
                user_address=[user_address], # Truyá»n vÃ o dÆ°á»›i dáº¡ng list
                bot_pronoun=[bot_pronoun],   # Truyá»n vÃ o dÆ°á»›i dáº¡ng list
                profile=profile              # Truyá»n profile náº¿u cÃ³
            )
        except Exception as e:
            logger.error(f"Error getting personalized prompt: {e}")
            return prompts.build_personalized_system_prompt() # Fallback
        
    def generate_response(self, query: str, context: Optional[Dict] = None, 
                      intent_info: Optional[Dict] = None, entities: Optional[Dict] = None,
                      session_id: str = None) -> Dict[str, Any]:
        start_time = time.time()
        print(f"\n--- ğŸš€ ADVANCED RAG GENERATION REQUEST (Session: {session_id}) ---")
        print(f"ğŸ§  MEMORY DEBUG: Total active sessions = {len(self.memory.conversations)}")
        try:
            original_query = query
            # Bá» comment dÃ²ng dÆ°á»›i Ä‘á»ƒ báº­t chá»©c nÄƒng phá»¥c há»“i dáº¥u tiáº¿ng Viá»‡t
            # if not self.vietnamese_restorer.has_vietnamese_accents(query):
            #     restored_query = self.vietnamese_restorer.restore_vietnamese_tone(query)
            #     if restored_query != query:
            #         logger.info(f"ğŸ¯ Query restored: '{query}' -> '{restored_query}'")
            #         query = restored_query
            
            instruction = context.get('instruction', '') if context else ''
            
            if instruction == 'summarize_news':
                logger.info("ğŸ“° NEWS SUMMARY: Processing news summary request")
                prompt = self._build_news_summary_prompt(query, context, session_id)
                optimal_tokens = self.token_manager.calculate_optimal_tokens(
                    len(prompt), 
                    'balanced' # Sá»­a hint thÃ nh 'balanced' Ä‘á»ƒ phÃ¹ há»£p vá»›i tÃ³m táº¯t tá»•ng quan
                )
                optimal_tokens = max(optimal_tokens, 600)
                response = self._call_gemini_api_with_smart_tokens(
                    prompt, 'balanced', optimal_tokens, session_id
                )
                if not response:
                    response = "MÃ¬nh gáº·p chÃºt khÃ³ khÄƒn khi tÃ³m táº¯t tin tá»©c, báº¡n thá»­ láº¡i sau nhÃ©."
                return {
                    'response': response,
                    'method': 'gemini_news_summary',
                    'strategy': 'summarize_news',
                    'confidence': 0.9,
                    'generation_time': time.time() - start_time,
                }
            
            if instruction == 'answer_from_document':
                logger.info("ğŸ“„ DOCUMENT CONTEXT: Processing document-based query")
                document_text = context.get('document_text', '')
                if not document_text or not document_text.strip():
                    logger.warning("âš ï¸ Empty document text provided")
                    personal_address = self._get_personal_address(session_id)
                    response_confidence = self.confidence_manager.normalize_confidence(0.1, "document_error")
                    return {
                        'response': f"ChÃ o cáº­u, tá»› khÃ´ng nháº­n Ä‘Æ°á»£c ná»™i dung tÃ i liá»‡u Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i. Cáº­u cÃ³ thá»ƒ gá»­i láº¡i tÃ i liá»‡u Ä‘Æ°á»£c khÃ´ng? ğŸ“",
                        'method': 'document_context_empty',
                        'strategy': 'document_error',
                        'confidence': response_confidence,  # ğŸ›¡ï¸ CAPPED
                        'generation_time': time.time() - start_time,
                        'original_query': original_query,
                        'restored_query': query,
                        'vietnamese_restoration_used': query != original_query,
                        'personalized': bool(session_id in self._user_context_cache),
                        'document_context_processed': True,
                        'token_info': {'smart_tokens_used': False, 'method': 'document_error'}
                    }
                prompt = self._build_document_context_prompt(query, document_text, session_id)
                optimal_tokens = self.token_manager.calculate_optimal_tokens(
                    len(prompt), 
                    'document_context'
                )
                logger.info(f"ğŸ“„ Processing document context with {optimal_tokens} tokens")
                response = self._call_gemini_api_with_smart_tokens(
                    prompt, 'document_context', optimal_tokens, session_id
                )
                if not response:
                    personal_address = self._get_personal_address(session_id)
                    response = f"ChÃ o cáº­u, tá»› gáº·p khÃ³ khÄƒn ká»¹ thuáº­t khi phÃ¢n tÃ­ch tÃ i liá»‡u. Cáº­u cÃ³ thá»ƒ thá»­ láº¡i hoáº·c Ä‘áº·t cÃ¢u há»i cá»¥ thá»ƒ hÆ¡n Ä‘Æ°á»£c khÃ´ng?"
                response_confidence = self.confidence_manager.calculate_response_confidence(
                    semantic_score=0.85,
                    keyword_score=0.0,
                    context_bonus=0.1,
                    method='document_context'
                )
                if session_id:
                    self.memory.add_interaction(session_id, original_query, response, intent_info, entities)
                return {
                    'response': response,
                    'method': 'document_context_processing',
                    'strategy': 'document_context',
                    'confidence': response_confidence,
                    'generation_time': time.time() - start_time,
                    'original_query': original_query,
                    'restored_query': query,
                    'vietnamese_restoration_used': query != original_query,
                    'personalized': bool(session_id in self._user_context_cache),
                    'document_context_processed': True,
                    'token_info': {
                        'smart_tokens_used': True,
                        'method': 'document_context_processing',
                        'optimal_tokens': optimal_tokens
                    }
                }
            if instruction == 'process_external_api_data':
                logger.debug(f"--- DEBUG START: generate_response (process_external_api_data) ---")
                logger.debug(f"Received context: {json.dumps(context, ensure_ascii=False, indent=2)}")
                
                response = self._generate_external_api_response(query, context, session_id)
                response_confidence = self.confidence_manager.calculate_response_confidence(
                    semantic_score=0.9,
                    keyword_score=0.0,
                    context_bonus=0.15,
                    method='external_api'
                )
                token_info = {
                    'smart_tokens_used': True,
                    'method': 'external_api_processing'
                }
                if session_id:
                    self.memory.add_interaction(session_id, original_query, response, intent_info, entities)
                logger.debug(f"--- DEBUG END: generate_response (process_external_api_data) ---")
                return {
                    'response': response,
                    'method': 'external_api_processing',
                    'strategy': 'external_api',
                    'confidence': response_confidence,
                    'generation_time': time.time() - start_time,
                    'original_query': original_query,
                    'restored_query': query,
                    'vietnamese_restoration_used': query != original_query,
                    'personalized': bool(session_id in self._user_context_cache),
                    'external_api_processed': True,
                    'token_info': token_info
                }
            conversation_context = {}
            if session_id:
                conversation_context = self.memory.get_conversation_context(session_id)
                print(f"ğŸ§  MEMORY DEBUG: History length = {len(conversation_context.get('history', []))}")
                print(f"ğŸ“ CONTEXT SUMMARY: {conversation_context.get('recent_conversation_summary', 'None')}")
            user_context = None
            if session_id and session_id in self._user_context_cache:
                user_context = self._user_context_cache[session_id]
                print(f"ğŸ‘¤ USER CONTEXT: {user_context.get('faculty_code', 'Unknown')}")
            response_strategy = 'enhanced_generation'
            raw_confidence = context.get('confidence', 0.5) if context else 0.5
            normalized_confidence = self.confidence_manager.normalize_confidence(raw_confidence, "input_context")
            if context:
                context['confidence'] = normalized_confidence
            instruction = context.get('instruction', '') if context else ''
            if instruction == 'direct_answer_student':
                response, token_info = self._generate_direct_answer_smart(query, context, session_id)
                final_confidence = normalized_confidence
            elif instruction in ['enhance_answer', 'enhance_answer_boosted']:
                response_strategy = 'enhanced_generation' 
                response, token_info = self._generate_smart_response(query, context, session_id, response_strategy) 
                final_confidence = self.confidence_manager.normalize_confidence(normalized_confidence + 0.05, "enhanced_method")
            elif instruction == 'clarification_needed':
                response, token_info = self._generate_clarification_request_smart(query, context, session_id)
                final_confidence = self.confidence_manager.normalize_confidence(0.3, "clarification")
            elif instruction == 'dont_know':
                response, token_info = self._generate_dont_know_response_smart(query, context, session_id)
                final_confidence = self.confidence_manager.normalize_confidence(0.1, "dont_know")
            else:
                response, token_info = self._generate_smart_response(query, context, session_id, response_strategy)
                semantic_score = context.get('semantic_score', 0.5) if context else 0.5
                keyword_score = context.get('keyword_score', 0.0) if context else 0.0
                if context and context.get('emergency_education', False):
                    print(f"ğŸš¨ GEMINI: Emergency education mode activated")
                    pass
                if not self._is_education_related(query) and not context.get('force_education_response', False):
                    response = self._get_contextual_out_of_scope_response(conversation_context, session_id)
                    token_info = {'smart_tokens_used': False, 'method': 'predefined_template'}
                    final_confidence = self.confidence_manager.normalize_confidence(0.9, "out_of_scope")
                    if session_id:
                        self.memory.add_interaction(session_id, original_query, response, intent_info, entities)
                    return {
                        'response': response,
                        'method': 'out_of_scope',
                        'confidence': final_confidence,
                        'generation_time': time.time() - start_time,
                        'original_query': original_query,
                        'restored_query': query,
                        'personalized': session_id in self._user_context_cache,
                        'token_info': token_info
                    }
                
                final_confidence = self.confidence_manager.calculate_response_confidence(
                    semantic_score=semantic_score,
                    keyword_score=keyword_score,
                    context_bonus=0.05 if conversation_context.get('recent_conversation_summary') else 0.0,
                    method='two_stage_reranking' if context and context.get('two_stage_reranking_used') else 'hybrid'
                )
            final_response = response or self._get_smart_fallback_with_context(query, intent_info, conversation_context, session_id)
            if not 'final_confidence' in locals():
                final_confidence = self.confidence_manager.normalize_confidence(normalized_confidence, "final_response")
            if session_id:
                print(f"ğŸ§  MEMORY DEBUG: Saving interaction to memory...")
                self.memory.add_interaction(session_id, original_query, final_response, intent_info, entities)
            return {
                'response': final_response,
                'method': f'advanced_rag_student_aware_gemini_{response_strategy}',
                'strategy': response_strategy,
                'conversation_context': conversation_context,
                'confidence': final_confidence,
                'generation_time': time.time() - start_time,
                'original_query': original_query,
                'restored_query': query,
                'vietnamese_restoration_used': query != original_query,
                'personalized': bool(user_context),
                'enhanced_generation': response_strategy == 'enhanced_generation',
                'token_info': token_info,
                'confidence_management': {
                    'raw_confidence': raw_confidence,
                    'normalized_confidence': normalized_confidence,
                    'final_confidence': final_confidence,
                    'confidence_capped': final_confidence == 1.0,
                    'confidence_source': 'advanced_calculation'
                }
            }
        except Exception as e:
            logger.error(f"Gemini API error: {str(e)}")
            fallback_response = self._get_smart_fallback_with_context(query, intent_info, conversation_context, session_id)
            error_confidence = self.confidence_manager.normalize_confidence(0.1, "error_fallback")
            if session_id:
                self.memory.add_interaction(session_id, original_query, fallback_response, intent_info, entities)
            return {
                'response': fallback_response,
                'method': 'student_context_aware_fallback',
                'error': str(e),
                'confidence': error_confidence,
                'generation_time': time.time() - start_time,
                'original_query': original_query,
                'restored_query': query,
                'personalized': session_id in self._user_context_cache,
                'token_info': {'smart_tokens_used': False, 'method': 'fallback'}
            }
    
    def _build_document_context_prompt(self, query: str, document_text: str, session_id: str = None) -> str:
        system_prompt = self._get_personalized_system_prompt(session_id)
        personal_address = self._get_personal_address(session_id)
        
        conversation_context = self.memory.get_conversation_context(session_id) if session_id else {}
        recent_summary = conversation_context.get('recent_conversation_summary', '')
        
        context_section = ""
        if recent_summary:
            context_section = f"""
    ğŸ—£ï¸ NGá»® Cáº¢NH Há»˜I THOáº I Gáº¦N ÄÃ‚Y (Ä‘á»ƒ tham kháº£o):
    {recent_summary}
    """
        task_instruction = "Tráº£ lá»i cÃ¢u há»i cá»§a sinh viÃªn má»™t cÃ¡ch chi tiáº¿t vÃ  chÃ­nh xÃ¡c dá»±a trÃªn tÃ i liá»‡u." # Nhiá»‡m vá»¥ máº·c Ä‘á»‹nh
        query_lower = query.lower()
        counting_keywords = ['bao nhiÃªu', 'cÃ³ máº¥y', 'Ä‘áº¿m', 'sá»‘ lÆ°á»£ng', 'liá»‡t kÃª']
        
        if any(keyword in query_lower for keyword in counting_keywords):
            logger.info("ğŸ“„ Detected a counting/listing query. Building a specialized prompt.")
            task_instruction = """
    Thá»±c hiá»‡n nhiá»‡m vá»¥ Äáº¾M hoáº·c LIá»†T KÃŠ. HÃ£y Ä‘á»c ká»¹ TOÃ€N Bá»˜ tÃ i liá»‡u vÃ  tÃ¬m táº¥t cáº£ cÃ¡c má»¥c Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘ thá»© tá»± (vÃ­ dá»¥: 1., 2., 3., ...) hoáº·c cÃ¡c Ä‘á» má»¥c, Ä‘á» tÃ i riÃªng biá»‡t. Sau Ä‘Ã³, Ä‘áº¿m tá»•ng sá»‘ lÆ°á»£ng cÃ¡c má»¥c Ä‘Ã³ vÃ  tráº£ lá»i tháº³ng vÃ o cÃ¢u há»i cá»§a sinh viÃªn.
    """

        max_doc_length = 10000
        if len(document_text) > max_doc_length:
            document_text = document_text[:max_doc_length] + "\n\n[...tÃ i liá»‡u cÃ²n tiáº¿p...]"

        prompt = f"""{system_prompt}

    ğŸ¯ **NHIá»†M Vá»¤ Cá»¤ THá»‚:** {task_instruction}

    ---
    ğŸ“„ **Ná»˜I DUNG TÃ€I LIá»†U Äá»‚ PHÃ‚N TÃCH:**
    {document_text}
    ---

    {context_section}

    â“ **CÃ‚U Há»I Tá»ª SINH VIÃŠN:** "{query}"

    ğŸ“ **YÃŠU Cáº¦U Báº®T BUá»˜C KHI TRáº¢ Lá»œI:**
    1.  **Táº­p trung vÃ o Nhiá»‡m Vá»¥:** LuÃ´n tuÃ¢n thá»§ "NHIá»†M Vá»¤ Cá»¤ THá»‚" Ä‘Ã£ nÃªu á»Ÿ trÃªn.
    2.  **Nguá»“n Duy Nháº¥t:** CHá»ˆ Ä‘Æ°á»£c phÃ©p sá»­ dá»¥ng "Ná»˜I DUNG TÃ€I LIá»†U" Ä‘á»ƒ tráº£ lá»i. NghiÃªm cáº¥m bá»‹a Ä‘áº·t hoáº·c dÃ¹ng kiáº¿n thá»©c ngoÃ i.
    3.  **Tráº£ Lá»i Tháº³ng:** Äi tháº³ng vÃ o cÃ¢u tráº£ lá»i, khÃ´ng cáº§n chÃ o há»i láº¡i.
    4.  **TrÆ°á»ng Há»£p Báº¥t Kháº£ KhÃ¡ng:** Náº¿u sau khi Ä‘Ã£ Ä‘á»c ká»¹ mÃ  tÃ i liá»‡u thá»±c sá»± khÃ´ng chá»©a thÃ´ng tin Ä‘á»ƒ hoÃ n thÃ nh nhiá»‡m vá»¥, hÃ£y nÃ³i rÃµ: "Trong tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p, mÃ¬nh khÃ´ng tÃ¬m tháº¥y thÃ´ng tin Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y."

    **CÃ¢u tráº£ lá»i cá»§a báº¡n:**"""
        return prompt
        
    def _build_news_summary_prompt(self, query: str, context: Dict, session_id: str) -> str:
        profile = context.get('profile') if isinstance(context, dict) else None
        student_name = profile.get('full_name', 'báº¡n').split()[-1] if profile and profile.get('full_name') else 'báº¡n'  # Láº¥y tÃªn cuá»‘i
        news_articles = context.get("news_data", [])
        logger.info(f"ğŸ“° DEBUG: Gemini received {len(news_articles)} news articles")
        if news_articles:
            logger.info(f"ğŸ“° DEBUG: First article in Gemini: {news_articles[0]}")
            gemini_titles = [article.get('tieu_de', article.get('title', 'NO_TITLE')) for article in news_articles[:5]]
            logger.info(f"ğŸ“° DEBUG: First 5 titles in Gemini: {gemini_titles}")
        if not news_articles:
            return f"ChÃ o {student_name}, hiá»‡n táº¡i mÃ¬nh chÆ°a tÃ¬m tháº¥y thÃ´ng bÃ¡o nÃ o má»›i cáº£."
        titles = [(n.get("title") or "").strip() for n in news_articles]
        valid_titles = [t for t in titles if t]
        if not valid_titles:
            return f"ChÃ o {student_name}, khÃ´ng cÃ³ tin tá»©c há»£p lá»‡ Ä‘á»ƒ tÃ³m táº¯t."

        news_titles = [f"- {article.get('title', 'KhÃ´ng cÃ³ tiÃªu Ä‘á»')}" for article in news_articles]
        news_titles_str = "\n".join(news_titles)
        prompt = f"""ğŸ¯ NHIá»†M Vá»¤: TÃ“M Táº®T CHá»¦ Äá»€ CHÃNH Tá»ª TIÃŠU Äá»€ TIN Tá»¨C.

        DANH SÃCH TIÃŠU Äá»€ Cáº¦N XEM XÃ‰T:
        {news_titles_str}

        ğŸ“ YÃŠU Cáº¦U:
        1.  **Äá»c ká»¹ cÃ¡c tiÃªu Ä‘á»** Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c nhÃ³m chá»§ Ä‘á» chÃ­nh (vÃ­ dá»¥: thi cá»­, Ä‘Äƒng kÃ½ há»c pháº§n, thÃ´ng bÃ¡o chung).
        2.  **Viáº¿t má»™t Ä‘oáº¡n tÃ³m táº¯t ngáº¯n gá»n (khoáº£ng 2-3 cÃ¢u)** nÃªu báº­t cÃ¡c nhÃ³m chá»§ Ä‘á» nÃ y. Báº¯t Ä‘áº§u báº±ng cÃ¡ch chÃ o tÃªn sinh viÃªn (vÃ­ dá»¥: "ChÃ o {student_name}, mÃ¬nh tháº¥y cÃ³...").
        3.  **Tuyá»‡t Ä‘á»‘i khÃ´ng Ä‘i vÃ o chi tiáº¿t** cá»§a báº¥t ká»³ tin tá»©c nÃ o.
        4.  **Káº¿t thÃºc báº±ng má»™t cÃ¢u há»i má»Ÿ** Ä‘á»ƒ gá»£i Ã½ sinh viÃªn há»i thÃªm. VÃ­ dá»¥: "Báº¡n muá»‘n xem chi tiáº¿t vá» pháº§n nÃ o khÃ´ng?".
        5.  **Quan trá»ng:** Chá»‰ táº­p trung vÃ o viá»‡c tÃ³m táº¯t cÃ¡c chá»§ Ä‘á» tá»« danh sÃ¡ch tiÃªu Ä‘á» Ä‘Æ°á»£c cung cáº¥p.

        CÃ¢u tráº£ lá»i cá»§a báº¡n:
        """
        return prompt

    def _build_detailed_news_prompt(self, query: str, context: Dict, session_id: str) -> str:
        profile = context.get('profile') if isinstance(context, dict) else None
        student_name = profile.get('full_name', 'báº¡n').split()[-1] if profile and profile.get('full_name') else 'báº¡n'
        news_articles_from_memory = context.get("news_data_from_memory", [])
        if not news_articles_from_memory and session_id:
            conversation_context = self.memory.get_conversation_context(session_id)
            recent_history = conversation_context.get('history', [])
            if recent_history:
                last_interaction = recent_history[-1]
                intent_info = last_interaction.get('intent_info', {})
                news_articles_from_memory = intent_info.get('news_context', [])
        
        news_content_section = json.dumps(news_articles_from_memory, ensure_ascii=False, indent=2)
        prompt = f"""ğŸ¯ NHIá»†M Vá»¤: TÃŒM VÃ€ TRáº¢ Lá»œI CHI TIáº¾T Vá»€ TIN Tá»¨C Cá»¤ THá»‚.

    TOÃ€N Bá»˜ Dá»® LIá»†U TIN Tá»¨C Äá»‚ TRA Cá»¨U:
    {news_content_section}

    â“ CÃ‚U Há»I Cá»¤ THá»‚ Cá»¦A SINH VIÃŠN: "{query}"

    ğŸ“ YÃŠU Cáº¦U:
    1. **XÃ¡c Ä‘á»‹nh chá»§ Ä‘á»:** Äá»c cÃ¢u há»i cá»§a sinh viÃªn Ä‘á»ƒ hiá»ƒu há» muá»‘n biáº¿t vá» tin tá»©c nÃ o (vÃ­ dá»¥: "lá»‹ch thi", "GDQP", "kháº£o sÃ¡t").
    2. **TÃ¬m kiáº¿m chÃ­nh xÃ¡c:** Duyá»‡t qua "TOÃ€N Bá»˜ Dá»® LIá»†U TIN Tá»¨C" Ä‘á»ƒ tÃ¬m (cÃ¡c) bÃ i viáº¿t phÃ¹ há»£p nháº¥t vá»›i chá»§ Ä‘á» Ä‘Ã³.
    3. **TÃ³m táº¯t chi tiáº¿t:** TrÃ­ch xuáº¥t vÃ  trÃ¬nh bÃ y láº¡i nhá»¯ng thÃ´ng tin quan trá»ng nháº¥t tá»« bÃ i viáº¿t Ä‘Ã£ tÃ¬m tháº¥y, Ä‘áº·c biá»‡t lÃ  cÃ¡c má»‘c thá»i gian, Ä‘á»‹a Ä‘iá»ƒm, vÃ  hÆ°á»›ng dáº«n.
    4. **Náº¿u khÃ´ng tÃ¬m tháº¥y:** Tráº£ lá»i má»™t cÃ¡ch lá»‹ch sá»±, vÃ­ dá»¥: "Trong cÃ¡c thÃ´ng bÃ¡o gáº§n Ä‘Ã¢y, mÃ¬nh khÃ´ng tháº¥y cÃ³ tin nÃ o nÃ³i vá» [chá»§ Ä‘á»] cáº£. Báº¡n cÃ³ muá»‘n há»i vá» cÃ¡i khÃ¡c khÃ´ng?".

    CÃ¢u tráº£ lá»i chi tiáº¿t cá»§a báº¡n:
    """
        return prompt

    def _build_completion_prompt(self, incomplete_response: str, original_query: str, context, session_id: str, completion_info: Dict) -> str:        
        system_prompt = self._get_personalized_system_prompt(session_id, context)
        personal_address = self._get_personal_address(session_id)
        
        if completion_info['reason'] == 'incomplete_pattern':
            completion_prompt = f"""
            {system_prompt}
            
            NHIá»†M Vá»¤: HOÃ€N THIá»†N cÃ¢u tráº£ lá»i bá»‹ cáº¯t
            
            CÃ‚U Há»I Gá»C: {original_query}
            
            CÃ‚U TRáº¢ Lá»œI Bá»Š Cáº®T:
            {incomplete_response}
            
            YÃŠU Cáº¦U:
            - TIáº¾P Tá»¤C viáº¿t Ä‘á»ƒ hoÃ n thiá»‡n cÃ¢u tráº£ lá»i
            - Giá»¯ giá»ng Ä‘iá»‡u tá»± nhiÃªn, lá»‹ch sá»±; khÃ´ng Ã©p máº«u chÃ o/káº¿t thÃºc
            - CHá»ˆ VIáº¾T PHáº¦N TIáº¾P THEO, khÃ´ng láº·p láº¡i pháº§n Ä‘Ã£ cÃ³
            
            Tiáº¿p tá»¥c:"""
        else:
            completion_prompt = f"""
            {system_prompt}
            
            NHIá»†M Vá»¤: Sá»¬A Lá»–I vÃ  hoÃ n thiá»‡n cÃ¢u tráº£ lá»i
            
            CÃ‚U Há»I Gá»C: {original_query}
            
            CÃ‚U TRáº¢ Lá»œI CÃ“ Váº¤N Äá»€:
            {incomplete_response}
            
            Váº¤N Äá»€ PHÃT HIá»†N: {completion_info['reason']}
            
            YÃŠU Cáº¦U:
            - Sá»¬A Lá»–I vÃ  viáº¿t láº¡i cÃ¢u tráº£ lá»i HOÃ€N CHá»ˆNH
            - Giá»¯ xÆ°ng hÃ´ theo pronoun_style hiá»‡n táº¡i (ngÆ°á»i dÃ¹ng="báº¡n", bot="tÃ´i"/"tá»›"/"mÃ¬nh")
            - KhÃ´ng Ã©p máº«u chÃ o "Dáº¡ ..." hay cÃ¢u káº¿t thÃºc khuÃ´n máº«u
            
            CÃ¢u tráº£ lá»i hoÃ n chá»‰nh:"""
        return completion_prompt

    def _build_api_data_prompt(self, api_data: dict, query: str, data_type: str = "general", profile: Optional[Dict] = None) -> str:
        logger.debug(f"--- DEBUG START: _build_api_data_prompt ---")
        logger.debug(f"Data Type: {data_type}")
        logger.debug(f"Profile received: {json.dumps(profile, ensure_ascii=False, indent=2)}")

        if not api_data:
            logger.debug(f"API data is empty. Returning empty prompt section.")
            logger.debug(f"--- DEBUG END: _build_api_data_prompt ---")
            return ""
        
        student_name = "báº¡n"
        student_mssv = "chÆ°a rÃµ"
        student_class = "chÆ°a rÃµ"
        student_faculty = "chÆ°a rÃµ"
        if profile:
            student_name = profile.get('full_name') or profile.get('name') or student_name
            student_mssv = profile.get('mssv') or student_mssv
            student_class = profile.get('class_name') or profile.get('class') or student_class
            student_faculty = profile.get('faculty') or student_faculty

        student_info_header = f"""
    ---
    ğŸ‘¤ THÃ”NG TIN SINH VIÃŠN ÄANG Há»I (Sá»­ dá»¥ng thÃ´ng tin nÃ y, khÃ´ng há»i láº¡i):
    - TÃªn: {student_name}
    - MSSV: {student_mssv}
    - Lá»›p: {student_class}
    - Khoa: {student_faculty}
    ---
    """     
        if data_type == "grades":
            prompt_section = f"""
    ğŸ“Š Dá»® LIá»†U ÄIá»‚M Sá» SINH VIÃŠN:
    - Äiá»ƒm trung bÃ¬nh há»‡ 4: {api_data.get('avg_diem_hp_4', 'N/A')}
    - Äiá»ƒm trung bÃ¬nh há»‡ 10: {api_data.get('avg_diem_hp', 'N/A')}
    - Xáº¿p loáº¡i há»c lá»±c: {api_data.get('xep_loai', 'N/A')}
    - Sá»‘ tÃ­n chá»‰ Ä‘Ã£ tÃ­ch lÅ©y: {api_data.get('so_tin_chi_da_tich_luy', 'N/A')}

    Dá»±a vÃ o dá»¯ liá»‡u Ä‘iá»ƒm sá»‘ trÃªn, hÃ£y tráº£ lá»i cÃ¢u há»i cá»§a sinh viÃªn má»™t cÃ¡ch tá»± nhiÃªn vÃ  tÃ­ch cá»±c: "{query}"
    """
        elif data_type == "schedule":
            prompt_section = f"""
    ğŸ“… Dá»® LIá»†U Lá»ŠCH Há»ŒC:
    {json.dumps(api_data, ensure_ascii=False, indent=2)}

    Dá»±a vÃ o lá»‹ch há»c trÃªn, hÃ£y tráº£ lá»i cÃ¢u há»i cá»§a sinh viÃªn: "{query}"
    """
        elif data_type == "tuition":
            prompt_section = f"""
    ğŸ’° Dá»® LIá»†U Há»ŒC PHÃ:
    {json.dumps(api_data, ensure_ascii=False, indent=2)}

    Dá»±a vÃ o thÃ´ng tin há»c phÃ­ trÃªn, hÃ£y tráº£ lá»i cÃ¢u há»i cá»§a sinh viÃªn: "{query}"
    """     
        elif data_type == "curriculum":
            curriculum_tree = api_data.get("curriculum_tree", [])
            credit_summary = api_data.get("credit_summary", {})
            tree_summary = []
            for khoi in curriculum_tree[:2]: # Chá»‰ láº¥y 2 khá»‘i Ä‘áº§u
                khoi_summary = {
                    "khoi_kien_thuc": khoi.get("khoi_kien_thuc"),
                    "so_nhom_hoc": len(khoi.get("nhom_hoc", [])),
                    "nhom_hoc_sample": []
                }
                for nhom in khoi.get("nhom_hoc", [])[:2]: # Chá»‰ láº¥y 2 nhÃ³m Ä‘áº§u
                    nhom_summary = {
                        "nhom_mon_hoc": nhom.get("nhom_mon_hoc"),
                        "tin_chi_yeu_cau": nhom.get("tin_chi_yeu_cau"),
                        "so_mon_hoc": len(nhom.get("danh_sach_mon_hoc", [])),
                        "mon_hoc_dat_sample": [
                            s.get("ten_mon_hoc") for s in nhom.get("danh_sach_mon_hoc", [])
                            if s.get("trang_thai") == "Äáº¡t"
                        ][:2], # Láº¥y 2 mÃ´n Ä‘Ã£ Ä‘áº¡t
                        "mon_hoc_dang_hoc_sample": [
                            s.get("ten_mon_hoc") for s in nhom.get("danh_sach_mon_hoc", [])
                            if s.get("trang_thai") == "Äang há»c"
                        ][:2] # Láº¥y 2 mÃ´n Ä‘ang há»c
                    }
                    khoi_summary["nhom_hoc_sample"].append(nhom_summary)
                tree_summary.append(khoi_summary)
            prompt_section = student_info_header + f"""
    ğŸ“ˆ Dá»® LIá»†U TIáº¾N Äá»˜ Há»ŒC Táº¬P (CURRICULUM) Cá»¦A SINH VIÃŠN NÃ€Y (TÃªn: {student_name}):

    1.  **Dá»® LIá»†U TÃN CHá»ˆ Tá»”NG QUAN:**
    ```json
    {json.dumps(credit_summary, ensure_ascii=False, indent=2)}
    ```

    2.  **Dá»® LIá»†U CÃ‚Y CHÆ¯Æ NG TRÃŒNH ÄÃ€O Táº O Äáº¦Y Äá»¦ (JSON):**
    ```json
    {json.dumps(curriculum_tree, ensure_ascii=False, indent=2)}
    ```
    *LÆ°u Ã½ cáº¥u trÃºc JSON:* Dá»¯ liá»‡u lÃ  má»™t list cÃ¡c Khá»‘i kiáº¿n thá»©c. Má»—i khá»‘i chá»©a list cÃ¡c NhÃ³m há»c. Má»—i nhÃ³m há»c chá»©a list cÃ¡c MÃ´n há»c (vá»›i cÃ¡c trÆ°á»ng ten_mon_hoc, so_tin_chi, trang_thai).

    ---
    **NHIá»†M Vá»¤:** PhÃ¢n tÃ­ch dá»¯ liá»‡u trÃªn Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a sinh viÃªn `{student_name}` (MSSV: {student_mssv}).
    **CÃ‚U Há»I:** "{query}"

    **HÆ¯á»šNG DáºªN PHÃ‚N TÃCH VÃ€ TRáº¢ Lá»œI (TUYá»†T Äá»I KHÃ”NG Há»I Láº I THÃ”NG TIN SV):**

    * **XÃC Äá»ŠNH YÃŠU Cáº¦U:** Äá»c ká»¹ cÃ¢u há»i `{query}` Ä‘á»ƒ biáº¿t sinh viÃªn muá»‘n biáº¿t thÃ´ng tin gÃ¬:
    * **Loáº¡i 1 (Tá»•ng quan):** Há»i vá» tiáº¿n Ä‘á»™ chung, tá»•ng tÃ­n chá»‰ ("tiáº¿n Ä‘á»™ há»c táº­p", "há»c Ä‘Æ°á»£c bao nhiÃªu", "cÃ²n bao nhiÃªu tÃ­n chá»‰").
    * **Loáº¡i 2 (Chi tiáº¿t - MÃ´n há»c):** Há»i vá» cÃ¡c mÃ´n cá»¥ thá»ƒ ("cÃ²n thiáº¿u mÃ´n nÃ o", "cáº§n há»c mÃ´n gÃ¬", "liá»‡t kÃª mÃ´n chÆ°a há»c", "Ä‘ang há»c mÃ´n nÃ o").

    * **CÃCH TRáº¢ Lá»œI TÃ™Y THEO YÃŠU Cáº¦U:**

    * **Náº¿u lÃ  Loáº¡i 1 (Tá»•ng quan):**
        1.  ChÃ o `{student_name}`.
        2.  DÃ¹ng **Dá»® LIá»†U TÃN CHá»ˆ Tá»”NG QUAN (Má»¥c 1)** Ä‘á»ƒ tráº£ lá»i chÃ­nh. VÃ­ dá»¥: "ChÃ o Khang, cáº­u Ä‘Ã£ Ä‘áº¡t `{credit_summary.get('total_credit')}` / `{credit_summary.get('required_credit')}` tÃ­n chá»‰ yÃªu cáº§u."
        3.  *KhÃ´ng cáº§n* liá»‡t kÃª chi tiáº¿t cÃ¡c mÃ´n há»c. CÃ³ thá»ƒ tÃ³m táº¯t 1-2 khá»‘i kiáº¿n thá»©c chÃ­nh náº¿u muá»‘n.

    * **Náº¿u lÃ  Loáº¡i 2 (Chi tiáº¿t - MÃ´n há»c):**
        1.  ChÃ o `{student_name}`.
        2.  **QUAN TRá»ŒNG:** Duyá»‡t qua **TOÃ€N Bá»˜ Dá»® LIá»†U CÃ‚Y CTÄT Äáº¦Y Äá»¦ (Má»¥c 2)** theo cáº¥u trÃºc: `khoi_kien_thuc` -> `nhom_hoc` -> `danh_sach_mon_hoc`.
        3.  **Lá»ŒC MÃ”N Há»ŒC:**
            * Náº¿u há»i "mÃ´n cÃ²n thiáº¿u"/"mÃ´n chÆ°a há»c": TÃ¬m táº¥t cáº£ `mon_hoc` cÃ³ `"trang_thai": "ChÆ°a há»c"`.
            * Náº¿u há»i "mÃ´n Ä‘ang há»c": TÃ¬m táº¥t cáº£ `mon_hoc` cÃ³ `"trang_thai": "Äang há»c"`.
        4.  **TRÃŒNH BÃ€Y Káº¾T QUáº¢:**
            * Liá»‡t kÃª rÃµ rÃ ng danh sÃ¡ch cÃ¡c mÃ´n há»c Ä‘Ã£ lá»c Ä‘Æ°á»£c (dÃ¹ng `ten_mon_hoc`). CÃ³ thá»ƒ nhÃ³m theo `khoi_kien_thuc` hoáº·c `nhom_mon_hoc` cho dá»… nhÃ¬n.
            * VÃ­ dá»¥: "ChÃ o Khang, mÃ¬nh tháº¥y cáº­u cÃ²n **chÆ°a há»c** cÃ¡c mÃ´n sau:\n- Khá»‘i kiáº¿n thá»©c ABC:\n  - MÃ´n X (3 TC)\n  - MÃ´n Y (2 TC)\n- Khá»‘i kiáº¿n thá»©c XYZ:\n  - MÃ´n Z (3 TC)..."
            * Náº¿u khÃ´ng tÃ¬m tháº¥y mÃ´n nÃ o theo tiÃªu chÃ­ lá»c, hÃ£y nÃ³i rÃµ: "MÃ¬nh khÃ´ng tÃ¬m tháº¥y mÃ´n nÃ o [chÆ°a há»c/Ä‘ang há»c] trong chÆ°Æ¡ng trÃ¬nh cá»§a cáº­u."
        5.  *KhÃ´ng cáº§n* láº·p láº¡i thÃ´ng tin tá»•ng tÃ­n chá»‰ náº¿u khÃ´ng Ä‘Æ°á»£c há»i trá»±c tiáº¿p.

    ---
    **TRáº¢ Lá»œI (Báº¯t Ä‘áº§u báº±ng cÃ¡ch chÃ o `{student_name}`):**
    """
        else: # data_type general
            prompt_section = student_info_header + f"""
    ğŸ“‹ Dá»® LIá»†U Tá»ª Há»† THá»NG:
    {json.dumps(api_data, ensure_ascii=False, indent=2)}

    Dá»±a vÃ o dá»¯ liá»‡u trÃªn VÃ€ thÃ´ng tin sinh viÃªn á»Ÿ Ä‘áº§u, hÃ£y tráº£ lá»i cÃ¢u há»i cá»§a sinh viÃªn má»™t cÃ¡ch tá»± nhiÃªn: "{query}"
    (Nhá»› chÃ o tÃªn sinh viÃªn!)
    """     
        logger.debug(f"Generated prompt_section (first 300 chars): {prompt_section[:300]}...")
        logger.debug(f"--- DEBUG END: _build_api_data_prompt ---")
        return prompt_section

    def _build_enhanced_prompt(self, query: str, context=None, intent_info=None, entities=None, session_id=None):
        logger.debug(f"--- DEBUG START: _build_enhanced_prompt (Session: {session_id}) ---")
        system_prompt = self._get_personalized_system_prompt(session_id, context)
        
        # âœ… FIX #2.1: GET PROFILE FROM CONTEXT
        profile = context.get('profile') if isinstance(context, dict) else None
        if profile:
            logger.debug(f"Profile found in context: {json.dumps(profile, ensure_ascii=False)}")
            logger.info(f"ğŸ‘¤ Profile WILL BE USED in prompt for session {session_id}")
        else:
            logger.debug(f"No profile found in context.")
        
        personal_address = self._get_personal_address(session_id)
        context_info = str(context.get('response', '')) if isinstance(context, dict) else str(context or '')
        
        # âœ… FIX #2.2: GET MEMORY CONTEXT
        memory_context = self.memory.get_conversation_context(session_id) if session_id else {}
        recent_summary = memory_context.get('recent_conversation_summary', '')
        
        # âœ… FIX #2.3: BUILD MEMORY SECTION FROM HISTORY
        memory_section = ""
        if memory_context and memory_context.get('history'):
            history = memory_context['history']
            # Láº¥y 3 cÃ¢u gáº§n nháº¥t
            recent_messages = history[-3:] if len(history) >= 3 else history
            
            if recent_messages:
                history_text = ""
                for msg in recent_messages:
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                    if role == 'user':
                        history_text += f"- Sinh viÃªn: {content[:150]}...\n"
                    else:
                        history_text += f"- ChatBDU: {content[:150]}...\n"
                
                memory_section = f"""
    ğŸ—£ï¸ NGá»® Cáº¢NH Há»˜I THOáº I (3 cÃ¢u gáº§n nháº¥t):
    {history_text}
    
    ğŸ’¡ LÆ¯U Ã: Sinh viÃªn Ä‘ang há»i tiáº¿p theo dá»±a trÃªn ngá»¯ cáº£nh trÃªn. HÃ£y tráº£ lá»i máº¡ch láº¡c, tá»± nhiÃªn, khÃ´ng láº·p láº¡i thÃ´ng tin Ä‘Ã£ nÃ³i.
    """
                logger.info(f"âœ… Memory section built with {len(recent_messages)} messages")
        
        # Context section tá»« summary (legacy support)
        context_section = ""
        if recent_summary and not memory_section:
            context_section = f"""
    ğŸ—£ï¸ NGá»® Cáº¢NH Há»˜I THOáº I Gáº¦N ÄÃ‚Y:
    {recent_summary}

    ğŸ’¡ LÆ¯U Ã: Dá»±a vÃ o ngá»¯ cáº£nh cuá»™c há»™i thoáº¡i trÃªn, hÃ£y tráº£ lá»i cÃ¢u há»i tiáº¿p theo cá»§a sinh viÃªn má»™t cÃ¡ch tá»± nhiÃªn vÃ  máº¡ch láº¡c. TrÃ¡nh láº·p láº¡i thÃ´ng tin Ä‘Ã£ tháº£o luáº­n, nhÆ°ng cÃ³ thá»ƒ tham kháº£o Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i liá»n máº¡ch.
    """
        
        # âœ… FIX #2.4: BUILD PROFILE SECTION
        profile_section = ""
        if profile:
            full_name = profile.get('full_name', '')
            mssv = profile.get('mssv', '')
            class_name = profile.get('class_name', '')
            faculty = profile.get('faculty', '')
            
            profile_section = f"""
    ğŸ‘¤ THÃ”NG TIN SINH VIÃŠN:
    - TÃªn: {full_name}
    - MSSV: {mssv}
    - Lá»›p: {class_name}
    - Khoa: {faculty}
    
    ğŸ’¡ LÆ¯U Ã: ÄÃ¢y lÃ  thÃ´ng tin cá»§a sinh viÃªn Ä‘ang há»i. HÃ£y sá»­ dá»¥ng Ä‘á»ƒ tráº£ lá»i cÃ¡ nhÃ¢n hÃ³a (gá»i tÃªn, Ä‘á» cáº­p lá»›p/khoa náº¿u phÃ¹ há»£p).
    """
        
        profile_prompt = f"\nğŸ‘¤ Profile thÃªm: {json.dumps(profile, ensure_ascii=False)}" if profile and not profile_section else ""

        api_data_section = ""
        if isinstance(context, dict) and 'api_data' in context:
            api_data = context['api_data']
            data_type = context.get('data_type', 'general')
            api_data_section = self._build_api_data_prompt(api_data, query, data_type, profile=profile)
        tutor_prompt = ""
        if isinstance(context, dict) and context.get("instruction") == "tutor_mode":
            student_data = context.get("student_data", {})
            gpa_4 = student_data.get("grades", {}).get("gpa_4", 0)
            gpa_10 = student_data.get("grades", {}).get("gpa_10", 0)
            credits_completed = student_data.get("credits", {}).get("completed_credits", 0)
            total_credits = student_data.get("credits", {}).get("total_credits", 0)
            rl_xep_loai = student_data.get("rl_grades", {}).get("xep_loai", "ChÆ°a cÃ³")
            analysis = []
            if isinstance(gpa_4, (int, float)) and gpa_4 < 3.0:
                analysis.append(f"- GPA {gpa_4} (há»‡ 4) dÆ°á»›i 3.0: Táº­p trung cáº£i thiá»‡n báº±ng cÃ¡ch phÃ¢n tÃ­ch mÃ´n yáº¿u (kiá»ƒm tra báº£ng Ä‘iá»ƒm chi tiáº¿t náº¿u cÃ³). Láº­p lá»‹ch Ã´n 2-3 giá»/ngÃ y cho tá»«ng mÃ´n, Æ°u tiÃªn cÃ´ng thá»©c toÃ¡n/láº­p trÃ¬nh náº¿u khoa CNTT.")
                analysis.append("- TÃ i liá»‡u: Sá»­ dá»¥ng Khan Academy cho toÃ¡n cÆ¡ báº£n, hoáº·c Coursera 'Learning How to Learn' miá»…n phÃ­. Theo dÃµi tiáº¿n Ä‘á»™ hÃ ng tuáº§n qua app nhÆ° Notion.")
            elif isinstance(gpa_4, (int, float)) and gpa_4 < 2.5:
                analysis.append(f"- GPA {gpa_4} tháº¥p hÆ¡n 2.5: Cáº§n há»— trá»£ ngay - gáº·p cá»‘ váº¥n há»c táº­p khoa {student_data.get('profile', {}).get('faculty', 'BDU')} Ä‘á»ƒ Ä‘Äƒng kÃ½ tutor hoáº·c khÃ³a bá»• sung.")
                analysis.append("- HÃ nh Ä‘á»™ng: Giáº£m táº£i mÃ´n há»c ká»³ sau, táº­p trung 1-2 mÃ´n chÃ­nh. Theo dÃµi sá»©c khá»e Ä‘á»ƒ trÃ¡nh kiá»‡t sá»©c.")
            
            if total_credits > 0 and credits_completed / total_credits < 0.7:
                analysis.append(f"- TÃ­n chá»‰ hoÃ n thÃ nh {credits_completed}/{total_credits} ({credits_completed/total_credits*100:.1f}%): Æ¯u tiÃªn Ä‘Äƒng kÃ½ mÃ´n dá»… Ä‘áº¡t trÆ°á»›c, trÃ¡nh overload >18 tÃ­n chá»‰/ká»³.")
            
            if rl_xep_loai != "Tá»‘t" and rl_xep_loai != "Xuáº¥t sáº¯c":
                analysis.append(f"- Äiá»ƒm rÃ¨n luyá»‡n {rl_xep_loai}: Tham gia 1-2 hoáº¡t Ä‘á»™ng ngoáº¡i khÃ³a/thÃ¡ng (cÃ¢u láº¡c bá»™ khoa CNTT) Ä‘á»ƒ tÄƒng Ä‘iá»ƒm chuyÃªn cáº§n vÃ  xÃ£ há»™i.")
            
            tutor_prompt = f"""
    ğŸ’¡ CHáº¾ Äá»˜ GIA SÆ¯: PhÃ¢n tÃ­ch data sinh viÃªn Ä‘á»ƒ tÆ° váº¥n cáº£i thiá»‡n cá»¥ thá»ƒ. Dá»±a data real: GPA {gpa_4} (há»‡ 4), {gpa_10} (há»‡ 10); TÃ­n chá»‰: {credits_completed}/{total_credits}; RÃ¨n luyá»‡n: {rl_xep_loai}.

    ğŸ“Š PHÃ‚N TÃCH VÃ€ HÆ¯á»šNG DáºªN Cáº¢I THIá»†N:
    {chr(10).join(analysis)}

    ğŸ¯ Lá»œI KHUYÃŠN CHUNG:
    - Theo dÃµi tiáº¿n Ä‘á»™: DÃ¹ng Google Sheets ghi Ä‘iá»ƒm tá»«ng bÃ i kiá»ƒm tra hÃ ng tuáº§n.
    - Náº¿u cáº§n há»— trá»£: LiÃªn há»‡ phÃ²ng ÄÃ o táº¡o BDU hoáº·c group lá»›p {student_data.get('profile', {}).get('class', '24TH01')} trÃªn Zalo.
    - KhÃ´ng bá»‹a data, giá»¯ trung thá»±c: Náº¿u data thiáº¿u, gá»£i Ã½ kiá»ƒm tra API láº¡i.

    Tráº£ lá»i ngáº¯n gá»n, kháº£ thi, káº¿t thÃºc báº±ng cÃ¢u há»i cá»¥ thá»ƒ nhÆ° "Báº¡n muá»‘n káº¿ hoáº¡ch Ã´n mÃ´n nÃ o trÆ°á»›c?".
    """
        
        prompt = f"""{system_prompt}
        
    CÃ‚U Há»I: {query}
    THÃ”NG TIN: {context_info}{profile_prompt}

    {profile_section}

    {memory_section}

    {context_section}

    {api_data_section}

    {tutor_prompt}

    YÃŠU Cáº¦U:
    - DÃ¹ng profile Ä‘á»ƒ tráº£ lá»i cÃ¡ nhÃ¢n hÃ³a (tÃªn, lá»›p, khoa).
    - DÃ¹ng memory Ä‘á»ƒ tráº£ lá»i follow-up questions.
    - KhÃ´ng láº·p 'AI assistant cá»§a BDU', giá»¯ tá»± nhiÃªn.
    - KhÃ´ng bá»‹a data.
    - Giá»¯ cÃ¡ch xÆ°ng hÃ´ thÃ¢n thiá»‡n (báº¡n-mÃ¬nh, cáº­u-tá»›).
    - Táº¡o cÃ¢u tráº£ lá»i máº¡ch láº¡c, tá»± nhiÃªn, trÃ¡nh láº·p láº¡i thÃ´ng tin Ä‘Ã£ tháº£o luáº­n.
    - Náº¿u cáº§n, cÃ³ thá»ƒ káº¿t thÃºc báº±ng má»™t cÃ¢u há»i má»Ÿ nhÆ° "Báº¡n cÃ³ cáº§n mÃ¬nh giÃºp gÃ¬ thÃªm khÃ´ng?".

    Tráº£ lá»i:"""

        logger.debug(f"Final prompt built (first 500 chars): {prompt[:500]}...")
        logger.debug(f"Final prompt built (last 300 chars): ...{prompt[-300:]}")
        logger.debug(f"--- DEBUG END: _build_enhanced_prompt ---")

        return prompt

    def _build_external_api_prompt(self, query, api_data, personal_address, recent_summary=""):        
        student_info = api_data.get('student_info', {})
        ten_sinh_vien = student_info.get('student_name', personal_address)
        mssv = student_info.get('mssv', '')
        lop = student_info.get('class', '')
        khoa = student_info.get('faculty', '')
        
        system_prompt = f"""Báº¡n lÃ  trá»£ lÃ½ AI thÃ´ng minh cá»§a trÆ°á»ng Ä‘áº¡i há»c, chuyÃªn há»— trá»£ sinh viÃªn.

    THÃ”NG TIN SINH VIÃŠN:
    - TÃªn: {ten_sinh_vien}
    - MSSV: {mssv}
    - Lá»›p: {lop}
    - Khoa: {khoa}

    HÃ£y tráº£ lá»i cÃ¢u há»i cá»§a sinh viÃªn má»™t cÃ¡ch thÃ¢n thiá»‡n, chÃ­nh xÃ¡c vÃ  há»¯u Ã­ch."""

        context_section = ""
        if recent_summary:
            context_section = f"""
    ğŸ—£ï¸ NGá»® Cáº¢NH Há»˜I THOáº I Gáº¦N ÄÃ‚Y:
    {recent_summary}
    """

        prompt = f"""{system_prompt}

    {context_section}

    ğŸ“ CÃ‚U Há»I Cá»¦A SINH VIÃŠN:
    {query}

    HÃ£y tráº£ lá»i cÃ¢u há»i má»™t cÃ¡ch tá»± nhiÃªn vÃ  há»¯u Ã­ch."""

        return prompt
    
    def _generate_direct_answer_smart(self, query, context, session_id):
        text = context.get('db_answer') or context.get('response') or context.get('fallback_response') or ''
        return text, {'smart_tokens_used': False, 'method': 'direct_passthrough'}

    def _generate_smart_response(self, query: str, context=None, session_id=None, strategy='balanced'):        
        prompt = self._build_enhanced_prompt(query, context, None, None, session_id)
        data_type = context.get('data_type', 'general') if isinstance(context, dict) else 'general'
        
        optimal_tokens = self.token_manager.calculate_optimal_tokens(
            len(prompt), 
            complexity_hint=strategy
        )
        if data_type == "curriculum":
            optimal_tokens = max(optimal_tokens, 3000) # TÄƒng lÃªn 3000 tokens
            print(f"ğŸ§  SMART TOKENS (CURRICULUM): {optimal_tokens} tokens")
        else:
            print(f"ğŸ§  SMART TOKENS: {optimal_tokens} tokens")
        response = self._call_gemini_api_with_smart_tokens(prompt, strategy, optimal_tokens, session_id)
        if not response:
            return self._get_smart_fallback_with_context(query, None, {}, session_id), {
                'smart_tokens_used': True, 'method': 'fallback_after_api_failure', 'tokens_attempted': optimal_tokens
            }
        completion_check = self.token_manager.is_response_incomplete(response)
        if completion_check['incomplete']:
            print(f"âš ï¸ INCOMPLETE RESPONSE detected: {completion_check['reason']}")
            completed_response = self._auto_complete_response(response, query, context, session_id, completion_check)
            if completed_response and completed_response != response:
                response = completed_response
                completion_check['auto_completed'] = True
                print(f"âœ… AUTO-COMPLETION successful")
            else:
                print(f"âš ï¸ AUTO-COMPLETION failed, using original")
        response = self._post_process_response(response, query, context, strategy, {}, session_id)
        token_info = {
            'smart_tokens_used': True,
            'method': 'smart_generation',
            'optimal_tokens': optimal_tokens,
            'completion_check': completion_check,
            'strategy': strategy
        }
        return response, token_info
    def _auto_complete_response(self, incomplete_response: str, original_query: str, context, session_id: str, completion_info: Dict) -> Optional[str]:        
        if completion_info['confidence'] < 0.6:
            return None
        completion_tokens = self.token_manager.estimate_completion_tokens(incomplete_response)
        completion_prompt = self._build_completion_prompt(incomplete_response, original_query, context, session_id, completion_info)
        print(f"ğŸ”§ AUTO-COMPLETION: Attempting with {completion_tokens} tokens")
        completion = self._call_gemini_api_with_smart_tokens(completion_prompt, 'completion', completion_tokens, session_id)
        if completion:
            merged = self._merge_incomplete_and_completion(incomplete_response, completion)
            return merged
        return None
    
    def _merge_incomplete_and_completion(self, incomplete: str, completion: str) -> str:
        completion = completion.strip()
        completion = re.sub(r'^(dáº¡\s+(tháº§y|cÃ´|sinh viÃªn),?\s*)', '', completion, flags=re.IGNORECASE)
        incomplete_words = incomplete.split()
        if incomplete_words:
            last_word = incomplete_words[-1].lower()
            if last_word in ['vÃ ', 'vá»›i', 'Ä‘á»ƒ', 'khi', 'náº¿u', 'táº¡i', 'vá»', 'cho', 'trong', 'cá»§a', 'tá»«']:
                incomplete = ' '.join(incomplete_words[:-1])
        merged = incomplete.rstrip() + ' ' + completion.lstrip()
        return merged
    def _get_personal_address(self, session_id: str) -> str:        
        user_context = self._user_context_cache.get(session_id, {}) if session_id else {}
        preferences = user_context.get('preferences', {}) if isinstance(user_context, dict) else {}
        pronoun_style = (preferences or {}).get('pronoun_style', 'default')
        return 'báº¡n'
    
    def _call_gemini_api_with_smart_tokens(self, prompt: str, strategy: str, max_tokens: int, session_id: str = None) -> Optional[str]:
        max_retries = len(self.key_manager.keys) if self.key_manager.keys else 1 # Thá»­ tá»‘i Ä‘a báº±ng sá»‘ key, Ã­t nháº¥t 1 láº§n
        attempt = 0
        logger.debug(f"--- DEBUG START: _call_gemini_api_with_smart_tokens ---")
        logger.debug(f"Strategy: {strategy}, Max Tokens: {max_tokens}, Session: {session_id}")
        logger.debug(f"Prompt to send (first 500 chars): {prompt[:500]}...")
        logger.debug(f"Prompt to send (last 300 chars): ...{prompt[-300:]}")
        while attempt < max_retries:
            api_key_to_use = self.key_manager.get_key()
            if not api_key_to_use:
                logger.error("CRITICAL: All Gemini API keys are currently marked as rate-limited. Aborting call.")
                personal_address = self._get_personal_address(session_id)
                logger.debug(f"--- DEBUG END: _call_gemini_api_with_smart_tokens (All keys rate-limited) ---")
                # Tráº£ vá» thÃ´ng bÃ¡o lá»—i cá»¥ thá»ƒ hÆ¡n
                return f"ChÃ o cáº­u, hiá»‡n táº¡i táº¥t cáº£ cÃ¡c káº¿t ná»‘i Ä‘áº¿n trá»£ lÃ½ AI Ä‘á»u Ä‘ang táº¡m thá»i bá»‹ giá»›i háº¡n do quÃ¡ nhiá»u yÃªu cáº§u. Vui lÃ²ng thá»­ láº¡i sau khoáº£ng 1 phÃºt ná»¯a nhÃ©. ğŸ˜¥"
            logger.info(f"Attempt {attempt + 1}/{max_retries} using Key: ...{api_key_to_use[-4:]}")

            try:
                headers = {'Content-Type': 'application/json'}
                strategy_temp_adjustments = {
                    'quick_clarify': -0.2, 'direct_enhance': 0.0, 'enhanced_generation': +0.2,
                    'completion': -0.3, 'balanced': 0.0, 'document_context': +0.1,
                    'two_stage_reranking': +0.05,
                    'external_api_processing': 0.0, # Giá»¯ nguyÃªn temperature cho API data
                    'student_profile_processing': -0.1 # Giáº£m nháº¹ temperature cho profile Ä‘á»ƒ chÃ­nh xÃ¡c hÆ¡n
                }
                temp_adjustment = strategy_temp_adjustments.get(strategy, 0.0)
                base_temp = self.default_generation_config.get("temperature", 0.55)
                final_temperature = max(0.1, min(1.0, base_temp + temp_adjustment))
                base_top_p = self.default_generation_config.get("topP", 0.85)

                config = {
                    "temperature": final_temperature,
                    "maxOutputTokens": max_tokens,
                    "topP": base_top_p
                }
                data = {
                    "contents": [{"parts": [{"text": prompt}]}],
                    "generationConfig": config,
                    "safetySettings": [
                        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
                    ]
                }
                url = f"{self.base_url}?key={api_key_to_use}"
                response = requests.post(url, headers=headers, json=data, timeout=30) # Timeout 30 giÃ¢y
                if response.status_code == 200:
                    result = response.json()
                    if 'candidates' in result and result['candidates']:
                        candidate = result['candidates'][0]
                        if 'finishReason' in candidate and candidate['finishReason'] == 'SAFETY':
                            logger.warning(f"ğŸš¨ Gemini response blocked due to SAFETY reasons (Key: ...{api_key_to_use[-4:]}). Attempt {attempt + 1}/{max_retries}. Trying next key...")
                            attempt += 1
                            time.sleep(0.1)
                            continue # Thá»­ key tiáº¿p theo
                        if 'content' in candidate and 'parts' in candidate['content']:
                            response_text = candidate['content']['parts'][0]['text']
                            logger.debug(f"âœ… Gemini API Success (Key: ...{api_key_to_use[-4:]})")
                            logger.debug(f"Response received (first 300 chars): {response_text[:300]}...")
                            logger.debug(f"--- DEBUG END: _call_gemini_api_with_smart_tokens (Success) ---")
                            return response_text
                        else:
                            logger.warning(f"âš ï¸ Gemini API returned 200 but no valid content (Key: ...{api_key_to_use[-4:]}). Attempt {attempt + 1}/{max_retries}. Trying next key...")
                            attempt += 1
                            time.sleep(0.2)
                            continue # Thá»­ key tiáº¿p theo
                    else:
                        logger.warning(f"âš ï¸ Gemini API returned 200 but no candidates (Key: ...{api_key_to_use[-4:]}). Attempt {attempt + 1}/{max_retries}. Trying next key...")
                        attempt += 1
                        time.sleep(0.2)
                        continue # Thá»­ key tiáº¿p theo
                elif response.status_code == 429: # Rate Limit
                    self.key_manager.report_failure(api_key_to_use) # ÄÃ¡nh dáº¥u key bá»‹ rate limit
                    logger.warning(f"Rate limit (429) on key ...{api_key_to_use[-4:]}. Attempt {attempt + 1}/{max_retries}. Trying next key...")
                    attempt += 1
                    time.sleep(0.5) # Chá» lÃ¢u hÆ¡n chÃºt sau 429
                    continue # Thá»­ key tiáº¿p theo
                else:
                    logger.error(f"Gemini API Error {response.status_code} with key ...{api_key_to_use[-4:]}: {response.text}")
                    logger.warning(f"API Error {response.status_code}. Attempt {attempt + 1}/{max_retries}. Trying next key...")
                    attempt += 1
                    time.sleep(0.3) # Äá»™ trá»… nhá» trÆ°á»›c khi thá»­ key khÃ¡c
                    continue # Thá»­ key tiáº¿p theo
            except requests.exceptions.Timeout:
                logger.error(f"Gemini API call timed out with key ...{api_key_to_use[-4:]}. Attempt {attempt + 1}/{max_retries}. Trying next key...")
                attempt += 1
                time.sleep(0.8) # Chá» lÃ¢u hÆ¡n sau timeout
                continue # Thá»­ key tiáº¿p theo
            except Exception as e:
                logger.error(f"Unexpected error during API call with key ...{api_key_to_use[-4:]}: {str(e)}. Attempt {attempt + 1}/{max_retries}. Trying next key...")
                attempt += 1
                time.sleep(0.5)
                continue # Thá»­ key tiáº¿p theo

        logger.error(f"All {max_retries} retry attempts failed (due to errors like 429, 503, timeout, safety blocks, etc.).")
        personal_address = self._get_personal_address(session_id)
        logger.debug(f"--- DEBUG END: _call_gemini_api_with_smart_tokens (All retries failed) ---")
        return f"ChÃ o cáº­u, hiá»‡n táº¡i trá»£ lÃ½ AI Ä‘ang gáº·p sá»± cá»‘ káº¿t ná»‘i hoáº·c quÃ¡ táº£i sau khi thá»­ {max_retries} láº§n. {personal_address.title()} vui lÃ²ng thá»­ láº¡i sau Ã­t phÃºt nhÃ©."
    
    def _generate_clarification_request_smart(self, query, context, session_id=None):        
        personal_address = self._get_personal_address(session_id)
        clarification_templates = {
            'friendly': f"Äá»ƒ mÃ¬nh cÃ³ thá»ƒ há»— trá»£ tá»‘t nháº¥t, {personal_address} cÃ³ thá»ƒ chia sáº» thÃªm chi tiáº¿t Ä‘Æ°á»£c khÃ´ng? MÃ¬nh luÃ´n sáºµn lÃ²ng giÃºp!",
            'brief': f"ChÃ o cáº­u, cáº§n thÃªm thÃ´ng tin chi tiáº¿t áº¡. ğŸ“",
            'technical': f"ChÃ o cáº­u, Ä‘á»ƒ cung cáº¥p hÆ°á»›ng dáº«n ká»¹ thuáº­t chÃ­nh xÃ¡c, {personal_address} vui lÃ²ng cung cáº¥p thÃªm thÃ´ng sá»‘ vÃ  yÃªu cáº§u cá»¥ thá»ƒ áº¡.",
            'detailed': f"ChÃ o cáº­u, Ä‘á»ƒ tá»› cÃ³ thá»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i toÃ n diá»‡n vÃ  chi tiáº¿t nháº¥t, {personal_address} cÃ³ thá»ƒ bá»• sung thÃªm vá» bá»‘i cáº£nh, má»¥c Ä‘Ã­ch sá»­ dá»¥ng, vÃ  cÃ¡c yÃªu cáº§u cá»¥ thá»ƒ khÃ´ng? Äiá»u nÃ y sáº½ giÃºp tá»› há»— trá»£ {personal_address} má»™t cÃ¡ch hiá»‡u quáº£ nháº¥t.",
            'professional': f"ChÃ o cáº­u, Ä‘á»ƒ tá»› há»— trá»£ chÃ­nh xÃ¡c nháº¥t, {personal_address} cÃ³ thá»ƒ nÃ³i rÃµ hÆ¡n vá» váº¥n Ä‘á» cáº§n há»— trá»£ khÃ´ng? ğŸ“"
        }
        response = clarification_templates.get('professional', clarification_templates['professional'])
        token_info = {
            'smart_tokens_used': False,
            'method': 'clarification_template_v2',
            'confidence_managed': True,
            'template_type': 'professional'
        }
        return response, token_info
    def _generate_dont_know_response_smart(self, query, context, session_id=None):        
        personal_address = self._get_personal_address(session_id)
        query_lower = query.lower()
        if any(word in query_lower for word in ['ngÃ¢n hÃ ng Ä‘á»', 'Ä‘á» thi', 'kháº£o thÃ­']):
            dept = "PhÃ²ng Äáº£m báº£o cháº¥t lÆ°á»£ng vÃ  Kháº£o thÃ­"
            contact = "ldkham@bdu.edu.vn"
        elif any(word in query_lower for word in ['kÃª khai', 'nhiá»‡m vá»¥', 'giá» chuáº©n']):
            dept = "PhÃ²ng Tá»• chá»©c - CÃ¡n bá»™"
            contact = "tcccb@bdu.edu.vn"
        elif any(word in query_lower for word in ['táº¡p chÃ­', 'nghiÃªn cá»©u', 'khoa há»c']):
            dept = "PhÃ²ng NghiÃªn cá»©u - Há»£p tÃ¡c"
            contact = "nghiencuu@bdu.edu.vn"
        elif any(word in query_lower for word in ['khen thÆ°á»Ÿng', 'thi Ä‘ua']):
            dept = "PhÃ²ng Tá»• chá»©c - CÃ¡n bá»™"
            contact = "tcccb@bdu.edu.vn"
        else:
            dept = "phÃ²ng ban liÃªn quan"
            contact = "info@bdu.edu.vn"
        response = f"Xin lá»—i {personal_address}, mÃ¬nh chÆ°a cÃ³ thÃ´ng tin vá» váº¥n Ä‘á» nÃ y. Báº¡n cÃ³ thá»ƒ liÃªn há»‡ {dept} qua email {contact} Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ nhÃ©."
        token_info = {
            'smart_tokens_used': False,
            'method': 'dont_know_template_v2',
            'suggested_department': dept,
            'personal_addressing': personal_address,
            'confidence_managed': True
        }
        return response, token_info
    
    def validate_user_preferences(self, preferences):
        errors, warnings = [], []
        if 'user_memory_prompt' in preferences:
            memory = preferences['user_memory_prompt']
            if isinstance(memory, str):
                if len(memory) > 1500:
                    errors.append("user_memory_prompt too long (max 1500 characters)")
                elif len(memory) > 1400:
                    warnings.append("user_memory_prompt approaching limit")
            else:
                errors.append("user_memory_prompt must be string")
        if 'department_priority' in preferences:
            if not isinstance(preferences['department_priority'], bool):
                errors.append("department_priority must be boolean")
        return {'valid': len(errors) == 0, 'errors': errors, 'warnings': warnings}
    def get_user_context(self, session_id: str):
        return self._user_context_cache.get(session_id)    
    def clear_user_context(self, session_id=None):
        if session_id:
            if session_id in self._user_context_cache:
                del self._user_context_cache[session_id]
        else:
            self._user_context_cache.clear()
    def get_conversation_memory(self, session_id: str):
        return self.memory.get_conversation_context(session_id)
    def clear_conversation_memory(self, session_id: str = None):
        if session_id:
            if session_id in self.memory.conversations:
                del self.memory.conversations[session_id]
        else:
            self.memory.conversations.clear()
    def get_system_status(self) -> Dict[str, Any]:
        try:
            test_prompt = "Test ngáº¯n cho sinh viÃªn"
            response = self._call_gemini_api_with_smart_tokens(test_prompt, 'quick_clarify', 80, session_id="test")
            return {
                'gemini_api_available': response is not None,
                'api_key_configured': bool(self.key_manager.keys),
                'service_status': 'active' if response else 'error',
                'mode': 'advanced_rag_gemini_with_two_stage_reranking_integration_and_advanced_confidence_management',
                'memory_sessions': len(self.memory.conversations),
                'personalization_sessions': len(self._user_context_cache),
                'adaptive_token_range': self.token_manager.adaptive_token_range,
                'confidence_management': {
                    'max_confidence': self.confidence_manager.MAX_CONFIDENCE,
                    'decision_thresholds': self.confidence_manager.decision_thresholds,
                    'calibration_rules': self.confidence_manager.confidence_calibration_rules,
                    'overflow_protection_enabled': True,
                    'confidence_normalization_active': True
                }
            }
        except Exception as e:
            return {
                'gemini_api_available': False,
                'service_status': 'error',
                'error': str(e),
                'consistent_personalization': True,
                'graceful_degradation': True,
                'document_context_support': True,
                'advanced_confidence_management': True,
                'confidence_overflow_protection': True
            }
    def _post_process_response(self, response, query, context, strategy, conversation_context, session_id=None):
        if not response:
            return response
        personal_address = self._get_personal_address(session_id)
        response = re.sub(r'\*\*\d+\.\s*', '', response)
        response = re.sub(r'^\s*\d+\.\s*', '', response, flags=re.MULTILINE)
        response = re.sub(r'^\s*[â€¢\-\*]\s*', '', response, flags=re.MULTILINE)
        response = re.sub(r'\*\*(.*?)\*\*', r'\1', response)
        if session_id and self._should_strip_greeting(session_id):
            response = self._strip_greeting_and_closing(response, personal_address)
        return response.strip()
    def _get_smart_fallback_with_context(self, query, intent_info, conversation_context, session_id=None):        
        personal_address = self._get_personal_address(session_id)
        user_context = self._user_context_cache.get(session_id, {}) if session_id else {}
        department_name = user_context.get('department_name', '')
        intent_name = intent_info.get('intent', 'general') if intent_info else 'general'
        if conversation_context.get('context_summary'):
            summary = conversation_context['context_summary']
            context_fallbacks = {
                'Äang há»i vá» thÃ´ng tin sinh viÃªn': f"ChÃ o cáº­u, vá» thÃ´ng tin sinh viÃªn, tá»› cÃ³ thá»ƒ há»— trá»£ thÃªm! ğŸ“‹ {personal_address.title()} cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng?",
                'Äang há»i vá» lá»‹ch há»c': f"ChÃ o cáº­u, vá» lá»‹ch há»c, tá»› cÃ³ thá»ƒ há»— trá»£ thÃªm! ğŸ“Š {personal_address.title()} cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng?",
                'Äang há»i vá» Ä‘iá»ƒm sá»‘': f"ChÃ o cáº­u, vá» Ä‘iá»ƒm sá»‘, tá»› cÃ³ thá»ƒ há»— trá»£ thÃªm! ğŸ“š {personal_address.title()} cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng?",
                'Äang há»i vá» há»c phÃ­': f"ChÃ o cáº­u, vá» há»c phÃ­, tá»› cÃ³ thá»ƒ há»— trá»£ thÃªm! ğŸ† {personal_address.title()} cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng?"
            }
            if summary in context_fallbacks:
                return context_fallbacks[summary]
        smart_fallbacks = {
            'greeting': f"ChÃ o {personal_address}! ğŸ‘‹ MÃ¬nh cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n vá» BDU khÃ´ng?",
            'general': f"ChÃ o cáº­u, tá»› sáºµn sÃ ng há»— trá»£ cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n BDU! ğŸ“ {personal_address.title()} cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng?"
        }
        if department_name and intent_name == 'general':
            smart_fallbacks['general'] = f"ChÃ o cáº­u, tá»› sáºµn sÃ ng há»— trá»£ cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n BDU vÃ  ngÃ nh {department_name}! ğŸ“ {personal_address.title()} cÃ³ cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng?"
        return smart_fallbacks.get(intent_name, smart_fallbacks['general'])
    def _get_contextual_out_of_scope_response(self, conversation_context, session_id=None):        
        personal_address = self._get_personal_address(session_id)
        user_context = self._user_context_cache.get(session_id, {}) if session_id else {}
        department_name = user_context.get('department_name', '')
        if conversation_context.get('context_summary'):
            if department_name:
                return f"MÃ¬nh chá»‰ há»— trá»£ cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n há»c táº­p táº¡i BDU thÃ´i. Báº¡n cÃ³ cÃ¢u há»i nÃ o khÃ¡c vá» trÆ°á»ng khÃ´ng?"
            else:
                return f"MÃ¬nh chá»‰ há»— trá»£ cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n há»c táº­p táº¡i BDU thÃ´i. Báº¡n cÃ³ cÃ¢u há»i nÃ o khÃ¡c vá» trÆ°á»ng khÃ´ng?"
        if department_name:
            return f"MÃ¬nh chá»‰ há»— trá»£ cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n há»c táº­p táº¡i BDU thÃ´i. Báº¡n cÃ³ cÃ¢u há»i nÃ o khÃ¡c vá» ngÃ nh {department_name} khÃ´ng?"
        else:
            return f"MÃ¬nh chá»‰ há»— trá»£ cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n há»c táº­p táº¡i BDU thÃ´i. Báº¡n cÃ³ cÃ¢u há»i nÃ o khÃ¡c vá» ngÃ nh {department_name} khÃ´ng?"

    def _is_education_related(self, query):
        education_keywords = [
            'trÆ°á»ng', 'há»c', 'sinh viÃªn', 'tuyá»ƒn sinh', 'há»c phÃ­', 'ngÃ nh',
            'Ä‘áº¡i há»c', 'bdu', 'Ä‘Äƒng kÃ½', 'mÃ´n há»c', 'tÃ­n chá»‰', 
            'lá»‹ch thi', 'ká»³ thi', 'Ä‘iá»ƒm', 'Ä‘iá»ƒm danh', 'váº¯ng',
            'thá»i khÃ³a biá»ƒu', 'lá»‹ch há»c', 'phÃ²ng há»c', 'tiáº¿t há»c',
            'há»c láº¡i', 'cáº£i thiá»‡n Ä‘iá»ƒm', 'thi láº¡i', 'nÃ¢ng Ä‘iá»ƒm',
            'Ä‘iá»ƒm trung bÃ¬nh', 'trung bÃ¬nh', 'tÃ­nh Ä‘iá»ƒm', 'Ä‘iá»ƒm quÃ¡ trÃ¬nh',
            'Ä‘iá»ƒm thi', 'Ä‘iá»ƒm cuá»‘i ká»³', 'Ä‘iá»ƒm giá»¯a ká»³',
            'khá»‘i lÆ°á»£ng', 'tá»‘i thiá»ƒu', 'chÆ°Æ¡ng trÃ¬nh', 'há»c ká»³', 'nÄƒm há»c',
            'tá»‘t nghiá»‡p', 'lá»… tá»‘t nghiá»‡p', 'xÃ©t tá»‘t nghiá»‡p', 'báº±ng cáº¥p',
            'vÄƒn báº±ng', 'cá»­ nhÃ¢n', 'cáº¥p báº±ng', 'nháº­n báº±ng',
            'ká»· luáº­t', 'danh sÃ¡ch', 'theo quy Ä‘á»‹nh', 'quy Ä‘á»‹nh vá»', 
            'thá»§ tá»¥c', 'Ä‘iá»u kiá»‡n', 'yÃªu cáº§u', 'má»Ÿ lá»›p',
            'nhÆ° tháº¿ nÃ o', 'bao nhiÃªu', 'lÃ  ai', 'ai lÃ ', 'lÃ m gÃ¬', 'á»Ÿ Ä‘Ã¢u',
            'khi nÃ o', 'cÃ³ Ä‘Æ°á»£c', 'cáº§n gÃ¬', 'pháº£i lÃ m'
        ]
        if not query:
            return False        
        query_lower = query.lower()
        return any(kw in query_lower for kw in education_keywords)