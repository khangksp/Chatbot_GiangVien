import os
import logging
import torch
from django.conf import settings

# Setup logger
logger = logging.getLogger(__name__)

# Ki·ªÉm tra th∆∞ vi·ªán faster_whisper
try:
    from faster_whisper import WhisperModel
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    print("‚ö†Ô∏è Warning: faster_whisper not installed. Speech-to-text will not be available.")

# Ki·ªÉm tra gTTS
try:
    from gtts import gTTS
    GTTS_AVAILABLE = True
except ImportError:
    GTTS_AVAILABLE = False

# Ki·ªÉm tra pydub (ƒë·ªÉ x·ª≠ l√Ω audio speed n·∫øu c·∫ßn)
try:
    from pydub import AudioSegment
    PYDUB_AVAILABLE = True
except ImportError:
    PYDUB_AVAILABLE = False

class SpeechToTextService:
    def __init__(self):
        """
        Kh·ªüi t·∫°o d·ªãch v·ª• STT nh∆∞ng KH√îNG load model ngay (Lazy Loading).
        ƒêi·ªÅu n√†y gi√∫p server kh·ªüi ƒë·ªông nhanh v√† tr√°nh crash do thi·∫øu VRAM/Cuda context ban ƒë·∫ßu.
        """
        # Ki·ªÉm tra GPU
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # C·∫•u h√¨nh compute type: float16 cho GPU, int8 cho CPU
        self.compute_type = "float16" if self.device == 'cuda' else "int8"
        
        # ‚ö†Ô∏è C·∫§U H√åNH MODEL: LARGE-V3 (Theo y√™u c·∫ßu c·ªßa Khang)
        # L∆∞u √Ω: Model n√†y y√™u c·∫ßu kho·∫£ng 3GB-4GB VRAM. 
        self.model_size = "large-v3" 
        
        # Bi·∫øn ch·ª©a model (Kh·ªüi t·∫°o l√† None)
        self.model = None 
        
        logger.info(f"‚úÖ SpeechToTextService initialized (Lazy Loading). Device: {self.device}, Model: {self.model_size}")

    def _load_model(self):
        """
        H√†m n·ªôi b·ªô: Ch·ªâ load model khi th·ª±c s·ª± c·∫ßn d√πng (khi c√≥ request convert voice).
        """
        if self.model is not None:
            return

        if not WHISPER_AVAILABLE:
            raise ImportError("faster_whisper not available")
        
        logger.info(f"üöÄ Lazy Loading Whisper model '{self.model_size}' on {self.device}...")
        
        # 1. T√≠nh to√°n s·ªë worker t·ªëi ∆∞u d·ª±a tr√™n CPU
        cpu_count = os.cpu_count() or 4
        num_workers = max(1, cpu_count // 2)
        cpu_threads = max(1, cpu_count // 2)
        
        # 2. ‚úÖ KH·∫ÆC PH·ª§C L·ªñI SYMLINK WINDOWS & QUY·ªÄN ADMIN:
        # T·∫°o th∆∞ m·ª•c 'models_cache' ngay trong project thay v√¨ d√πng cache h·ªá th·ªëng.
        cache_dir = os.path.join(settings.BASE_DIR, 'models_cache', 'whisper')
        
        if not os.path.exists(cache_dir):
            try:
                os.makedirs(cache_dir, exist_ok=True)
                logger.info(f"üìÅ Created local model cache: {cache_dir}")
            except Exception as e:
                logger.error(f"‚ùå Could not create cache dir {cache_dir}: {e}")
                # Fallback: n·∫øu kh√¥ng t·∫°o ƒë∆∞·ª£c th∆∞ m·ª•c local, ƒë·ªÉ th∆∞ vi·ªán t·ª± quy·∫øt ƒë·ªãnh (c√≥ th·ªÉ l·ªói)
                cache_dir = None

        try:
            # Load model v·ªõi download_root tr·ªè v·ªÅ th∆∞ m·ª•c local
            self.model = WhisperModel(
                self.model_size,
                device=self.device,
                compute_type=self.compute_type,
                num_workers=num_workers,
                cpu_threads=cpu_threads,
                download_root=cache_dir, # üëà Quan tr·ªçng: Tr√°nh l·ªói Symlink
                local_files_only=False 
            )
            logger.info(f"‚úÖ Whisper model '{self.model_size}' loaded successfully.")
            
        except OSError as e:
            logger.error(f"‚ùå OS Error loading Whisper (Possible Symlink/Permission issue): {e}")
            logger.warning("‚ö†Ô∏è H√£y th·ª≠ ch·∫°y l·∫°i server v·ªõi quy·ªÅn Administrator ho·∫∑c ki·ªÉm tra dung l∆∞·ª£ng ·ªï c·ª©ng.")
            raise e
        except Exception as e:
            logger.error(f"‚ùå Failed to load Whisper model: {e}")
            raise e

    def transcribe_audio(self, audio_file_path, language='vi', beam_size=5):
        """
        Chuy·ªÉn ƒë·ªïi file √¢m thanh th√†nh vƒÉn b·∫£n.
        """
        if not WHISPER_AVAILABLE:
            return {"success": False, "text": "", "error": "Library 'faster_whisper' not installed"}

        try:
            # ‚úÖ G·ªåI H√ÄM LOAD MODEL (N·∫øu ch∆∞a load th√¨ gi·ªù m·ªõi load)
            self._load_model()
            
            # B·∫Øt ƒë·∫ßu transcribe
            # beam_size=5 gi√∫p tƒÉng ƒë·ªô ch√≠nh x√°c nh∆∞ng ch·∫≠m h∆°n m·ªôt ch√∫t
            segments, info = self.model.transcribe(
                audio_file_path, 
                beam_size=beam_size, 
                language=language
            )
            
            # G·ªôp c√°c ƒëo·∫°n text l·∫°i
            text_segments = []
            for segment in segments:
                text_segments.append(segment.text)
            
            full_text = " ".join(text_segments).strip()
            
            return {
                "success": True,
                "text": full_text,
                "language": info.language,
                "language_probability": info.language_probability
            }
            
        except Exception as e:
            logger.error(f"Error transcribing audio: {str(e)}")
            # N·∫øu l·ªói OOM (Out Of Memory), g·ª£i √Ω h·∫° model
            if "CUDA out of memory" in str(e):
                logger.critical("‚ùå GPU VRAM OOM! H√£y th·ª≠ h·∫° model_size xu·ªëng 'medium' ho·∫∑c 'small'.")
            return {"success": False, "text": "", "error": str(e)}
            
    def get_system_status(self):
        """Tr·∫£ v·ªÅ tr·∫°ng th√°i hi·ªán t·∫°i c·ªßa service"""
        return {
            'available': WHISPER_AVAILABLE,
            'model_loaded': self.model is not None,
            'device': self.device,
            'model_size': self.model_size,
            'compute_type': self.compute_type
        }

class TextToSpeechService:
    def __init__(self):
        self.available = GTTS_AVAILABLE
        if self.available:
            logger.info("‚úÖ Text-to-Speech service (gTTS) initialized")
        else:
            logger.warning("‚ö†Ô∏è gTTS not installed. Text-to-Speech disabled.")
    
    def text_to_audio_base64(self, text, lang='vi', slow=False):
        """
        Chuy·ªÉn text sang audio v√† tr·∫£ v·ªÅ base64 (ƒë·ªÉ play tr√™n frontend)
        """
        if not self.available:
            return None
            
        try:
            import io
            import base64
            
            # T·∫°o file audio trong b·ªô nh·ªõ
            mp3_fp = io.BytesIO()
            tts = gTTS(text=text, lang=lang, slow=slow)
            tts.write_to_fp(mp3_fp)
            
            # Chuy·ªÉn sang base64
            mp3_fp.seek(0)
            audio_base64 = base64.b64encode(mp3_fp.read()).decode('utf-8')
            return audio_base64
            
        except Exception as e:
            logger.error(f"Error in TTS: {e}")
            return None
    
    def get_system_status(self):
        return {
            'available': self.available,
            'supported_languages': ['vi', 'en']
        }