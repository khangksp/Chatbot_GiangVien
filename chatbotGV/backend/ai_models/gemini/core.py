import logging
import time
import requests
import json
import re
import os
from typing import Dict, Any, List, Optional

# Import c√°c module con v·ª´a t√°ch
from .key_manager import GeminiApiKeyManager
from .token_manager import SmartTokenManager
from .confidence_manager import AdvancedConfidenceManager
from .memory import ConversationMemory
from .utils import SimpleVietnameseRestorer, build_personalized_system_prompt

# ‚ö†Ô∏è L∆ØU √ù: Import ner_service d√πng 2 d·∫•u ch·∫•m (..) v√¨ n√≥ n·∫±m ·ªü th∆∞ m·ª•c cha
try:
    from ..ner_service import SimpleEntityExtractor
except ImportError:
    # Fallback n·∫øu ch·∫°y test ƒë·ªôc l·∫≠p
    import sys
    sys.path.append("..") 
    from ner_service import SimpleEntityExtractor

logger = logging.getLogger(__name__)

class GeminiResponseGenerator:    
    def __init__(self):
        self.key_manager = GeminiApiKeyManager()
        self.model_name = "gemini-2.5-flash" 
        self.base_url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model_name}:generateContent"
        self.memory = ConversationMemory(max_history=30)
        self.vietnamese_restorer = SimpleVietnameseRestorer(self.key_manager)
        
        self.token_manager = SmartTokenManager()
        self.confidence_manager = AdvancedConfidenceManager()
        self._user_context_cache = {}
        
        self.default_generation_config = {
            "temperature": 0.4,
            "topP": 0.85
        }
        
        self.role_consistency_rules = {
            'identity': 'AI assistant c·ªßa ƒê·∫°i h·ªçc B√¨nh D∆∞∆°ng (BDU) h·ªó tr·ª£ gi·∫£ng vi√™n',
            'personality': 'l·ªãch s·ª±, chuy√™n nghi·ªáp, t√¥n tr·ªçng',
            'knowledge_scope': 'chuy√™n v·ªÅ th√¥ng tin BDU v√† h·ªó tr·ª£ gi·∫£ng vi√™n',
            'addressing': 'lu√¥n x∆∞ng h√¥ ƒë√∫ng c√°ch, kh√¥ng bao gi·ªù d√πng b·∫°n/m√¨nh',
            'prohibited_roles': [
                'sinh vi√™n', 'h·ªçc sinh', 'ph·ª• huynh', 'ng∆∞·ªùi ngo√†i tr∆∞·ªùng'
            ]
        }
        
        logger.info("‚úÖ Enhanced Gemini Response Generator initialized with Advanced Confidence Management, Smart Token Management, v√† Two-Stage Re-ranking Integration")

    def _build_document_context_prompt(self, query: str, document_text: str, session_id: str = None) -> str:
        system_prompt = self._get_personalized_system_prompt(session_id)
        personal_address = self._get_personal_address(session_id)
        
        conversation_context = self.memory.get_conversation_context(session_id) if session_id else {}
        recent_summary = conversation_context.get('recent_conversation_summary', '')
        
        context_section = ""
        if recent_summary:
            context_section = f"""
üó£Ô∏è NG·ªÆ C·∫¢NH H·ªòI THO·∫†I G·∫¶N ƒê√ÇY:
{recent_summary}

üí° L∆ØU √ù: H√£y tham kh·∫£o ng·ªØ c·∫£nh tr√™n ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c, tr√°nh l·∫∑p l·∫°i th√¥ng tin ƒë√£ th·∫£o lu·∫≠n.
"""
        
        # Truncate document text if too long (keep within token limits)
        max_doc_length = 3000  # characters
        if len(document_text) > max_doc_length:
            document_text = document_text[:max_doc_length] + "\n\n[...t√†i li·ªáu c√≤n ti·∫øp...]"
        
        # ‚≠ê NHI·ªÜM V·ª§ 2: Th√™m kh·ªëi ch·ªâ d·∫´n ƒë·∫∑c bi·ªát cho vi·ªác x·ª≠ l√Ω d·ªØ li·ªáu OCR "b·∫©n"
        ocr_guidance = """---
‚≠ê H∆Ø·ªöNG D·∫™N X·ª¨ L√ù D·ªÆ LI·ªÜU OCR ƒê·∫∂C BI·ªÜT (R·∫•t quan tr·ªçng)
D·ªØ li·ªáu d∆∞·ªõi ƒë√¢y ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª± ƒë·ªông t·ª´ file PDF/DOCX, do ƒë√≥ c√≥ th·ªÉ ch·ª©a c√°c l·ªói ƒë·ªãnh d·∫°ng, ƒë·∫∑c bi·ªát l√† c√°c b·∫£ng (table) b·ªã chuy·ªÉn th√†nh vƒÉn b·∫£n thu·∫ßn t√∫y.
1.  **X·ª≠ l√Ω b·∫£ng (Table):** M·ªôt d√≤ng vƒÉn b·∫£n c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin li√™n quan (v√≠ d·ª•: s·ªë th·ª© t·ª±, h·ªç t√™n, ch·ª©c v·ª•, nhi·ªám v·ª•). B·∫†N PH·∫¢I T·ª∞ SUY LU·∫¨N ƒë·ªÉ li√™n k·∫øt c√°c th√¥ng tin c√≥ v·∫ª n·∫±m tr√™n c√πng m·ªôt h√†ng v·ªõi nhau. V√≠ d·ª•: d√≤ng "1 B√† A Ch·ª©c v·ª• B Nhi·ªám v·ª• C" c√≥ nghƒ©a l√† B√† A c√≥ ch·ª©c v·ª• B v√† nhi·ªám v·ª• C.
2.  **ƒê·∫øm s·ªë l∆∞·ª£ng:** N·∫øu ƒë∆∞·ª£c h·ªèi "c√≥ m·∫•y ƒëi·ªÅu", "c√≥ bao nhi√™u th√†nh vi√™n", h√£y t√¨m v√† ƒë·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa c√°c t·ª´ kh√≥a nh∆∞ "ƒêi·ªÅu 1.", "ƒêi·ªÅu 2.", ho·∫∑c c√°c s·ªë th·ª© t·ª± trong danh s√°ch (1, 2, 3...).
3.  **T√¨m ki·∫øm ch√≠nh x√°c:** H√£y ƒë·ªçc th·∫≠t k·ªπ v√† t√¨m ki·∫øm ch√≠nh x√°c c√°c t·ª´ kh√≥a trong c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng trong to√†n b·ªô vƒÉn b·∫£n, ngay c·∫£ khi n√≥ kh√¥ng c√≥ c·∫•u tr√∫c.
---"""

        prompt = f"""{system_prompt}

üéØ NHI·ªÜM V·ª§ ƒê·∫∂C BI·ªÜT: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p

{ocr_guidance}

üìÑ N·ªòI DUNG T√ÄI LI·ªÜU:
{document_text}

{context_section}

‚ùì C√ÇU H·ªéI C·ª¶A GI·∫¢NG VI√äN: {query}

üìù Y√äU C·∫¶U TR·∫¢ L·ªúI QUAN TR·ªåNG:
- X∆∞ng h√¥: "D·∫° {personal_address},"
- CH·ªà TR·∫¢ L·ªúI D·ª∞A V√ÄO n·ªôi dung t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p ·ªü tr√™n
- KH√îNG S·ª¨ D·ª§NG ki·∫øn th·ª©c b√™n ngo√†i t√†i li·ªáu
- N·∫øu t√†i li·ªáu kh√¥ng ch·ª©a th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥
- Tr√≠ch d·∫´n c·ª• th·ªÉ t·ª´ t√†i li·ªáu khi c√≥ th·ªÉ
- T·∫°o c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu v√† m·∫°ch l·∫°c
- K·∫øt th√∫c: "{personal_address.title()} c√≥ c·∫ßn em h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?"
- TUY·ªÜT ƒê·ªêI KH√îNG b·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu

Tr·∫£ l·ªùi:"""

        return prompt

    def _generate_external_api_response(self, query, context, session_id=None):        
        api_data = context.get('api_data', {})
        lecturer_info = api_data.get('lecturer_info', {})
        schedule_summary = api_data.get('schedule_summary', {})
        daily_schedule = api_data.get('daily_schedule', {})
        personal_address = self._get_personal_address_from_api_data(lecturer_info, session_id)
        conversation_context = self.memory.get_conversation_context(session_id) if session_id else {}
        recent_summary = conversation_context.get('recent_conversation_summary', '')
        prompt = self._build_external_api_prompt(
            query, api_data, personal_address, recent_summary
        )
        optimal_tokens = self.token_manager.calculate_optimal_tokens(
            len(prompt), 
            'external_api_processing'
        )
        logger.info(f"üåê Processing external API data with {optimal_tokens} tokens")
        response = self._call_gemini_api_with_smart_tokens(
            prompt, 'external_api_processing', optimal_tokens, session_id
        )
        
        if not response:
            return self._get_external_api_fallback_response(api_data, personal_address)
        response = self._post_process_external_api_response(
            response, lecturer_info, query, session_id
        )
        return response
    
    def _build_external_api_prompt(self, query, api_data, personal_address, recent_summary=""):        
        lecturer_info = api_data.get('lecturer_info', {})
        schedule_summary = api_data.get('schedule_summary', {})
        daily_schedule = api_data.get('daily_schedule', {})
        query_context = api_data.get('query_context', '')
        
        ten_giang_vien = lecturer_info.get('ten_giang_vien', personal_address)
        ma_giang_vien = lecturer_info.get('ma_giang_vien', '')
        chuc_danh = lecturer_info.get('chuc_danh', '')
        gmail = lecturer_info.get('gmail', '')
        trinh_do = lecturer_info.get('trinh_do', '')
        
        total_classes = schedule_summary.get('total_classes', 0)
        unique_subjects = schedule_summary.get('unique_subjects', 0)
        total_periods = schedule_summary.get('total_periods', 0)
        
        schedule_text = self._format_schedule_for_prompt(daily_schedule)
        system_prompt = self._get_personalized_system_prompt_for_external_api(
            lecturer_info
        )
        
        context_section = ""
        if recent_summary:
            context_section = f"""
üó£Ô∏è NG·ªÆ C·∫¢NH H·ªòI THO·∫†I G·∫¶N ƒê√ÇY:
{recent_summary}

üí° L∆ØU √ù: H√£y tham kh·∫£o ng·ªØ c·∫£nh tr√™n ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c, tr√°nh l·∫∑p l·∫°i th√¥ng tin ƒë√£ n√≥i.
"""
        
        prompt = f"""{system_prompt}

üéØ NHI·ªÜM V·ª§ ƒê·∫∂C BI·ªÜT: X·ª≠ l√Ω th√¥ng tin C√Å NH√ÇN t·ª´ h·ªá th·ªëng c·ªßa tr∆∞·ªùng

üìã TH√îNG TIN GI·∫¢NG VI√äN:
- M√£ gi·∫£ng vi√™n: {ma_giang_vien}
- H·ªç v√† t√™n: {ten_giang_vien}
- Ch·ª©c danh: {chuc_danh}
- Tr√¨nh ƒë·ªô: {trinh_do}
- Email: {gmail}

üìÖ T·ªîNG QUAN L·ªäCH GI·∫¢NG D·∫†Y:
- T·ªïng s·ªë bu·ªïi h·ªçc: {total_classes}
- S·ªë m√¥n h·ªçc: {unique_subjects}
- T·ªïng s·ªë ti·∫øt: {total_periods}

üìñ CHI TI·∫æT L·ªäCH GI·∫¢NG D·∫†Y:
{schedule_text}

{context_section}

‚ùì C√ÇU H·ªéI C·ª¶A GI·∫¢NG VI√äN: {query}
üîç NG·ªÆ C·∫¢NH T√åM KI·∫æM: {query_context}

üìù Y√äU C·∫¶U TR·∫¢ L·ªúI:
- X∆∞ng h√¥: "D·∫° {personal_address},"
- Tr·∫£ l·ªùi CH√çNH X√ÅC d·ª±a tr√™n d·ªØ li·ªáu th·ª±c t·∫ø t·ª´ h·ªá th·ªëng
- T·∫°o c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c, tr√°nh l·∫∑p l·∫°i th√¥ng tin ƒë√£ th·∫£o lu·∫≠n
- ƒê·ªãnh d·∫°ng th√¥ng tin d·ªÖ ƒë·ªçc, r√µ r√†ng
- Bao g·ªìm c√°c chi ti·∫øt quan tr·ªçng: th·ªùi gian, ƒë·ªãa ƒëi·ªÉm, m√¥n h·ªçc
- K·∫øt th√∫c: "{personal_address.title()} c√≥ c·∫ßn em h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?"
- KH√îNG CH·∫æ T·∫†O th√¥ng tin kh√¥ng c√≥ trong d·ªØ li·ªáu

Tr·∫£ l·ªùi:"""
        return prompt
    
    def _format_schedule_for_prompt(self, daily_schedule):
        if not daily_schedule:
            return "Hi·ªán t·∫°i kh√¥ng c√≥ l·ªãch gi·∫£ng d·∫°y trong kho·∫£ng th·ªùi gian n√†y."
        
        formatted_lines = []
        sorted_dates = sorted(daily_schedule.keys())
        
        for date_str in sorted_dates:
            classes = daily_schedule[date_str]
            try:
                from datetime import datetime
                date_obj = datetime.strptime(date_str, '%d-%m-%Y')
                weekdays = ['Th·ª© Hai', 'Th·ª© Ba', 'Th·ª© T∆∞', 'Th·ª© NƒÉm', 'Th·ª© S√°u', 'Th·ª© B·∫£y', 'Ch·ªß Nh·∫≠t']
                weekday = weekdays[date_obj.weekday()]
                formatted_date = f"{weekday}, {date_str}"
            except:
                formatted_date = date_str
            
            formatted_lines.append(f"\nüìÖ {formatted_date}:")
            sorted_classes = sorted(classes, key=lambda x: x.get('tiet_bat_dau', 0))
            
            for class_info in sorted_classes:
                ma_mon_hoc = class_info.get('ma_mon_hoc', '')
                ten_mon_hoc = class_info.get('ten_mon_hoc', '')
                ma_lop = class_info.get('ma_lop', '')
                ma_phong = class_info.get('ma_phong', '')
                tiet_bat_dau = class_info.get('tiet_bat_dau', '')
                so_tiet = class_info.get('so_tiet', '')
                so_luong_sv = class_info.get('so_luong_sv', '')
 
                class_line = f"   ‚Ä¢ {ten_mon_hoc} ({ma_mon_hoc})"
                class_line += f" - L·ªõp {ma_lop}"
                class_line += f" - Ph√≤ng {ma_phong}"
                class_line += f" - Ti·∫øt {tiet_bat_dau}"
                if so_tiet:
                    class_line += f" ({so_tiet} ti·∫øt)"
                if so_luong_sv:
                    class_line += f" - {so_luong_sv} SV"
                
                formatted_lines.append(class_line)
        
        return '\n'.join(formatted_lines) if formatted_lines else "Kh√¥ng c√≥ l·ªãch gi·∫£ng d·∫°y."

    def _get_personalized_system_prompt_for_external_api(self, lecturer_info):        
        ten_giang_vien = lecturer_info.get('ten_giang_vien', '')
        gender = lecturer_info.get('gender', 'other')
        chuc_danh = lecturer_info.get('chuc_danh', '')
        
        if gender == 'male':
            salutation = 'th·∫ßy'
        elif gender == 'female':
            salutation = 'c√¥'
        else:
            salutation = 'gi·∫£ng vi√™n'
        
        name_parts = ten_giang_vien.split() if ten_giang_vien else []
        name_suffix = name_parts[-1] if name_parts else ''
        
        if salutation in ['th·∫ßy', 'c√¥']:
            personal_address = f"{salutation} {name_suffix}" if name_suffix else salutation
        else:
            personal_address = f"{salutation} {ten_giang_vien}" if ten_giang_vien else salutation
        
        base_prompt = f"""B·∫°n l√† AI assistant c·ªßa ƒê·∫°i h·ªçc B√¨nh D∆∞∆°ng (BDU), chuy√™n h·ªó tr·ª£ gi·∫£ng vi√™n.

üéØ TH√îNG TIN NG∆Ø·ªúI D√ôNG:
- B·∫°n ƒëang tr·∫£ l·ªùi cho {chuc_danh} {ten_giang_vien}
- X∆∞ng h√¥: "{personal_address}" (TUY·ªÜT ƒê·ªêI KH√îNG d√πng "b·∫°n", "m√¨nh", "anh/ch·ªã")
- ƒê√¢y l√† th√¥ng tin C√Å NH√ÇN t·ª´ h·ªá th·ªëng ch√≠nh th·ª©c c·ªßa tr∆∞·ªùng

üéØ QUY T·∫ÆC QUAN TR·ªåNG:
- LU√îN b·∫Øt ƒë·∫ßu: "D·∫° {personal_address},"
- K·∫øt th√∫c: "{personal_address.title()} c√≥ c·∫ßn em h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?"
- S·ª¨ D·ª§NG CH√çNH X√ÅC th√¥ng tin t·ª´ h·ªá th·ªëng - KH√îNG CH·∫æ T·∫†O
- Tr√¨nh b√†y th√¥ng tin c√° nh√¢n m·ªôt c√°ch t·ª± nhi√™n, d·ªÖ hi·ªÉu
- KH√îNG d√πng format ph·ª©c t·∫°p v·ªõi **1. **2. hay bullets khi kh√¥ng c·∫ßn thi·∫øt"""

        return base_prompt

    def _get_personal_address_from_api_data(self, lecturer_info, session_id):
        ten_giang_vien = lecturer_info.get('ten_giang_vien', '')
        gender = lecturer_info.get('gender', 'other')
        
        if gender == 'male':
            salutation = 'th·∫ßy'
        elif gender == 'female':
            salutation = 'c√¥'
        else:
            salutation = 'gi·∫£ng vi√™n'
        
        if ten_giang_vien:
            if salutation in ['th·∫ßy', 'c√¥']:
                name_suffix = ten_giang_vien.split()[-1]
                return f"{salutation} {name_suffix}"
            else:
                return f"{salutation} {ten_giang_vien}"
        
        return self._get_personal_address(session_id)

    def _post_process_external_api_response(self, response, lecturer_info, query, session_id):
        if not response:
            return response
        ten_giang_vien = lecturer_info.get('ten_giang_vien', '')
        gender = lecturer_info.get('gender', 'other')
        
        if gender == 'male':
            salutation = 'th·∫ßy'
        elif gender == 'female':
            salutation = 'c√¥'
        else:
            salutation = 'gi·∫£ng vi√™n'
        
        if ten_giang_vien:
            if salutation in ['th·∫ßy', 'c√¥']:
                name_suffix = ten_giang_vien.split()[-1]
                personal_address = f"{salutation} {name_suffix}"
            else:
                personal_address = f"{salutation} {ten_giang_vien}"
        else:
            personal_address = salutation
        
        # üëá FIX: X·ª≠ l√Ω l·ªói x∆∞ng h√¥ "em l√† gi·∫£ng vi√™n" do Ollama d√πng t·ª´ "t√¥i/m√¨nh"
        # B·∫Øt c√°c c·ª•m t·ª´ nh∆∞: "em l√† gi·∫£ng vi√™n", "t√¥i l√† gi·∫£ng vi√™n", "m√¨nh l√† gi·∫£ng vi√™n"
        # ƒê·ªïi th√†nh: "th·∫ßy Tu·∫•n l√† gi·∫£ng vi√™n"
        response = re.sub(
            r'\b(em|t√¥i|m√¨nh)\s+(l√†|ƒë∆∞·ª£c ghi nh·∫≠n l√†)\s+(m·ªôt\s+)?(gi·∫£ng vi√™n|c√°n b·ªô|tr∆∞·ªüng|ph√≥|ng∆∞·ªùi)', 
            f'{personal_address} \\2 \\3\\4', 
            response, 
            flags=re.IGNORECASE
        )

        # Sau ƒë√≥ m·ªõi ch·∫°y c√°c replacement ƒë·∫°i t·ª´ chung
        response = re.sub(r'\bb·∫°n\b', personal_address, response, flags=re.IGNORECASE)
        response = re.sub(r'\bm√¨nh\b', 'em', response, flags=re.IGNORECASE)
        response = re.sub(r'\bt√¥i\b', 'em', response, flags=re.IGNORECASE)
        
        response_stripped = response.strip()
        personalized_start = f"D·∫° {personal_address},"
        
        if not response_stripped.lower().startswith(f'd·∫° {personal_address.lower()}'):
            if response_stripped.lower().startswith('d·∫°'):
                response = personalized_start + ' ' + response_stripped[3:].strip()
            else:
                response = personalized_start + ' ' + response_stripped
        
        if not response.strip().endswith('c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?'):
            response = re.sub(r'\s*(c√≥ c·∫ßn.*?kh√¥ng ·∫°\?|C·∫ßn.*?kh√¥ng\?|C√≥.*?kh√¥ng\?)?\s*$', '', response.strip())
            response += f' {personal_address.title()} c√≥ c·∫ßn em h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?'
        
        # Clean up formatting artifacts
        response = re.sub(r'\*\*\d+\.\s*', '', response)
        response = re.sub(r'^\s*\d+\.\s*', '', response, flags=re.MULTILINE)
        response = re.sub(r'^\s*[‚Ä¢\-\*]\s*', '', response, flags=re.MULTILINE)
        response = re.sub(r'\*\*(.*?)\*\*', r'\1', response)
        
        return response.strip()

    def _get_external_api_fallback_response(self, api_data, personal_address):
        lecturer_info = api_data.get('lecturer_info', {})
        schedule_summary = api_data.get('schedule_summary', {})
        
        ten_giang_vien = lecturer_info.get('ten_giang_vien', personal_address)
        total_classes = schedule_summary.get('total_classes', 0)
        
        return f"""D·∫° {personal_address}, em ƒë√£ t√¨m th·∫•y th√¥ng tin t·ª´ h·ªá th·ªëng c·ªßa tr∆∞·ªùng:

üë§ Th√¥ng tin c·ªßa {ten_giang_vien}:
- M√£ gi·∫£ng vi√™n: {lecturer_info.get('ma_giang_vien', 'Kh√¥ng x√°c ƒë·ªãnh')}
- Ch·ª©c danh: {lecturer_info.get('chuc_danh', 'Kh√¥ng x√°c ƒë·ªãnh')}
- Email: {lecturer_info.get('gmail', 'Kh√¥ng c√≥')}

üìÖ L·ªãch gi·∫£ng d·∫°y: {total_classes} bu·ªïi h·ªçc ƒë∆∞·ª£c l√™n l·ªãch

ƒê·ªÉ xem chi ti·∫øt, {personal_address} c√≥ th·ªÉ truy c·∫≠p h·ªá th·ªëng qu·∫£n l√Ω ƒë√†o t·∫°o c·ªßa tr∆∞·ªùng ·∫°. üéì

{personal_address.title()} c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?"""

    def set_user_context(self, session_id: str, user_context: dict):
        
        print("\n" + "="*20 + " DEBUG: set_user_context " + "="*20)
        print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è [set_user_context] ƒêang c√†i ƒë·∫∑t context cho session: {session_id}")
        print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è [set_user_context] D·ªØ li·ªáu context nh·∫≠n ƒë∆∞·ª£c: {user_context}")
        if 'gender' in user_context:
            print(f"‚úÖ [set_user_context] T√åM TH·∫§Y 'gender' trong context: '{user_context['gender']}'")
        else:
            print(f"‚ùå [set_user_context] KH√îNG T√åM TH·∫§Y 'gender' trong context!")
        print("="*60 + "\n")
        
        self._user_context_cache[session_id] = user_context
        logger.info(f"‚úÖ Set user context for session {session_id}: {user_context.get('faculty_code', 'Unknown')}")

    def _get_personalized_system_prompt(self, session_id: str = None):
        try:
            personal_address = self._get_personal_address(session_id)
            user_context = self._user_context_cache.get(session_id, {})
            user_memory_prompt = user_context.get('preferences', {}).get('user_memory_prompt', '')
            return build_personalized_system_prompt(user_memory_prompt, personal_address)
        except Exception as e:
            logger.error(f"Error getting personalized prompt: {e}")
            return build_personalized_system_prompt()  # Fallback

    def generate_response(self, query: str, context: Optional[Dict] = None, 
                          intent_info: Optional[Dict] = None, entities: Optional[Dict] = None,
                          session_id: str = None) -> Dict[str, Any]:
        start_time = time.time()
        print(f"\n--- üöÄ ADVANCED RAG GENERATION REQUEST (Session: {session_id}) ---")
        
        try:
            # üî• NEW: X·ª≠ l√Ω c√°c Mode ƒë·∫∑c bi·ªát (Chat Only / General Knowledge)
            mode = context.get('mode') if context else None
            
            if mode in ['chat_only', 'general_knowledge']:
                personal_address = self._get_personal_address(session_id)
                
                if mode == 'chat_only':
                    # Prompt cho Chit-chat
                    prompt = f"""
                    B·∫°n l√† ChatBDU, tr·ª£ l√Ω ·∫£o c·ªßa ƒê·∫°i h·ªçc B√¨nh D∆∞∆°ng.
                    Ng∆∞·ªùi d√πng (x∆∞ng h√¥ l√† {personal_address}) ƒëang ch√†o h·ªèi ho·∫∑c h·ªèi v·ªÅ b·∫°n. 
                    H√£y tr·∫£ l·ªùi th√¢n thi·ªán, t·ª± nhi√™n, ng·∫Øn g·ªçn v√† x∆∞ng l√† 'em'.
                    
                    Ng∆∞·ªùi d√πng: {query}
                    """
                    strategy = 'chat_only'
                else:
                    # Prompt cho ki·∫øn th·ª©c chung (Fallback)
                    prompt = f"""
                    B·∫°n l√† ChatBDU. C√¢u h·ªèi n√†y n·∫±m ngo√†i d·ªØ li·ªáu n·ªôi b·ªô c·ªßa tr∆∞·ªùng.
                    Ng∆∞·ªùi d√πng (x∆∞ng h√¥ l√† {personal_address}) ƒëang h·ªèi: "{query}"
                    
                    H√£y tr·∫£ l·ªùi h·ªØu √≠ch d·ª±a tr√™n ki·∫øn th·ª©c chung c·ªßa b·∫°n. 
                    N·∫øu c√¢u h·ªèi qu√° chuy√™n s√¢u, h√£y kh√©o l√©o t·ª´ ch·ªëi v√† ƒë·ªÅ ngh·ªã li√™n h·ªá tr·ª±c ti·∫øp nh√† tr∆∞·ªùng.
                    X∆∞ng h√¥ l√† 'em'.
                    """
                    strategy = 'general_knowledge'

                # G·ªçi API (Ollama/Gemini)
                response = self._call_gemini_api_with_smart_tokens(
                    prompt, strategy, max_tokens=1000, session_id=session_id
                )
                
                if not response:
                    response = f"D·∫° {personal_address}, em ƒëang g·∫∑p ch√∫t tr·ª•c tr·∫∑c. {personal_address.title()} n√≥i l·∫°i ƒë∆∞·ª£c kh√¥ng ·∫°? üòì"

                # L∆∞u Memory
                if session_id:
                    self.memory.add_interaction(session_id, query, response, intent_info, entities)

                return {
                    'response': response,
                    'method': mode,
                    'strategy': strategy,
                    'confidence': 1.0 if mode == 'chat_only' else 0.5,
                    'generation_time': time.time() - start_time,
                    'original_query': query,
                    'personalized': True
                }

            # =================================================================
            # LOGIC C≈® (GI·ªÆ NGUY√äN CHO RAG V√Ä API X·ª¨ L√ù)
            # =================================================================
            
            original_query = query
            instruction = context.get('instruction', '') if context else ''
            
            if instruction == 'answer_from_document':
                logger.info("üìÑ DOCUMENT CONTEXT: Processing document-based query")
                
                document_text = context.get('document_text', '')
                if not document_text or not document_text.strip():
                    logger.warning("‚ö†Ô∏è Empty document text provided")
                    personal_address = self._get_personal_address(session_id)
                    response_confidence = self.confidence_manager.normalize_confidence(0.1, "document_error")
                    return {
                        'response': f"D·∫° {personal_address}, em kh√¥ng nh·∫≠n ƒë∆∞·ª£c n·ªôi dung t√†i li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. {personal_address.title()} c√≥ th·ªÉ g·ª≠i l·∫°i t√†i li·ªáu kh√¥ng ·∫°? üéì",
                        'method': 'document_context_empty',
                        'strategy': 'document_error',
                        'confidence': response_confidence, 
                        'generation_time': time.time() - start_time,
                        'original_query': original_query,
                        'restored_query': query,
                        'vietnamese_restoration_used': False,
                        'personalized': bool(session_id in self._user_context_cache),
                        'document_context_processed': True,
                        'token_info': {'smart_tokens_used': False, 'method': 'document_error'}
                    }
                prompt = self._build_document_context_prompt(query, document_text, session_id)
                optimal_tokens = self.token_manager.calculate_optimal_tokens(
                    len(prompt), 
                    'document_context'
                )
                logger.info(f"üìÑ Processing document context with {optimal_tokens} tokens")
                response = self._call_gemini_api_with_smart_tokens(
                    prompt, 'document_context', optimal_tokens, session_id
                )
                if not response:
                    personal_address = self._get_personal_address(session_id)
                    response = f"D·∫° {personal_address}, em g·∫∑p kh√≥ khƒÉn k·ªπ thu·∫≠t khi ph√¢n t√≠ch t√†i li·ªáu. {personal_address.title()} c√≥ th·ªÉ th·ª≠ l·∫°i ho·∫∑c ƒë·∫∑t c√¢u h·ªèi c·ª• th·ªÉ h∆°n kh√¥ng ·∫°? üéì"
                response_confidence = self.confidence_manager.calculate_response_confidence(
                    semantic_score=0.85,
                    keyword_score=0.0,
                    context_bonus=0.1,
                    method='document_context'
                )

                if session_id:
                    self.memory.add_interaction(session_id, original_query, response, intent_info, entities)

                return {
                    'response': response,
                    'method': 'document_context_processing',
                    'strategy': 'document_context',
                    'confidence': response_confidence,
                    'generation_time': time.time() - start_time,
                    'original_query': original_query,
                    'restored_query': query,
                    'vietnamese_restoration_used': False,
                    'personalized': bool(session_id in self._user_context_cache),
                    'document_context_processed': True,
                    'token_info': {
                        'smart_tokens_used': True,
                        'method': 'document_context_processing',
                        'optimal_tokens': optimal_tokens
                    }
                }
            
            if instruction == 'process_external_api_data':
                response = self._generate_external_api_response(query, context, session_id)
                response_confidence = self.confidence_manager.calculate_response_confidence(
                    semantic_score=0.9,
                    keyword_score=0.0,
                    context_bonus=0.15,
                    method='external_api'
                )
                token_info = {
                    'smart_tokens_used': True,
                    'method': 'external_api_processing'
                }
                if session_id:
                    self.memory.add_interaction(session_id, original_query, response, intent_info, entities)

                return {
                    'response': response,
                    'method': 'external_api_processing',
                    'strategy': 'external_api',
                    'confidence': response_confidence,
                    'generation_time': time.time() - start_time,
                    'original_query': original_query,
                    'restored_query': query,
                    'vietnamese_restoration_used': False,
                    'personalized': bool(session_id in self._user_context_cache),
                    'external_api_processed': True,
                    'token_info': token_info
                }

            conversation_context = {}
            if session_id:
                conversation_context = self.memory.get_conversation_context(session_id)
                print(f"üß† MEMORY DEBUG: History length = {len(conversation_context.get('history', []))}")
                print(f"üìù CONTEXT SUMMARY: {conversation_context.get('recent_conversation_summary', 'None')}")

            user_context = None
            if session_id and session_id in self._user_context_cache:
                user_context = self._user_context_cache[session_id]
                print(f"üë§ USER CONTEXT: {user_context.get('faculty_code', 'Unknown')}")

            response_strategy = self._determine_lecturer_response_strategy(
                query, context, intent_info, conversation_context
            )
            
            raw_confidence = context.get('confidence', 0.5) if context else 0.5
            normalized_confidence = self.confidence_manager.normalize_confidence(raw_confidence, "input_context")
            
            if context:
                context['confidence'] = normalized_confidence
            
            if instruction == 'direct_answer_lecturer':
                response, token_info = self._generate_direct_lecturer_answer_smart(query, context, session_id)
                final_confidence = normalized_confidence
            elif instruction in ['enhance_answer_lecturer', 'enhance_answer_lecturer_boosted']:
                response, token_info = self._generate_enhanced_lecturer_answer_smart(query, context, intent_info, entities, session_id)
                final_confidence = self.confidence_manager.normalize_confidence(normalized_confidence + 0.05, "enhanced_method")
            elif instruction == 'clarification_needed':
                response, token_info = self._generate_clarification_request_smart(query, context, session_id)
                final_confidence = self.confidence_manager.normalize_confidence(0.3, "clarification")
            elif instruction == 'dont_know_lecturer':
                response, token_info = self._generate_dont_know_response_smart(query, context, session_id)
                final_confidence = self.confidence_manager.normalize_confidence(0.1, "dont_know")
            else:
                # Default case for normal RAG
                if not self._is_lecturer_education_related(query) and not context.get('force_education_response', False):
                    response = self._get_contextual_out_of_scope_response_lecturer(conversation_context, session_id)
                    token_info = {'smart_tokens_used': False, 'method': 'predefined_template'}
                    final_confidence = self.confidence_manager.normalize_confidence(0.9, "out_of_scope")
                    if session_id:
                        self.memory.add_interaction(session_id, original_query, response, intent_info, entities)
                    return {
                        'response': response,
                        'method': 'out_of_scope_lecturer',
                        'confidence': final_confidence,
                        'generation_time': time.time() - start_time,
                        'original_query': original_query,
                        'restored_query': query,
                        'personalized': session_id in self._user_context_cache,
                        'token_info': token_info
                    }
                response, token_info = self._generate_smart_response(query, context, session_id, response_strategy)
                semantic_score = context.get('semantic_score', 0.5) if context else 0.5
                keyword_score = context.get('keyword_score', 0.0) if context else 0.0
                
                final_confidence = self.confidence_manager.calculate_response_confidence(
                    semantic_score=semantic_score,
                    keyword_score=keyword_score,
                    context_bonus=0.05 if conversation_context.get('recent_conversation_summary') else 0.0,
                    method='two_stage_reranking' if context and context.get('two_stage_reranking_used') else 'hybrid'
                )
            
            final_response = response or self._get_smart_fallback_with_context_lecturer(query, intent_info, conversation_context, session_id)
            if not 'final_confidence' in locals():
                final_confidence = self.confidence_manager.normalize_confidence(normalized_confidence, "final_response")
            
            if session_id:
                print(f"üß† MEMORY DEBUG: Saving interaction to memory...")
                self.memory.add_interaction(session_id, original_query, final_response, intent_info, entities)
            
            return {
                'response': final_response,
                'method': f'advanced_rag_lecturer_aware_gemini_{response_strategy}',
                'strategy': response_strategy,
                'conversation_context': conversation_context,
                'confidence': final_confidence,
                'generation_time': time.time() - start_time,
                'original_query': original_query,
                'restored_query': query,
                'vietnamese_restoration_used': False,
                'personalized': bool(user_context),
                'enhanced_generation': response_strategy == 'enhanced_generation',
                'token_info': token_info,
                'confidence_management': {
                    'raw_confidence': raw_confidence,
                    'normalized_confidence': normalized_confidence,
                    'final_confidence': final_confidence,
                    'confidence_capped': final_confidence == 1.0,
                    'confidence_source': 'advanced_calculation'
                }
            }
        except Exception as e:
            logger.error(f"Gemini API error: {str(e)}")
            fallback_response = self._get_smart_fallback_with_context_lecturer(query, intent_info, conversation_context, session_id)
            error_confidence = self.confidence_manager.normalize_confidence(0.1, "error_fallback")
            if session_id:
                self.memory.add_interaction(session_id, original_query, fallback_response, intent_info, entities)
            return {
                'response': fallback_response,
                'method': 'lecturer_context_aware_fallback',
                'error': str(e),
                'confidence': error_confidence,
                'generation_time': time.time() - start_time,
                'original_query': original_query,
                'restored_query': query,
                'personalized': session_id in self._user_context_cache,
                'token_info': {'smart_tokens_used': False, 'method': 'fallback'}
            }
            
    def _generate_smart_response(self, query: str, context=None, session_id=None, strategy='balanced'):        
        prompt = self._build_enhanced_prompt(query, context, None, None, session_id)
        optimal_tokens = self.token_manager.calculate_optimal_tokens(
            len(prompt), 
            complexity_hint=strategy
        )
        
        print(f"üß† SMART TOKENS: {optimal_tokens} tokens")
        response = self._call_gemini_api_with_smart_tokens(prompt, strategy, optimal_tokens, session_id)
        
        if not response:
            return self._get_smart_fallback_with_context_lecturer(query, None, {}, session_id), {
                'smart_tokens_used': True, 'method': 'fallback_after_api_failure', 'tokens_attempted': optimal_tokens
            }
        completion_check = self.token_manager.is_response_incomplete(response)
        if completion_check['incomplete']:
            print(f"‚ö†Ô∏è INCOMPLETE RESPONSE detected: {completion_check['reason']}")
            completed_response = self._auto_complete_response(response, query, context, session_id, completion_check)
            
            if completed_response and completed_response != response:
                response = completed_response
                completion_check['auto_completed'] = True
                print(f"‚úÖ AUTO-COMPLETION successful")
            else:
                print(f"‚ö†Ô∏è AUTO-COMPLETION failed, using original")
        response = self._post_process_with_lecturer_consistency(response, query, context, strategy, {}, session_id)
        
        token_info = {
            'smart_tokens_used': True,
            'method': 'smart_generation',
            'optimal_tokens': optimal_tokens,
            'completion_check': completion_check,
            'strategy': strategy
        }
        
        return response, token_info

    def _auto_complete_response(self, incomplete_response: str, original_query: str, context, session_id: str, completion_info: Dict) -> Optional[str]:        
        if completion_info['confidence'] < 0.6:
            return None
        completion_tokens = self.token_manager.estimate_completion_tokens(incomplete_response)
        completion_prompt = self._build_completion_prompt(incomplete_response, original_query, context, session_id, completion_info)
        print(f"üîß AUTO-COMPLETION: Attempting with {completion_tokens} tokens")
        completion = self._call_gemini_api_with_smart_tokens(completion_prompt, 'completion', completion_tokens, session_id)
        if completion:
            if completion_info['reason'] == 'missing_proper_ending':
                personal_address = self._get_personal_address(session_id)
                return incomplete_response.rstrip() + f' {personal_address.title()} c√≥ c·∫ßn em h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?'
            elif completion_info['reason'] == 'missing_proper_greeting':
                personal_address = self._get_personal_address(session_id)
                return f"D·∫° {personal_address}, " + incomplete_response.lstrip()
            else:
                merged = self._merge_incomplete_and_completion(incomplete_response, completion)
                return merged
        
        return None
    def _build_completion_prompt(self, incomplete_response: str, original_query: str, context, session_id: str, completion_info: Dict) -> str:        
        system_prompt = self._get_personalized_system_prompt(session_id)
        personal_address = self._get_personal_address(session_id)
        
        if completion_info['reason'] == 'incomplete_pattern':
            completion_prompt = f"""
            {system_prompt}
            
            NHI·ªÜM V·ª§: HO√ÄN THI·ªÜN c√¢u tr·∫£ l·ªùi b·ªã c·∫Øt
            
            C√ÇU H·ªéI G·ªêC: {original_query}
            
            C√ÇU TR·∫¢ L·ªúI B·ªä C·∫ÆT:
            {incomplete_response}
            
            Y√äU C·∫¶U:
            - TI·∫æP T·ª§C vi·∫øt ƒë·ªÉ ho√†n thi·ªán c√¢u tr·∫£ l·ªùi
            - ƒê·∫£m b·∫£o k·∫øt th√∫c: "{personal_address.title()} c√≥ c·∫ßn em h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?"
            - CH·ªà VI·∫æT PH·∫¶N TI·∫æP THEO, kh√¥ng l·∫∑p l·∫°i ph·∫ßn ƒë√£ c√≥
            
            Ti·∫øp t·ª•c:"""
        else:
            completion_prompt = f"""
            {system_prompt}
            
            NHI·ªÜM V·ª§: S·ª¨A L·ªñI v√† ho√†n thi·ªán c√¢u tr·∫£ l·ªùi
            
            C√ÇU H·ªéI G·ªêC: {original_query}
            
            C√ÇU TR·∫¢ L·ªúI C√ì V·∫§N ƒê·ªÄ:
            {incomplete_response}
            
            V·∫§N ƒê·ªÄ PH√ÅT HI·ªÜN: {completion_info['reason']}
            
            Y√äU C·∫¶U:
            - S·ª¨A L·ªñI v√† vi·∫øt l·∫°i c√¢u tr·∫£ l·ªùi HO√ÄN CH·ªàNH
            - B·∫Øt ƒë·∫ßu: "D·∫° {personal_address},"
            - K·∫øt th√∫c: "{personal_address.title()} c√≥ c·∫ßn em h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?"
            
            C√¢u tr·∫£ l·ªùi ho√†n ch·ªânh:"""
        
        return completion_prompt

    def _merge_incomplete_and_completion(self, incomplete: str, completion: str) -> str:
        completion = completion.strip()
        completion = re.sub(r'^(d·∫°\s+(th·∫ßy|c√¥|gi·∫£ng vi√™n),?\s*)', '', completion, flags=re.IGNORECASE)
        incomplete_words = incomplete.split()
        if incomplete_words:
            last_word = incomplete_words[-1].lower()
            if last_word in ['v√†', 'v·ªõi', 'ƒë·ªÉ', 'khi', 'n·∫øu', 't·∫°i', 'v·ªÅ', 'cho', 'trong', 'c·ªßa', 't·ª´']:
                incomplete = ' '.join(incomplete_words[:-1])
        
        merged = incomplete.rstrip() + ' ' + completion.lstrip()
        return merged

    def _get_personal_address(self, session_id: str) -> str:
        # Debug Visuals
        logger.info("\n" + "="*20 + " DEBUG: _get_personal_address " + "="*20)
        logger.info(f"üïµÔ∏è‚Äç‚ôÇÔ∏è [_get_personal_address] ƒêang l·∫•y x∆∞ng h√¥ cho session: {session_id}")
        
        # L·∫•y context t·ª´ cache
        user_context = self._user_context_cache.get(session_id, {}) if session_id else {}
        logger.info(f"üïµÔ∏è‚Äç‚ôÇÔ∏è [_get_personal_address] Context ƒë·ªçc t·ª´ cache: {user_context}")

        full_name = user_context.get('full_name', '')
        raw_gender = user_context.get('gender', 'other')

        # Chu·∫©n h√≥a gi·ªõi t√≠nh (x·ª≠ l√Ω c·∫£ tr∆∞·ªùng h·ª£p s·ªë 0, 1 ho·∫∑c chu·ªói)
        gender = 'other'
        if str(raw_gender).lower() in ['male', 'nam', '0']:
            gender = 'male'
        elif str(raw_gender).lower() in ['female', 'n·ªØ', '1']:
            gender = 'female'

        logger.info(f"üïµÔ∏è‚Äç‚ôÇÔ∏è [_get_personal_address] Gi·ªõi t√≠nh g·ªëc: '{raw_gender}' -> ƒê√£ chu·∫©n h√≥a: '{gender}'")
        
        # Logic x√°c ƒë·ªãnh x∆∞ng h√¥
        salutation = ''
        if gender == 'male':
            salutation = 'th·∫ßy'
        elif gender == 'female':
            salutation = 'c√¥'
        
        # Tr∆∞·ªùng h·ª£p KH√îNG x√°c ƒë·ªãnh ƒë∆∞·ª£c gi·ªõi t√≠nh (other)
        if not salutation:
            if full_name:
                logger.info(f"‚úÖ [_get_personal_address] -> Tr·∫£ v·ªÅ t√™n ƒë·∫ßy ƒë·ªß (kh√¥ng r√µ gi·ªõi t√≠nh): '{full_name}'")
                logger.info("="*60 + "\n")
                return full_name
            else:
                logger.info(f"‚úÖ [_get_personal_address] -> Tr·∫£ v·ªÅ fallback: 'gi·∫£ng vi√™n'")
                logger.info("="*60 + "\n")
                return 'gi·∫£ng vi√™n'

        # Tr∆∞·ªùng h·ª£p C√ì gi·ªõi t√≠nh (male/female)
        if full_name:
            # L·∫•y t√™n cu·ªëi (VD: D∆∞∆°ng Anh Tu·∫•n -> Tu·∫•n)
            name_suffix = full_name.strip().split()[-1]
            address = f"{salutation} {name_suffix}"
            logger.info(f"‚úÖ [_get_personal_address] -> Tr·∫£ v·ªÅ x∆∞ng h√¥: '{address}'")
            logger.info("="*60 + "\n")
            return address
        
        # C√≥ gi·ªõi t√≠nh nh∆∞ng kh√¥ng c√≥ t√™n
        logger.info(f"‚úÖ [_get_personal_address] -> Tr·∫£ v·ªÅ x∆∞ng h√¥: '{salutation}'")
        logger.info("="*60 + "\n")
        return salutation

    def _call_gemini_api_with_smart_tokens(self, prompt: str, strategy: str, max_tokens: int, session_id: str = None, retry_count=0) -> Optional[str]:
        api_key_to_use = self.key_manager.get_key()
        if not api_key_to_use:
            if retry_count == 0:
                logger.warning("All keys are limited. Waiting 5 seconds before one last retry...")
                time.sleep(5)
                return self._call_gemini_api_with_smart_tokens(prompt, strategy, max_tokens, session_id, retry_count=1)
            else:
                logger.error("CRITICAL: All Gemini API keys are rate-limited. Aborting call.")
                personal_address = self._get_personal_address(session_id)
                return f"D·∫° {personal_address}, hi·ªán t·∫°i h·ªá th·ªëng ƒëang qu√° t·∫£i, t·∫•t c·∫£ c√°c k·∫øt n·ªëi ƒë·ªÅu ƒëang b·∫≠n. Vui l√≤ng th·ª≠ l·∫°i sau kho·∫£ng 1 ph√∫t n·ªØa ·∫°. üò•"

        try:
            headers = {'Content-Type': 'application/json'}
            
            strategy_temp_adjustments = {
                'quick_clarify': -0.2, 'direct_enhance': 0.0, 'enhanced_generation': +0.2,
                'completion': -0.3, 'balanced': 0.0, 'document_context': +0.1,
                'two_stage_reranking': +0.05
            }
            temp_adjustment = strategy_temp_adjustments.get(strategy, 0.0)
            final_temperature = max(0.1, min(1.0, self.default_generation_config["temperature"] + temp_adjustment))
            
            config = {
                "temperature": final_temperature, "maxOutputTokens": max_tokens,
                "topP": self.default_generation_config["topP"]
            }
            
            data = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": config,
                "safetySettings": [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
                ]
            }
            
            url = f"{self.base_url}?key={api_key_to_use}"
            response = requests.post(url, headers=headers, json=data, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and result['candidates']:
                    candidate = result['candidates'][0]
                    if 'finishReason' in candidate and candidate['finishReason'] == 'SAFETY':
                        logger.warning("üö® Gemini response blocked due to SAFETY reasons.")
                        personal_address = self._get_personal_address(session_id)
                        return f"D·∫° {personal_address}, em kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y v√¨ l√Ω do an to√†n v√† ch√≠nh s√°ch n·ªôi dung."
                    
                    if 'content' in candidate and 'parts' in candidate['content']:
                        return candidate['content']['parts'][0]['text']
            
            elif response.status_code == 429:
                self.key_manager.report_failure(api_key_to_use)
                if retry_count == 0:
                    logger.warning(f"Rate limit on key. Retrying immediately with a new key...")
                    return self._call_gemini_api_with_smart_tokens(prompt, strategy, max_tokens, session_id, retry_count=1)
                else:
                    logger.error("Rate limit hit on retry attempt as well. Aborting call.")
                    personal_address = self._get_personal_address(session_id)
                    return f"D·∫° {personal_address}, hi·ªán t·∫°i h·ªá th·ªëng ƒëang qu√° t·∫£i. Vui l√≤ng th·ª≠ l·∫°i sau √≠t ph√∫t ·∫°."
            
            else:
                logger.error(f"Gemini API Error {response.status_code} with key '{api_key_to_use[:4]}...': {response.text}")
            
            return None
        
        except requests.exceptions.Timeout:
            logger.error("Gemini API call timed out.")
            personal_address = self._get_personal_address(session_id)
            return f"D·∫° {personal_address}, y√™u c·∫ßu x·ª≠ l√Ω m·∫•t qu√° nhi·ªÅu th·ªùi gian v√† ƒë√£ b·ªã ng·∫Øt. {personal_address.title()} c√≥ th·ªÉ th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi ng·∫Øn g·ªçn h∆°n kh√¥ng ·∫°?"
        except Exception as e:
            logger.error(f"Smart Gemini API call failed: {str(e)}")
            return None

    def _generate_direct_lecturer_answer_smart(self, query, context, session_id=None):
        personal_address = self._get_personal_address(session_id)
        
        system_prompt = self._get_personalized_system_prompt(session_id)
        db_answer = context.get('db_answer', context.get('response', ''))
        db_answer = (db_answer[:3500] + '...') if len(db_answer) > 3500 else db_answer

        conversation_context = self.memory.get_conversation_context(session_id) if session_id else {}
        recent_summary = conversation_context.get('recent_conversation_summary', '')
        
        context_section = ""
        if recent_summary:
            context_section = f"""
üó£Ô∏è NG·ªÆ C·∫¢NH H·ªòI THO·∫†I G·∫¶N ƒê√ÇY:
{recent_summary}

üí° L∆ØU √ù: Tham kh·∫£o ng·ªØ c·∫£nh tr√™n ƒë·ªÉ tr√°nh l·∫∑p l·∫°i th√¥ng tin, t·∫°o c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c.
"""
        prompt = f"""{system_prompt}

---
B·ªêI C·∫¢NH V√Ä NHI·ªÜM V·ª§

1.  **Ki·∫øn th·ª©c n·ªÅn (t·ª´ CSDL):**
    "{db_answer}"

2.  **C√¢u h·ªèi c·ªßa gi·∫£ng vi√™n:**
    "{query}"

{context_section}

3.  **Y√äU C·∫¶U CU·ªêI C√ôNG (QUAN TR·ªåNG):**
    Nhi·ªám v·ª• ch√≠nh c·ªßa b·∫°n b√¢y gi·ªù l√† **nh·∫≠p vai m·ªôt tr·ª£ l√Ω AI** v·ªõi c√°c ƒë·∫∑c ƒëi·ªÉm v√† quy t·∫Øc ƒë∆∞·ª£c gi·∫£ng vi√™n ƒë·ªãnh nghƒ©a trong ph·∫ßn "GHI NH·ªö RI√äNG".
    H√£y s·ª≠ d·ª•ng "Ki·∫øn th·ª©c n·ªÅn" ƒë·ªÉ tr·∫£ l·ªùi "C√¢u h·ªèi c·ªßa gi·∫£ng vi√™n" trong khi v·∫´n duy tr√¨ ƒë√∫ng vai tr√≤ ƒë√≥.
    N·∫øu "GHI NH·ªö RI√äNG" tr·ªëng, h√£y tr·∫£ l·ªùi m·ªôt c√°ch chuy√™n nghi·ªáp, r√µ r√†ng theo quy t·∫Øc m·∫∑c ƒë·ªãnh.
    T·∫°o c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c, t·ª± nhi√™n, tr√°nh l·∫∑p l·∫°i th√¥ng tin ƒë√£ th·∫£o lu·∫≠n.
---
Tr·∫£ l·ªùi:
"""

        optimal_tokens = self.token_manager.calculate_optimal_tokens(len(prompt), 'direct_enhance')
        response = self._call_gemini_api_with_smart_tokens(prompt, 'direct_enhance', optimal_tokens, session_id)
        
        fallback = f"D·∫° {personal_address}, {db_answer} üéì {personal_address.title()} c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?"
        
        token_info = {
            'smart_tokens_used': True, 
            'method': 'direct_answer_smart_v6_advanced_confidence', 
            'optimal_tokens': optimal_tokens,
            'personal_addressing': personal_address,
            'context_aware': bool(recent_summary),
            'confidence_managed': True
        }

        return response or fallback, token_info

    def _generate_enhanced_lecturer_answer_smart(self, query, context, intent_info, entities, session_id):
        personal_address = self._get_personal_address(session_id)
        system_prompt = self._get_personalized_system_prompt(session_id)
        db_answer = context.get('db_answer', context.get('response', ''))
        db_answer = (db_answer[:3500] + '...') if len(db_answer) > 3500 else db_answer

        conversation_context = self.memory.get_conversation_context(session_id) if session_id else {}
        recent_summary = conversation_context.get('recent_conversation_summary', '')
        
        context_section = ""
        if recent_summary:
            context_section = f"""
üó£Ô∏è NG·ªÆ C·∫¢NH H·ªòI THO·∫†I G·∫¶N ƒê√ÇY:
{recent_summary}

üí° L∆ØU √ù: Tham kh·∫£o ng·ªØ c·∫£nh tr√™n ƒë·ªÉ tr√°nh l·∫∑p l·∫°i th√¥ng tin, t·∫°o c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c v√† t·ª± nhi√™n.
"""

        prompt = f"""{system_prompt}

---
B·ªêI C·∫¢NH V√Ä NHI·ªÜM V·ª§

1.  **Ki·∫øn th·ª©c n·ªÅn (t·ª´ CSDL):**
    "{db_answer}"

2.  **C√¢u h·ªèi c·ªßa gi·∫£ng vi√™n:**
    "{query}"

{context_section}

3.  **Y√äU C·∫¶U CU·ªêI C√ôNG (QUAN TR·ªåNG):**
    Nhi·ªám v·ª• ch√≠nh c·ªßa b·∫°n b√¢y gi·ªù l√† **nh·∫≠p vai m·ªôt tr·ª£ l√Ω AI** v·ªõi c√°c ƒë·∫∑c ƒëi·ªÉm v√† quy t·∫Øc ƒë∆∞·ª£c gi·∫£ng vi√™n ƒë·ªãnh nghƒ©a trong ph·∫ßn "GHI NH·ªö RI√äNG".
    H√£y s·ª≠ d·ª•ng "Ki·∫øn th·ª©c n·ªÅn" ƒë·ªÉ tr·∫£ l·ªùi "C√¢u h·ªèi c·ªßa gi·∫£ng vi√™n" trong khi v·∫´n duy tr√¨ ƒë√∫ng vai tr√≤ ƒë√≥.
    N·∫øu "GHI NH·ªö RI√äNG" tr·ªëng, h√£y tr·∫£ l·ªùi m·ªôt c√°ch chuy√™n nghi·ªáp, r√µ r√†ng theo quy t·∫Øc m·∫∑c ƒë·ªãnh.
    T·∫°o c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c, t·ª± nhi√™n, tr√°nh l·∫∑p l·∫°i th√¥ng tin ƒë√£ th·∫£o lu·∫≠n.
    ƒê·∫∂C BI·ªÜT: T·∫°o c√¢u tr·∫£ l·ªùi chi ti·∫øt v√† to√†n di·ªán h∆°n.
---
Tr·∫£ l·ªùi:
"""

        complexity_hint = 'enhanced_generation' if context.get('generation_boosted', False) else 'two_stage_reranking'
        optimal_tokens = self.token_manager.calculate_optimal_tokens(len(prompt), complexity_hint)
        response = self._call_gemini_api_with_smart_tokens(prompt, complexity_hint, optimal_tokens, session_id)
        
        fallback = f"D·∫° {personal_address}, {db_answer} üéì {personal_address.title()} c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?"
        
        token_info = {
            'smart_tokens_used': True, 
            'method': 'enhanced_answer_smart_v6_advanced_confidence', 
            'optimal_tokens': optimal_tokens, 
            'generation_boosted': context.get('generation_boosted', False),
            'context_aware': bool(recent_summary),
            'confidence_managed': True,
            'two_stage_compatible': True
        }

        return response or fallback, token_info

    def _generate_clarification_request_smart(self, query, context, session_id=None):        
        personal_address = self._get_personal_address(session_id)
        
        clarification_templates = {
            'friendly': f"D·∫° {personal_address}, ƒë·ªÉ em c√≥ th·ªÉ h·ªó tr·ª£ {personal_address} t·ªët nh·∫•t, {personal_address} c√≥ th·ªÉ chia s·∫ª th√™m chi ti·∫øt v·ªÅ v·∫•n ƒë·ªÅ n√†y ƒë∆∞·ª£c kh√¥ng ·∫°? üòä Em r·∫•t s·∫µn l√≤ng gi√∫p ƒë·ª°!",
            'brief': f"D·∫° {personal_address}, c·∫ßn th√™m th√¥ng tin chi ti·∫øt ·∫°. üéì",
            'technical': f"D·∫° {personal_address}, ƒë·ªÉ cung c·∫•p h∆∞·ªõng d·∫´n k·ªπ thu·∫≠t ch√≠nh x√°c, {personal_address} vui l√≤ng cung c·∫•p th√™m th√¥ng s·ªë v√† y√™u c·∫ßu c·ª• th·ªÉ ·∫°.",
            'detailed': f"D·∫° {personal_address}, ƒë·ªÉ em c√≥ th·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi to√†n di·ªán v√† chi ti·∫øt nh·∫•t, {personal_address} c√≥ th·ªÉ b·ªï sung th√™m v·ªÅ b·ªëi c·∫£nh, m·ª•c ƒë√≠ch s·ª≠ d·ª•ng, v√† c√°c y√™u c·∫ßu c·ª• th·ªÉ kh√¥ng ·∫°? ƒêi·ªÅu n√†y s·∫Ω gi√∫p em h·ªó tr·ª£ {personal_address} m·ªôt c√°ch hi·ªáu qu·∫£ nh·∫•t.",
            'professional': f"D·∫° {personal_address}, ƒë·ªÉ em h·ªó tr·ª£ ch√≠nh x√°c nh·∫•t, {personal_address} c√≥ th·ªÉ n√≥i r√µ h∆°n v·ªÅ v·∫•n ƒë·ªÅ c·∫ßn h·ªó tr·ª£ kh√¥ng ·∫°? üéì"
        }
        
        response = clarification_templates.get('professional', clarification_templates['professional'])
        
        token_info = {
            'smart_tokens_used': False,
            'method': 'clarification_template_v2',
            'confidence_managed': True,
            'template_type': 'professional'
        }
        
        return response, token_info

    def _generate_dont_know_response_smart(self, query, context, session_id=None):        
        personal_address = self._get_personal_address(session_id)
        query_lower = query.lower()
        if any(word in query_lower for word in ['ng√¢n h√†ng ƒë·ªÅ', 'ƒë·ªÅ thi', 'kh·∫£o th√≠']):
            dept = "Ph√≤ng ƒê·∫£m b·∫£o ch·∫•t l∆∞·ª£ng v√† Kh·∫£o th√≠"
            contact = "ldkham@bdu.edu.vn"
        elif any(word in query_lower for word in ['k√™ khai', 'nhi·ªám v·ª•', 'gi·ªù chu·∫©n']):
            dept = "Ph√≤ng T·ªï ch·ª©c - C√°n b·ªô"
            contact = "tcccb@bdu.edu.vn"
        elif any(word in query_lower for word in ['t·∫°p ch√≠', 'nghi√™n c·ª©u', 'khoa h·ªçc']):
            dept = "Ph√≤ng Nghi√™n c·ª©u - H·ª£p t√°c"
            contact = "nghiencuu@bdu.edu.vn"
        elif any(word in query_lower for word in ['khen th∆∞·ªüng', 'thi ƒëua']):
            dept = "Ph√≤ng T·ªï ch·ª©c - C√°n b·ªô"
            contact = "tcccb@bdu.edu.vn"
        else:
            dept = "ph√≤ng ban li√™n quan"
            contact = "info@bdu.edu.vn"
        response = f"D·∫° {personal_address}, em ch∆∞a c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y. {personal_address.title()} c√≥ th·ªÉ li√™n h·ªá {dept} qua email {contact} ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ chi ti·∫øt ·∫°. üéì"
        
        token_info = {
            'smart_tokens_used': False,
            'method': 'dont_know_template_v2',
            'suggested_department': dept,
            'personal_addressing': personal_address,
            'confidence_managed': True
        }
        
        return response, token_info

    def _determine_lecturer_response_strategy(self, query, context, intent_info, conversation_context):        
        has_real_history = bool(conversation_context.get('history') and len(conversation_context['history']) > 0)
        
        print(f"üîç LECTURER STRATEGY DEBUG: has_real_history = {has_real_history}")
        
        if has_real_history:
            last_interaction = conversation_context['history'][-1]
            last_query = last_interaction['user_query'].lower()
            current_query = query.lower()
            
            lecturer_topics = {
                'ng√¢n h√†ng ƒë·ªÅ thi': ['ng√¢n h√†ng', 'ƒë·ªÅ thi', 'ƒë·ªÅ', 'kh·∫£o th√≠'],
                'k√™ khai nhi·ªám v·ª•': ['k√™ khai', 'nhi·ªám v·ª•', 'gi·ªù chu·∫©n'],
                't·∫°p ch√≠ khoa h·ªçc': ['t·∫°p ch√≠', 'b√†i vi·∫øt', 'nghi√™n c·ª©u'],
                'thi ƒëua khen th∆∞·ªüng': ['thi ƒëua', 'khen th∆∞·ªüng', 'danh hi·ªáu'],
                'b√°o c√°o': ['b√°o c√°o', 'n·ªôp', 'h·∫°n cu·ªëi'],
                'l·ªãch gi·∫£ng d·∫°y': ['l·ªãch', 'gi·∫£ng d·∫°y', 'th·ªùi kh√≥a bi·ªÉu']
            }
            
            last_main_topic = None
            for topic, keywords in lecturer_topics.items():
                if any(kw in last_query for kw in keywords):
                    last_main_topic = topic
                    break
            
            current_main_topic = None
            for topic, keywords in lecturer_topics.items():
                if any(kw in current_query for kw in keywords):
                    current_main_topic = topic
                    break

            has_exact_same_topic = last_main_topic is not None and last_main_topic == current_main_topic
            strong_continuation_words = ['c√≤n', 'th√™m', 'n·ªØa', 'kh√°c', 'v√†', 'ti·∫øp theo']
            has_strong_continuation = any(word in current_query.split() for word in strong_continuation_words)
            strong_clarification_words = ['c·ª• th·ªÉ h∆°n', 'r√µ h∆°n', 'chi ti·∫øt h∆°n', 'gi·∫£i th√≠ch th√™m']
            has_strong_clarification = any(phrase in current_query for phrase in strong_clarification_words)
            memory_test_words = ['nh·ªõ kh√¥ng', 'h·ªèi g√¨', 'n√≥i g√¨ tr∆∞·ªõc', 'v·ª´a n√≥i', 't·ªïng h·ª£p']
            is_memory_test = any(word in current_query for word in memory_test_words)

            if has_strong_continuation and has_exact_same_topic:
                return 'follow_up_continuation'
            if has_strong_clarification and has_exact_same_topic:
                return 'follow_up_clarification'
            if is_memory_test:
                return 'memory_reference'
            if current_main_topic is not None and last_main_topic is not None and current_main_topic != last_main_topic:
                return 'topic_shift'
        
        raw_confidence = context.get('confidence', 0.5) if context else 0.5
        normalized_confidence = self.confidence_manager.normalize_confidence(raw_confidence, "strategy_decision")
        
        if normalized_confidence > 0.75:
            return 'direct_enhance'
        if normalized_confidence > 0.4:
            return 'enhanced_generation'
        if intent_info and intent_info.get('intent') in ['greeting', 'general'] and len(query.split()) <= 5:
            return 'quick_clarify'
        if any(word in query.lower() for word in ['kh√≥ khƒÉn', 'c·∫ßn g·∫•p', 'h·∫°n cu·ªëi', 'urgent']):
            return 'supportive_brief'
        return 'balanced'

    def _post_process_with_lecturer_consistency(self, response, query, context, strategy, conversation_context, session_id=None):
        if not response:
            return response
        personal_address = self._get_personal_address(session_id)
        prohibited_phrases = [
            'v·ªõi t∆∞ c√°ch l√† sinh vi√™n', 't√¥i l√† h·ªçc sinh',
            'b·∫°n', 'm√¨nh', 'anh', 'ch·ªã', 'em l√† sinh vi√™n'
        ]
        for phrase in prohibited_phrases:
            if phrase.lower() in response.lower():
                response = response.replace(phrase, 'em l√† AI assistant c·ªßa BDU')
        response = re.sub(r'\bb·∫°n\b', personal_address, response, flags=re.IGNORECASE)
        response = re.sub(r'\bm√¨nh\b', 'em', response, flags=re.IGNORECASE)
        response = re.sub(r'\bt√¥i\b', 'em', response, flags=re.IGNORECASE)
        response_stripped = response.strip()
        personalized_start = f"D·∫° {personal_address},"
        if not response_stripped.lower().startswith(f'd·∫° {personal_address.lower()}'):
            if response_stripped.lower().startswith('d·∫°'):
                response = personalized_start + ' ' + response_stripped[3:].strip()
            else:
                response = personalized_start + ' ' + response_stripped
        proper_ending_pattern = r'(th·∫ßy|c√¥|gi·∫£ng vi√™n)\s+[^.!?]*c√≥\s+c·∫ßn.*?h·ªó tr·ª£.*?th√™m.*?g√¨.*?kh√¥ng.*?·∫°\?'
        
        if not re.search(proper_ending_pattern, response.lower()):
            response = re.sub(r'\s*üéì.*', '', response.strip())
            response = re.sub(r'\s*(c√≥ c·∫ßn.*?kh√¥ng ·∫°\?|C·∫ßn.*?kh√¥ng\?|C√≥.*?kh√¥ng\?).*', '', response.strip())
            if not response.strip().endswith(('.', '!', '?')):
                response += '.'
            response += f' {personal_address.title()} c√≥ c·∫ßn em h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°? üéì'
        response = re.sub(r'\*\*\d+\.\s*', '', response)
        response = re.sub(r'^\s*\d+\.\s*', '', response, flags=re.MULTILINE)
        response = re.sub(r'^\s*[‚Ä¢\-\*]\s*', '', response, flags=re.MULTILINE)
        response = re.sub(r'\*\*(.*?)\*\*', r'\1', response)
        duplicate_name_pattern = f'({re.escape(personal_address.title())}).*?\\1'
        response = re.sub(duplicate_name_pattern, r'\1', response)
        return response.strip()
    
    def _get_contextual_out_of_scope_response_lecturer(self, conversation_context, session_id=None):        
        personal_address = self._get_personal_address(session_id)
        user_context = self._user_context_cache.get(session_id, {}) if session_id else {}
        department_name = user_context.get('department_name', '')
        if conversation_context.get('context_summary'):
            if department_name:
                return f"D·∫° {personal_address}, em ch·ªâ h·ªó tr·ª£ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn c√¥ng vi·ªác gi·∫£ng vi√™n t·∫°i BDU th√¥i ·∫°! üéì {personal_address.title()} c√≤n mu·ªën h·ªèi g√¨ v·ªÅ {conversation_context['context_summary'].lower()} cho ng√†nh {department_name} kh√¥ng ·∫°?"
            else:
                return f"D·∫° {personal_address}, em ch·ªâ h·ªó tr·ª£ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn c√¥ng vi·ªác gi·∫£ng vi√™n t·∫°i BDU th√¥i ·∫°! üéì {personal_address.title()} c√≤n mu·ªën h·ªèi g√¨ v·ªÅ {conversation_context['context_summary'].lower()} kh√¥ng ·∫°?"
        
        if department_name:
            return f"D·∫° {personal_address}, em ch·ªâ h·ªó tr·ª£ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn c√¥ng vi·ªác gi·∫£ng vi√™n t·∫°i BDU th√¥i ·∫°! üéì {personal_address.title()} c√≥ c√¢u h·ªèi n√†o kh√°c v·ªÅ ng√†nh {department_name} kh√¥ng ·∫°?"
        else:
            return f"D·∫° {personal_address}, em ch·ªâ h·ªó tr·ª£ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn c√¥ng vi·ªác gi·∫£ng vi√™n t·∫°i BDU th√¥i ·∫°! üéì {personal_address.title()} c√≥ c√¢u h·ªèi n√†o kh√°c v·ªÅ tr∆∞·ªùng kh√¥ng ·∫°?"
    
    def _get_smart_fallback_with_context_lecturer(self, query, intent_info, conversation_context, session_id=None):        
        personal_address = self._get_personal_address(session_id)
        user_context = self._user_context_cache.get(session_id, {}) if session_id else {}
        department_name = user_context.get('department_name', '')
        
        intent_name = intent_info.get('intent', 'general') if intent_info else 'general'
        
        if conversation_context.get('context_summary'):
            summary = conversation_context['context_summary']
            context_fallbacks = {
                'ƒêang h·ªèi v·ªÅ ng√¢n h√†ng ƒë·ªÅ thi': f"D·∫° {personal_address}, v·ªÅ ng√¢n h√†ng ƒë·ªÅ thi, em c√≥ th·ªÉ h·ªó tr·ª£ th√™m! üìã {personal_address.title()} c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?",
                'ƒêang h·ªèi v·ªÅ k√™ khai nhi·ªám v·ª• nƒÉm h·ªçc': f"D·∫° {personal_address}, v·ªÅ k√™ khai nhi·ªám v·ª• nƒÉm h·ªçc, em c√≥ th·ªÉ h·ªó tr·ª£ th√™m! üìä {personal_address.title()} c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?",
                'ƒêang h·ªèi v·ªÅ t·∫°p ch√≠ khoa h·ªçc': f"D·∫° {personal_address}, v·ªÅ t·∫°p ch√≠ khoa h·ªçc, em c√≥ th·ªÉ h·ªó tr·ª£ th√™m! üìö {personal_address.title()} c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?",
                'ƒêang h·ªèi v·ªÅ thi ƒëua khen th∆∞·ªüng': f"D·∫° {personal_address}, v·ªÅ thi ƒëua khen th∆∞·ªüng, em c√≥ th·ªÉ h·ªó tr·ª£ th√™m! üèÜ {personal_address.title()} c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?"
            }
            if summary in context_fallbacks:
                return context_fallbacks[summary]
        
        smart_fallbacks = {
            'greeting': f"D·∫° ch√†o {personal_address}! üëã Em c√≥ th·ªÉ h·ªó tr·ª£ g√¨ cho {personal_address} v·ªÅ BDU ·∫°?",
            'general': f"D·∫° {personal_address}, em s·∫µn s√†ng h·ªó tr·ª£ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn BDU! üéì {personal_address.title()} c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?"
        }
        
        if department_name and intent_name == 'general':
            smart_fallbacks['general'] = f"D·∫° {personal_address}, em s·∫µn s√†ng h·ªó tr·ª£ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn BDU v√† ng√†nh {department_name}! üéì {personal_address.title()} c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?"
        
        return smart_fallbacks.get(intent_name, smart_fallbacks['general'])
    
    def _is_lecturer_education_related(self, query):
        lecturer_education_keywords = [
            'tr∆∞·ªùng', 'h·ªçc', 'sinh vi√™n', 'tuy·ªÉn sinh', 'h·ªçc ph√≠', 'ng√†nh', 
            'ƒë·∫°i h·ªçc', 'bdu', 'gv', 'gi·∫£ng vi√™n', 'd·∫°y', 'quy ƒë·ªãnh',
            'h·ªôi ƒë·ªìng', 'nghi√™n c·ª©u', 'c√¥ng t√°c', 'b√°o c√°o', 'ƒë√°nh gi√°',
            'thi ƒëua', 'th√†nh t√≠ch', 'khen th∆∞·ªüng', 'x√©t', 'x√©t thi ƒëua',
            'nhi·ªám v·ª•', 'ch·ª©c nƒÉng', 'ti√™u chu·∫©n', 'ti√™u ch√≠', 'ƒë·ªãnh m·ª©c',
            'ki·ªÉm tra', 'gi√°m s√°t', 'qu·∫£n l√Ω', 'k·∫øt qu·∫£', 'hi·ªáu qu·∫£',
            'ph√¢n c√¥ng', 'giao nhi·ªám v·ª•', 'tr√°ch nhi·ªám', 'chu·∫©n ƒë·∫ßu ra',
            'h·ªçc k·ª≥', 'nƒÉm h·ªçc', 'k·ª≥ thi', 'b√†i gi·∫£ng', 'gi√°o √°n',
            'l·ªõp h·ªçc', 'm√¥n h·ªçc', 'h·ªçc ph·∫ßn', 't√≠n ch·ªâ', 'c·ªë v·∫•n',
            'ng√¢n h√†ng ƒë·ªÅ thi', 'file m·ªÅm', 'n·ªôp', 'email', 'ph√≤ng ban',
            'k√™ khai', 'gi·ªù chu·∫©n', 'th·ªânh gi·∫£ng', 't·∫°p ch√≠', 'b√†i vi·∫øt',
            'ƒëi·ªÉm', 'ƒë·∫°t', 'kh√¥ng ƒë·∫°t', 'h·ªçc l·∫°i', 'n√¢ng ƒëi·ªÉm', 'c·∫£i thi·ªán ƒëi·ªÉm',
            'ƒëi·ªÉm trung b√¨nh', 'trung b√¨nh', 't√≠nh ƒëi·ªÉm', 't√≠nh',
            'chuy·ªÉn ƒë·ªïi', 'c√¥ng nh·∫≠n', 'kh·ªëi l∆∞·ª£ng', 't·ªëi thi·ªÉu', 'ch∆∞∆°ng tr√¨nh', 
            'ph·∫ßn trƒÉm', 't·ªëi ƒëa', 'gi·ªõi h·∫°n',
            't·ªët nghi·ªáp', 'l·ªÖ t·ªët nghi·ªáp', 'tham d·ª±', 'ƒë∆∞·ª£c ph√©p', 'b·∫±ng c·∫•p', 
            'vƒÉn b·∫±ng', 'c·ª≠ nh√¢n', 'c·∫•p b·∫±ng', 'nh·∫≠n b·∫±ng',
            'th∆∞·ªùng tr·ª±c', 'k·ª∑ lu·∫≠t', 'h·ªôi ƒë·ªìng thi ƒëua', 'danh s√°ch', 'th√†nh ph·∫ßn',
            'theo quy ƒë·ªãnh', 'quy ƒë·ªãnh v·ªÅ', 'th·ªÉ l·ªá', 'h∆∞·ªõng d·∫´n', 'th·ªß t·ª•c',
            'ƒëi·ªÅu ki·ªán', 'y√™u c·∫ßu',
            'nh∆∞ th·∫ø n√†o', 'bao nhi√™u', 'l√† ai', 'ai l√†', 'l√†m g√¨', '·ªü ƒë√¢u', 
            'khi n√†o', 'c√≥ ƒë∆∞·ª£c',
            
            'truong', 'hoc', 'sinh vien', 'tuyen sinh', 'hoc phi', 'nganh',
            'dai hoc', 'giang vien', 'day', 'quy dinh', 'nghien cuu',
            'thi dua', 'thanh tich', 'khen thuong', 'nhiem vu', 'chuc nang',
            'tieu chuan', 'tieu chi', 'dinh muc', 'kiem tra', 'giam sat',
            'quan ly', 'ket qua', 'hieu qua', 'phan cong', 'giao nhiem vu',
            'hoc ky', 'nam hoc', 'ky thi', 'bai giang', 'giao an',
            'lop hoc', 'mon hoc', 'hoc phan', 'tin chi', 'co van',
            'ngan hang de thi', 'file mem', 'ke khai', 'gio chuan',
            'thinh giang', 'tap chi', 'bai viet'
            'diem', 'dat', 'khong dat', 'hoc lai', 'nang diem', 'cai thien diem',
            'diem trung binh', 'trung binh', 'tb', 'dtb', 'tinh diem', 'tinh',
            'chuyen doi', 'cong nhan', 'khoi luong', 'toi thieu', 'chuong trinh',
            'phan tram', 'toi da', 'gioi han',
            'tot nghiep', 'le tot nghiep', 'tham du', 'duoc phep', 'bang cap',
            'van bang', 'cu nhan', 'cap bang', 'nhan bang',
            'thuong truc', 'ky luat', 'hoi dong thi dua', 'danh sach', 'thanh phan',
            'ai phu trach', 'theo quy dinh', 'quy dinh ve', 'the le', 'huong dan', 'thu tuc',
            'dieu kien', 'yeu cau', 'nhu the nao', 'bao nhieu', 'la ai',
            'ai la', 'lam gi', 'o dau', 'khi nao', 'co duoc'
        ]
        
        if not query:
            return False        
        query_lower = query.lower()
        return any(kw in query_lower for kw in lecturer_education_keywords)

    def _build_enhanced_prompt(self, query: str, context=None, intent_info=None, entities=None, session_id=None):
        system_prompt = self._get_personalized_system_prompt(session_id)
        personal_address = self._get_personal_address(session_id)
        
        context_info = str(context.get('response', '')) if isinstance(context, dict) else str(context or '')
        
        conversation_context = self.memory.get_conversation_context(session_id) if session_id else {}
        recent_summary = conversation_context.get('recent_conversation_summary', '')
        
        context_section = ""
        if recent_summary:
            context_section = f"""
üó£Ô∏è NG·ªÆ C·∫¢NH H·ªòI THO·∫†I G·∫¶N ƒê√ÇY:
{recent_summary}

üí° L∆ØU √ù: Tham kh·∫£o ng·ªØ c·∫£nh tr√™n ƒë·ªÉ tr√°nh l·∫∑p l·∫°i th√¥ng tin, t·∫°o c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c.
"""
        
        prompt = f"""{system_prompt}
        
C√ÇU H·ªéI: {query}
TH√îNG TIN: {context_info}

{context_section}

Y√äU C·∫¶U:
- B·∫Øt ƒë·∫ßu: "D·∫° {personal_address},"
- K·∫øt th√∫c: "{personal_address.title()} c√≥ c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng ·∫°?"
- T·∫°o c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c, t·ª± nhi√™n, tr√°nh l·∫∑p l·∫°i th√¥ng tin ƒë√£ th·∫£o lu·∫≠n

Tr·∫£ l·ªùi:"""
        return prompt
    
    def validate_user_preferences(self, preferences):
        errors, warnings = [], []
        if 'user_memory_prompt' in preferences:
            memory = preferences['user_memory_prompt']
            if isinstance(memory, str):
                if len(memory) > 1500:
                    errors.append("user_memory_prompt too long (max 1500 characters)")
                elif len(memory) > 1400:
                    warnings.append("user_memory_prompt approaching limit")
            else:
                errors.append("user_memory_prompt must be string")
        if 'department_priority' in preferences:
            if not isinstance(preferences['department_priority'], bool):
                errors.append("department_priority must be boolean")
        
        return {'valid': len(errors) == 0, 'errors': errors, 'warnings': warnings}
    
    def get_user_context(self, session_id: str):
        return self._user_context_cache.get(session_id)    
    def clear_user_context(self, session_id=None):
        if session_id:
            if session_id in self._user_context_cache:
                del self._user_context_cache[session_id]
        else:
            self._user_context_cache.clear()
    def get_conversation_memory(self, session_id: str):
        return self.memory.get_conversation_context(session_id)
    def clear_conversation_memory(self, session_id: str = None):
        if session_id:
            if session_id in self.memory.conversations:
                del self.memory.conversations[session_id]
        else:
            self.memory.conversations.clear()
    def get_system_status(self) -> Dict[str, Any]:
        try:
            test_prompt = "Test ng·∫Øn cho gi·∫£ng vi√™n"
            response = self._call_gemini_api_with_smart_tokens(test_prompt, 'quick_clarify', 80, session_id="test")
            
            return {
                'gemini_api_available': response is not None,
                'api_key_configured': bool(self.key_manager.keys),
                'service_status': 'active' if response else 'error',
                'mode': 'advanced_rag_gemini_with_two_stage_reranking_integration_and_advanced_confidence_management',
                'memory_sessions': len(self.memory.conversations),
                'personalization_sessions': len(self._user_context_cache),
                'adaptive_token_range': self.token_manager.adaptive_token_range,
                'confidence_management': {
                    'max_confidence': self.confidence_manager.MAX_CONFIDENCE,
                    'decision_thresholds': self.confidence_manager.decision_thresholds,
                    'calibration_rules': self.confidence_manager.confidence_calibration_rules,
                    'overflow_protection_enabled': True,
                    'confidence_normalization_active': True
                },
                'features': [
                    'advanced_confidence_management',
                    'confidence_overflow_protection',
                    'confidence_normalization',
                    'two_stage_reranking_integration',
                    'advanced_rag_compatibility',
                    'smart_token_management',
                    'auto_response_completion',
                    'adaptive_token_allocation',
                    'incomplete_response_detection',
                    'lecturer_conversation_memory',
                    'lecturer_role_consistency',
                    'lecturer_context_aware_responses',
                    'lecturer_follow_up_detection',
                    'lecturer_topic_shift_handling',
                    'lecturer_clarification_requests',
                    'lecturer_department_suggestions',
                    'personalized_system_prompts',
                    'personalized_addressing',
                    'department_specific_responses',
                    'user_memory_prompt_support',
                    'flexible_personalization',
                    'external_api_data_processing',
                    'lecturer_schedule_formatting',
                    'personal_information_handling',
                    'gender_based_addressing',
                    'conversation_context_summary',
                    'm·∫°ch_l·∫°c_response_generation',
                    'consistent_personalization_in_errors',
                    'session_id_propagation_in_api_calls',
                    'graceful_error_handling_with_personalization',
                    'document_context_processing',
                    'pdf_docx_text_extraction',
                    'document_based_question_answering',
                    'ocr_integration_support',
                    'fine_tuned_model_compatibility',
                    'cross_encoder_simulation_support',
                    'hybrid_retrieval_enhancement'
                ]
            }
        except Exception as e:
            return {
                'gemini_api_available': False,
                'service_status': 'error',
                'error': str(e),
                'consistent_personalization': True,
                'graceful_degradation': True,
                'document_context_support': True,
                'advanced_confidence_management': True,
                'confidence_overflow_protection': True
            }
            
class LocalQwenGenerator(GeminiResponseGenerator):
    def __init__(self):
        super().__init__()
        self.model_name = "qwen2.5:7b" 
        base_url = os.getenv("OLLAMA_API_URL", "http://localhost:11434")
        
        # X·ª≠ l√Ω ƒë·ªÉ ƒë·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n lu√¥n ƒë√∫ng l√† .../api/chat
        if base_url.endswith("/api/chat"):
            self.api_url = base_url
        else:
            # X√≥a d·∫•u / ·ªü cu·ªëi n·∫øu c√≥ ƒë·ªÉ tr√°nh b·ªã 2 d·∫•u //
            base_url = base_url.rstrip("/")
            self.api_url = f"{base_url}/api/chat"
            
        logger.info(f"üöÄ LocalQwenGenerator initialized using {self.model_name} via Ollama at {self.api_url}")

    def _call_gemini_api_with_smart_tokens(self, prompt: str, strategy: str, max_tokens: int, session_id: str = None, retry_count=0) -> Optional[str]:
        """
        Ghi ƒë√® h√†m g·ªçi API: Chuy·ªÉn h∆∞·ªõng sang Ollama Local
        """
        try:
            # Mapping tham s·ªë strategy sang temperature
            strategy_temp = {
                'quick_clarify': 0.1,      # Th·∫•p ƒë·ªÉ ch√≠nh x√°c
                'direct_enhance': 0.2, 
                'enhanced_generation': 0.5, 
                'completion': 0.1, 
                'balanced': 0.3,
                'document_context': 0.1,
                'chat_only': 0.6,          # Cao h∆°n ch√∫t ƒë·ªÉ t·ª± nhi√™n
                'general_knowledge': 0.4
            }
            temperature = strategy_temp.get(strategy, 0.3)

            # üõ†Ô∏è SYSTEM PROMPT "TH√âP": Ch·ªëng ti·∫øng Trung tuy·ªát ƒë·ªëi
            system_instruction = """
            B·∫°n l√† ChatBDU, tr·ª£ l√Ω ·∫£o Ti·∫øng Vi·ªát c·ªßa ƒê·∫°i h·ªçc B√¨nh D∆∞∆°ng.
            QUY T·∫ÆC B·∫§T DI B·∫§T D·ªäCH:
            1. NG√îN NG·ªÆ: CH·ªà tr·∫£ l·ªùi b·∫±ng Ti·∫øng Vi·ªát.
            2. C·∫§M: Tuy·ªát ƒë·ªëi KH√îNG s·ª≠ d·ª•ng ti·∫øng Trung Qu·ªëc (Chinese/Mandarin) trong b·∫•t k·ª≥ ho√†n c·∫£nh n√†o.
            3. D·ªäCH THU·∫¨T: N·∫øu th√¥ng tin ƒë·∫ßu v√†o l√† ti·∫øng Anh ho·∫∑c ti·∫øng Trung, h√£y d·ªãch m∆∞·ª£t m√† sang ti·∫øng Vi·ªát.
            4. X∆ØNG H√î: X∆∞ng l√† 'em' v√† g·ªçi ng∆∞·ªùi d√πng l√† 'th·∫ßy/c√¥' ho·∫∑c 'b·∫°n' t√πy ng·ªØ c·∫£nh ƒë√£ cung c·∫•p.
            """

            payload = {
                "model": self.model_name,
                "messages": [
                    {
                        "role": "system",
                        "content": system_instruction
                    },
                    {
                        "role": "user", 
                        # Nh·∫Øc l·∫°i m·ªôt l·∫ßn n·ªØa ·ªü cu·ªëi prompt ƒë·ªÉ model nh·ªõ k·ªπ
                        "content": f"{prompt}\n\n(L∆∞u √Ω: H√£y tr·∫£ l·ªùi ho√†n to√†n b·∫±ng ti·∫øng Vi·ªát)"
                    }
                ],
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": temperature,
                    "top_p": 0.9,
                    "repeat_penalty": 1.1, # Ph·∫°t l·∫∑p t·ª´ ƒë·ªÉ tr√°nh vƒÉn phong l·∫∑p l·∫°i
                    "num_ctx": 4096
                }
            }

            logger.info(f"ü§ñ Sending request to Local Ollama ({self.model_name}) | Strategy: {strategy}")
            
            # üëá D√≤ng n√†y gi·ªØ nguy√™n nh∆∞ Khang h·ªèi, n√≥ s·∫Ω d√πng self.api_url ƒë√£ setup ·ªü tr√™n
            response = requests.post(self.api_url, json=payload, timeout=120) 
            
            if response.status_code == 200:
                result = response.json()
                content = result.get('message', {}).get('content', '')
                return content
            else:
                logger.error(f"‚ùå Ollama API Error {response.status_code}: {response.text}")
                return None

        except requests.exceptions.ConnectionError:
            # S·ª≠a log m·ªôt ch√∫t ƒë·ªÉ bi·∫øt n√≥ ƒëang g·ªçi v√†o ƒë√¢u
            logger.critical(f"‚ùå Could not connect to Ollama at {self.api_url}! Make sure 'ollama serve' is running.")
            return "Xin l·ªói, h·ªá th·ªëng AI n·ªôi b·ªô ƒëang m·∫•t k·∫øt n·ªëi. Vui l√≤ng ki·ªÉm tra server Ollama."
        except Exception as e:
            logger.error(f"‚ùå Local LLM Error: {str(e)}")
            return None