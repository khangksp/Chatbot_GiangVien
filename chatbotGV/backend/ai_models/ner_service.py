import re
import logging
import time

logger = logging.getLogger(__name__)

class SimpleEntityExtractor:
    """Tr√≠ch xu·∫•t th·ª±c th·ªÉ ƒë∆°n gi·∫£n t·ª´ Q&A ƒë·ªÉ build context memory, ho·∫°t ƒë·ªông ƒë·ªôc l·∫≠p."""
    
    def __init__(self):
        # üîß C·∫¢I TI·∫æN: Patterns ch·∫∑t ch·∫Ω h∆°n cho c√°c lo·∫°i entity
        self.entity_patterns = {
            'person_name': [
                # üîß IMPROVED: T√™n ri√™ng ng∆∞·ªùi Vi·ªát (2-4 t·ª´, vi·∫øt hoa ƒë·∫ßu t·ª´) - CH·∫∂T CH·∫º H·ªöN
                r'\b([A-Z√Ä√Å√É·∫†·∫¢ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªê·ªí·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªë·ªì·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ªµ·ª∑·ªπƒë]+)\s+([A-Z√Ä√Å√É·∫†·∫¢ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªê·ªí·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªë·ªì·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ªµ·ª∑·ªπƒë]+)(?:\s+([A-Z√Ä√Å√É·∫†·∫¢ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªê·ªí·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªë·ªì·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ªµ·ª∑·ªπƒë]+))?(?:\s+([A-Z√Ä√Å√É·∫†·∫¢ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªê·ªí·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª∞·ª¶≈®∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªë·ªì·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ªµ·ª∑·ªπƒë]+))?\b',
                # Pattern v·ªõi ti·∫øn sƒ©, gi√°o s∆∞
                r'(?:GS\.TS\.|TS\.|GS\.|ti·∫øn sƒ©|gi√°o s∆∞)\s+([A-Z√Ä√Å√É·∫†·∫¢ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªê·ªí·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªë·ªì·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ªµ·ª∑·ªπƒë]+(?:\s+[A-Z√Ä√Å√É·∫†·∫¢ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªê·ªí·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª∞·ª¶≈®∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªë·ªì·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ªµ·ª∑·ªπƒë]+){1,2})'
            ],
            'position': [
                # üîß IMPROVED: M·ªü r·ªông danh s√°ch ch·ª©c danh ph·ªï bi·∫øn, ƒë·∫∑c bi·ªát trong ƒë·∫°i h·ªçc
                r'\b(hi·ªáu tr∆∞·ªüng|ph√≥ hi·ªáu tr∆∞·ªüng|tr∆∞·ªüng ph√≤ng|ph√≥ tr∆∞·ªüng ph√≤ng|tr∆∞·ªüng khoa|ph√≥ tr∆∞·ªüng khoa|gi√°o s∆∞|ph√≥ gi√°o s∆∞|ti·∫øn sƒ©|th·∫°c sƒ©|gi·∫£ng vi√™n|tr·ª£ gi·∫£ng|ch·ªß nhi·ªám b·ªô m√¥n|ph√≥ ch·ªß nhi·ªám b·ªô m√¥n|ch·ªß t·ªãch h·ªôi ƒë·ªìng|ph√≥ ch·ªß t·ªãch h·ªôi ƒë·ªìng)\b',
                r'\b(ch·ªß t·ªãch|ph√≥ ch·ªß t·ªãch|·ªßy vi√™n|th√†nh vi√™n|tr∆∞·ªüng ban|ph√≥ ban|gi√°m ƒë·ªëc|ph√≥ gi√°m ƒë·ªëc|tr∆∞·ªüng nh√≥m|ph√≥ nh√≥m|chuy√™n vi√™n|c·ªë v·∫•n|tr·ª£ l√Ω)\b'
            ],
            'department': [
                # üîß IMPROVED: M·ªü r·ªông patterns cho ph√≤ng ban, t·∫≠p trung v√†o ƒë·∫°i h·ªçc B√¨nh D∆∞∆°ng
                r'(khoa [^.!?]*|ph√≤ng [^.!?]*|ban [^.!?]*|b·ªô m√¥n [^.!?]*|vi·ªán [^.!?]*|trung t√¢m [^.!?]*|th∆∞ vi·ªán [^.!?]*|ph√≤ng th√≠ nghi·ªám [^.!?]*)',
                r'(ƒë·∫°i h·ªçc b√¨nh d∆∞∆°ng|bdu|tr∆∞·ªùng ƒë·∫°i h·ªçc b√¨nh d∆∞∆°ng|khoa c√¥ng ngh·ªá th√¥ng tin|khoa kinh t·∫ø|khoa lu·∫≠t|khoa k·ªπ thu·∫≠t|khoa ngo·∫°i ng·ªØ|khoa s∆∞ ph·∫°m|khoa y d∆∞·ª£c|ph√≤ng ƒë√†o t·∫°o|ph√≤ng t√†i ch√≠nh|ph√≤ng h√†nh ch√≠nh|ph√≤ng khoa h·ªçc c√¥ng ngh·ªá|ph√≤ng quan h·ªá qu·ªëc t·∫ø|ban qu·∫£n l√Ω k√Ω t√∫c x√°|ban an ninh)'
            ],
            'numbers': [
                r'(\d+(?:[.,]\d+)*(?:\s*(?:tri·ªáu|ngh√¨n|t·ª∑|ƒë·ªìng|vnƒë|usd|ph·∫ßn trƒÉm|%))?)',
                r'(\d+(?:\.\d+)?(?:\s*t√≠n ch·ªâ)?)'
            ],
            'dates': [
                r'(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
                r'(ng√†y \d{1,2}|th√°ng \d{1,2}|nƒÉm \d{4})',
                r'(h·ªçc k·ª≥ \d+|nƒÉm h·ªçc \d{4}-\d{4})'
            ],
            # üÜï TH√äM M·ªöI: Th√™m lo·∫°i th·ª±c th·ªÉ m·ªõi cho email, s·ªë ƒëi·ªán tho·∫°i, ƒë·ªãa ch·ªâ
            'email': [
                r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
            ],
            'phone_number': [
                r'(?:\+84|0)\d{9,10}'
            ],
            'address': [
                r'\b(s·ªë \d+|ƒë∆∞·ªùng [^,!?]*|ph∆∞·ªùng [^,!?]*|qu·∫≠n [^,!?]*|th√†nh ph·ªë [^,!?]*|t·ªânh b√¨nh d∆∞∆°ng)\b'
            ]
        }
        
        # üÜï TH√äM M·ªöI: Blacklist ƒë·ªÉ lo·∫°i b·ªè false positives
        self.person_name_blacklist = {
            # C√°c c·ª•m t·ª´ th∆∞·ªùng b·ªã nh·∫≠n nh·∫ßm l√† t√™n ng∆∞·ªùi
            'hoc phi chinh', 'quy khanh', 'duc tin', 'duc hanh', 'duc duc', 'nam duc',
            'hoc phi', 'chi phi', 'muc phi', 'le phi', 'phi le', 'thu phi',
            'quy dinh', 'quy che', 'quy trinh', 'quy tac', 'quy luat',
            'duc tinh', 'duc tich', 'duc hanh vi', 'duc han che',
            'nam hoc', 'nam tu', 'nam toi', 'nam sau', 'nam truoc',
            'tin chi', 'tin tin', 'chi tiet', 'chi tieu', 'chi phi',
            'bao cao', 'cao cap', 'cao dang', 'cap hoc', 'cap do',
            'sinh vien', 'giang vien', 'can bo', 'hoc sinh', 'nghien cuu sinh',
            'dai hoc', 'cao hoc', 'tien si', 'thac si', 'cu nhan',
            'mon hoc', 'bai hoc', 'gio hoc', 'lop hoc', 'hoc tap',
            # Th√™m c√°c t·ª´ kh√≥a BDU th∆∞·ªùng g·∫∑p
            'binh duong', 'bdu', 'truong dai hoc', 'phong ban', 'khoa hoc',
            'nghien cuu', 'dao tao', 'quan ly', 'hanh chinh', 'ky thuat',
            'cong nghe', 'kinh te', 'ngoai ngu', 'su pham', 'y khoa',
            # üÜï TH√äM: M·ªü r·ªông blacklist v·ªõi c√°c c·ª•m t·ª´ ph·ªï bi·∫øn kh√°c
            'hoc bong', 'hoc phi', 'chi nhanh', 'chi nhanh binh duong', 'quy trinh dang ky',
            'duc day du', 'nam thanh cong', 'hay lam', 'la tot', 'co the lam'
        }
        
        # üÜï TH√äM M·ªöI: Common Vietnamese words that are not names
        self.common_words_blacklist = {
            'co the', 'co ban', 'co so', 'co hoi', 'co quan', 'co mat',
            'la mot', 'la cach', 'la gi', 'la ai', 'la khi', 'la lieu',
            'duoc su', 'duoc cap', 'duoc phep', 'duoc biet', 'duoc tang',
            'hay la', 'hay khong', 'hay nhat', 'hay gi', 'hay co',
            'neu co', 'neu khong', 'neu la', 'neu can', 'neu muon',
            # üÜï TH√äM: M·ªü r·ªông v·ªõi c√°c c·ª•m t·ª´ ph·ªï bi·∫øn h∆°n
            'the nao', 'lam the nao', 'tai sao', 'vi sao', 'nhu the nao',
            'dang ky', 'dang nhap', 'hoc tap', 'nghien cuu', 'cong viec'
        }
        
        logger.info("‚úÖ IMPROVED SimpleEntityExtractor initialized with enhanced patterns and blacklists")

    def extract_entities(self, text, query_context=""):
        """üîß IMPROVED: Tr√≠ch xu·∫•t entities v·ªõi filtering t·ªët h∆°n"""
        if not text:
            return {}
            
        entities = {}
        text_cleaned = text.strip()
        
        # üîß IMPROVED: Clean text - remove common phrases but preserve names
        text_cleaned = re.sub(r'\b(d·∫°|·∫°|th∆∞a|xin ch√†o|ch√†o|em|anh|ch·ªã|c·∫£m ∆°n)\b', ' ', text_cleaned, flags=re.IGNORECASE)
        text_cleaned = re.sub(r'\s+', ' ', text_cleaned).strip()
        
        # Extract theo t·ª´ng lo·∫°i pattern
        for entity_type, patterns in self.entity_patterns.items():
            entities[entity_type] = []
            
            for pattern in patterns:
                matches = re.finditer(pattern, text_cleaned, re.IGNORECASE)
                for match in matches:
                    if entity_type == 'person_name':
                        # üîß SPECIAL HANDLING: Person names need more careful extraction
                        entity_value = self._extract_person_name_from_match(match)
                    else:
                        entity_value = match.group(1) if match.groups() else match.group(0)
                    
                    entity_value = entity_value.strip()
                    
                    # üîß IMPROVED: Strict filtering v·ªõi blacklist
                    if self._is_valid_entity(entity_value, entity_type):
                        # Normalize entity
                        if entity_type == 'person_name':
                            entity_value = self._normalize_person_name(entity_value)
                        else:
                            entity_value = entity_value.lower()
                            
                        if entity_value not in entities[entity_type]:
                            entities[entity_type].append(entity_value)
        
        # Ch·ªâ gi·ªØ l·∫°i entities c√≥ √≠t nh·∫•t 1 item
        entities = {k: v for k, v in entities.items() if v}
        
        logger.debug(f"üîç Entity extraction result: {entities}")
        return entities

    def _extract_person_name_from_match(self, match):
        """üÜï TH√äM M·ªöI: Tr√≠ch xu·∫•t t√™n ng∆∞·ªùi t·ª´ regex match"""
        if match.groups():
            # Combine all non-empty groups
            name_parts = [group for group in match.groups() if group and group.strip()]
            return ' '.join(name_parts)
        else:
            return match.group(0)

    def _is_valid_entity(self, entity_value, entity_type):
        """üîß IMPROVED: Validate entity quality v·ªõi blacklist m·ªü r·ªông"""
        if not entity_value or len(entity_value.strip()) < 3:
            return False
            
        entity_lower = entity_value.lower().strip()
        
        # üÜï CHECK BLACKLIST FIRST
        if entity_type == 'person_name':
            # Check person name blacklist
            if entity_lower in self.person_name_blacklist:
                logger.debug(f"üö´ Rejected by person blacklist: '{entity_value}'")
                return False
            
            # Check common words blacklist
            if entity_lower in self.common_words_blacklist:
                logger.debug(f"üö´ Rejected by common words blacklist: '{entity_value}'")
                return False
            
            # Check for parts in blacklist
            entity_words = entity_lower.split()
            for word in entity_words:
                if word in self.person_name_blacklist or word in self.common_words_blacklist:
                    logger.debug(f"üö´ Rejected by word-level blacklist: '{entity_value}' (word: '{word}')")
                    return False
        
        # Remove noise patterns
        noise_patterns = [
            r'\b(c√≥|c·∫ßn|th·ªÉ|th√™m|g√¨|kh√¥ng|h·ªó|tr·ª£|ƒë·ªÉ|em|l√†|ai|n√≥i|r√µ|h∆°n|v·ªÅ|v·∫•n|ƒë·ªÅ|ch√≠nh|x√°c|nh·∫•t)\b',
            r'^(v√†|ho·∫∑c|v·ªõi|ƒë·ªÉ|khi|n·∫øu|t·∫°i|v·ªÅ|cho|trong|c·ªßa|t·ª´)',
            r'(·∫°|√†|∆°i|nh√©)$'
        ]
        
        for pattern in noise_patterns:
            if re.search(pattern, entity_lower):
                logger.debug(f"üö´ Rejected by noise pattern: '{entity_value}' (pattern: {pattern})")
                return False
        
        # Specific validation by type
        if entity_type == 'person_name':
            # üîß STRICTER: Must have 2-4 words, each word >= 2 chars
            words = entity_lower.split()
            if len(words) < 2 or len(words) > 4:
                logger.debug(f"üö´ Rejected by word count: '{entity_value}' ({len(words)} words)")
                return False
            
            if any(len(word) < 2 for word in words):
                logger.debug(f"üö´ Rejected by word length: '{entity_value}'")
                return False
            
            # üÜï NEW: Check for Vietnamese name patterns
            if not self._looks_like_vietnamese_name(entity_value):
                logger.debug(f"üö´ Rejected by Vietnamese name pattern: '{entity_value}'")
                return False
            
            # Must not contain common Vietnamese particles
            if any(word in ['c√¥', 'th·∫ßy', 'anh', 'ch·ªã', 'em', 'd·∫°', 'ƒë∆∞·ª£c', 'ph·∫£i', 'theo', 'nh∆∞', 't·ª´'] for word in words):
                logger.debug(f"üö´ Rejected by particle words: '{entity_value}'")
                return False
        
        elif entity_type == 'position':
            # üÜï IMPROVED: Update valid positions to match expanded patterns
            valid_positions = [
                'hi·ªáu tr∆∞·ªüng', 'ph√≥ hi·ªáu tr∆∞·ªüng', 'tr∆∞·ªüng ph√≤ng', 'ph√≥ tr∆∞·ªüng ph√≤ng', 'tr∆∞·ªüng khoa', 'ph√≥ tr∆∞·ªüng khoa',
                'gi√°o s∆∞', 'ph√≥ gi√°o s∆∞', 'ti·∫øn sƒ©', 'th·∫°c sƒ©', 'gi·∫£ng vi√™n', 'tr·ª£ gi·∫£ng', 'ch·ªß nhi·ªám b·ªô m√¥n',
                'ph√≥ ch·ªß nhi·ªám b·ªô m√¥n', 'ch·ªß t·ªãch h·ªôi ƒë·ªìng', 'ph√≥ ch·ªß t·ªãch h·ªôi ƒë·ªìng', 'ch·ªß t·ªãch', 'ph√≥ ch·ªß t·ªãch',
                '·ªßy vi√™n', 'th√†nh vi√™n', 'tr∆∞·ªüng ban', 'ph√≥ ban', 'gi√°m ƒë·ªëc', 'ph√≥ gi√°m ƒë·ªëc', 'tr∆∞·ªüng nh√≥m',
                'ph√≥ nh√≥m', 'chuy√™n vi√™n', 'c·ªë v·∫•n', 'tr·ª£ l√Ω'
            ]
            if entity_lower not in valid_positions:
                logger.debug(f"üö´ Rejected invalid position: '{entity_value}'")
                return False
        
        elif entity_type == 'department':
            # üÜï NEW: Validate department with length and no numbers
            if len(entity_lower) < 5 or re.search(r'\d', entity_lower):
                logger.debug(f"üö´ Rejected invalid department: '{entity_value}'")
                return False
        
        logger.debug(f"‚úÖ Valid entity: '{entity_value}' (type: {entity_type})")
        return True
    
    def _looks_like_vietnamese_name(self, name):
        """üÜï TH√äM M·ªöI: Ki·ªÉm tra xem c√≥ gi·ªëng t√™n ng∆∞·ªùi Vi·ªát kh√¥ng"""
        name_lower = name.lower()
        words = name_lower.split()
        
        # Vietnamese surname patterns (h·ªç ph·ªï bi·∫øn)
        common_surnames = {
            'nguy·ªÖn', 'tr·∫ßn', 'l√™', 'ph·∫°m', 'ho√†ng', 'hu·ª≥nh', 'phan', 'v≈©', 'v√µ', 'ƒë·∫∑ng', 
            'b√πi', 'ƒë·ªó', 'h·ªì', 'ng√¥', 'd∆∞∆°ng', 'l√Ω', 'cao', 'ƒë·∫≠u', 'l∆∞u', 't√¥',
            'nguyen', 'tran', 'le', 'pham', 'hoang', 'huynh', 'phan', 'vu', 'vo', 'dang',
            'bui', 'do', 'ho', 'ngo', 'duong', 'ly', 'cao', 'dau', 'luu', 'to',
            # üÜï TH√äM: M·ªü r·ªông v·ªõi h·ªç ph·ªï bi·∫øn h∆°n
            'tr∆∞∆°ng', 'ƒë√†o', 'ƒëinh', 'l√¢m', 'mai', 't·∫°', 'h√†', 'v∆∞∆°ng', 'tri·ªáu', 'kh·ªïng'
        }
        
        # Check if first word (surname) is common Vietnamese surname
        if words[0] in common_surnames:
            return True
        
        # Vietnamese name characteristics
        # - Usually has balanced syllable structure
        # - Contains Vietnamese-specific characters or patterns
        vietnamese_chars = 'ƒÉ√¢√™√¥∆°∆∞√†√°·∫°·∫£√£·∫±·∫Ø·∫∑·∫≥·∫µ·∫ß·∫•·∫≠·∫©·∫´·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ·ªì·ªë·ªô·ªï·ªó·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë'
        
        # Count Vietnamese-specific characters
        vietnamese_char_count = sum(1 for char in name_lower if char in vietnamese_chars)
        
        # If has Vietnamese chars and reasonable length, likely a Vietnamese name
        if vietnamese_char_count >= 1 and len(words) >= 2:
            return True
        
        # Pattern check: avoid common false positives
        false_positive_patterns = [
            r'phi|ph√≠|fee',  # Avoid fee-related terms
            r'quy|q√∫y',      # Avoid regulation-related terms
            r'h·ªçc|hoc',      # Avoid study-related terms
            r'ch√≠|chi',      # Avoid will/credit-related terms
            # üÜï TH√äM: M·ªü r·ªông false positives
            r'b√¨nh|b√¨nh d∆∞∆°ng', 'duong', 'bdu', 'tr∆∞·ªùng', 'ƒë·∫°i h·ªçc', 'khoa', 'ph√≤ng', 'ban'
        ]
        
        for pattern in false_positive_patterns:
            if re.search(pattern, name_lower):
                # Double check: if it really contains Vietnamese name elements, still accept
                if vietnamese_char_count >= 2:  # Higher threshold for suspicious cases
                    continue
                else:
                    return False
        
        # Default: accept if passes basic structure checks
        return True
    
    def build_entity_relationships(self, query, answer, entities):
        """üîß IMPROVED: X√¢y d·ª±ng m·ªëi quan h·ªá gi·ªØa entities d·ª±a v√†o ng·ªØ c·∫£nh Q&A, th√™m confidence d·ª±a tr√™n proximity"""
        relationships = []
        
        query_lower = query.lower()
        answer_lower = answer.lower()
        full_context = query_lower + " " + answer_lower
        
        # T√¨m m·ªëi quan h·ªá person - position
        if 'person_name' in entities and 'position' in entities:
            for person in entities['person_name']:
                for position in entities['position']:
                    # Ki·ªÉm tra trong query ho·∫∑c answer
                    if any(keyword in query_lower for keyword in ['l√† ai', 'ai l√†', 'ch·ª©c v·ª•', 'l√† g√¨']):
                        confidence = 0.8
                        # üÜï NEW: Increase confidence if entities are close in text
                        person_pos = full_context.find(person.lower())
                        position_pos = full_context.find(position)
                        if abs(person_pos - position_pos) < 50:  # Within 50 chars
                            confidence += 0.1
                        relationships.append({
                            'type': 'person_position',
                            'entity1': person,
                            'entity2': position,
                            'relation': 'has_position',
                            'confidence': min(confidence, 1.0),
                            'source': 'query_answer_pair'
                        })
        
        # T√¨m m·ªëi quan h·ªá person - department
        if 'person_name' in entities and 'department' in entities:
            for person in entities['person_name']:
                for dept in entities['department']:
                    confidence = 0.7
                    # üÜï NEW: Increase confidence if entities are close
                    person_pos = full_context.find(person.lower())
                    dept_pos = full_context.find(dept)
                    if abs(person_pos - dept_pos) < 50:
                        confidence += 0.1
                    relationships.append({
                        'type': 'person_department', 
                        'entity1': person,
                        'entity2': dept,
                        'relation': 'works_at',
                        'confidence': min(confidence, 1.0),
                        'source': 'context'
                    })
        
        # üÜï NEW: Th√™m m·ªëi quan h·ªá position - department
        if 'position' in entities and 'department' in entities:
            for position in entities['position']:
                for dept in entities['department']:
                    confidence = 0.6
                    position_pos = full_context.find(position)
                    dept_pos = full_context.find(dept)
                    if abs(position_pos - dept_pos) < 50:
                        confidence += 0.15
                    relationships.append({
                        'type': 'position_department',
                        'entity1': position,
                        'entity2': dept,
                        'relation': 'in_department',
                        'confidence': min(confidence, 1.0),
                        'source': 'context'
                    })
        
        return relationships

    def _normalize_person_name(self, name):
        """üîß IMPROVED: Normalize person name to consistent format"""
        # Title case each word
        words = name.lower().split()
        normalized_words = []
        for word in words:
            if len(word) > 0:
                normalized_words.append(word[0].upper() + word[1:])
        return ' '.join(normalized_words)

    def get_context_keywords(self, entities, relationships):
        """üöÄ FIX: Generate better context keywords"""
        context_keywords = []
        
        # From entities - prioritize person names and positions
        if 'person_name' in entities:
            for entity in entities['person_name'][:2]:  # Max 2 person names
                context_keywords.append(entity)
                
        if 'position' in entities:
            for entity in entities['position'][:2]:  # üÜï IMPROVED: Max 2 positions
                context_keywords.append(entity)
        
        if 'department' in entities:
            for entity in entities['department'][:2]:  # üÜï NEW: Add departments
                context_keywords.append(entity)
        
        # From relationships
        for rel in relationships[:3]:  # Max 3 relationships
            if rel['confidence'] > 0.7:
                context_keywords.extend([rel['entity1'], rel['entity2']])
        
        # üöÄ FIX: Deduplicate and limit
        context_keywords = list(set(context_keywords))
        context_keywords = [kw for kw in context_keywords if len(kw.strip()) > 2]
        
        return context_keywords[:5]  # üÜï IMPROVED: Increase limit to 5 for better coverage