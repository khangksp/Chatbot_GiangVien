import csv
import io
import time
from datetime import datetime
from django.conf import settings
from django.utils import timezone
import logging
import pandas as pd
from django.db import transaction
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload
from google.oauth2.service_account import Credentials
from django.db.models.signals import post_save, post_delete
from .signals import qa_entry_post_save_handler, qa_entry_post_delete_handler
from collections import defaultdict

from .models import QAEntry, QASyncLog

logger = logging.getLogger(__name__)

class GoogleDriveService:
    def __init__(self):
        self.service = None
        # ƒê·ªçc to√†n b·ªô config t·ª´ settings.py
        drive_config = getattr(settings, 'GOOGLE_DRIVE', {})
        
        self.drive_id = drive_config.get('DRIVE_ID') # ID c·ªßa Shared Drive
        self.folder_id = drive_config.get('FOLDER_ID') # ID c·ªßa th∆∞ m·ª•c b√™n trong
        self.csv_filename = drive_config.get('CSV_FILENAME', 'QA.csv')
        self.service_account_file = drive_config.get('SERVICE_ACCOUNT_FILE')
        self.scopes = drive_config.get('SCOPES', ['https://www.googleapis.com/auth/drive'])
        
        self._authenticate()
        logger.info(f"üöÄ GoogleDriveService initialized. Shared Drive ID: {self.drive_id}")

    def _authenticate(self):
        try:
            if not self.service_account_file or not self.service_account_file.exists():
                logger.error(f"‚ùå Service account file not found: {self.service_account_file}")
                return False
            
            credentials = Credentials.from_service_account_file(
                str(self.service_account_file), scopes=self.scopes
            )
            self.service = build('drive', 'v3', credentials=credentials)
            logger.info("‚úÖ Google Drive authentication successful (with write permissions)")
            return True
        except Exception as e:
            logger.error(f"‚ùå Google Drive authentication failed: {str(e)}")
            self.service = None
            return False

    def _find_csv_file(self, filename=None):
        if not self.service:
            return None
        
        target_filename = filename if filename else self.csv_filename
        
        try:
            query = f"name='{target_filename}' and parents in '{self.folder_id}' and trashed=false"
            
            list_params = {
                'q': query,
                'fields': "files(id, name, modifiedTime, size)",
                'supportsAllDrives': True,
                'includeItemsFromAllDrives': True,
            }
            if self.drive_id:
                list_params['driveId'] = self.drive_id
                list_params['corpora'] = 'drive'

            results = self.service.files().list(**list_params).execute()
            files = results.get('files', [])
            
            if files:
                logger.info(f"üìÅ Found file: {files[0]['name']} (ID: {files[0]['id']})")
                return files[0]
            else:
                logger.warning(f"‚ö†Ô∏è File '{target_filename}' not found in folder '{self.folder_id}'")
                return None
        except Exception as e:
            logger.error(f"‚ùå Error finding file '{target_filename}': {str(e)}")
            return None

    def get_specific_csv_content(self, filename: str) -> str | None:
        try:
            logger.info(f"üîÑ Attempting to get content for '{filename}' from Drive...")
            file_info = self._find_csv_file(filename=filename)
            if not file_info: return None
            return self._download_csv_content(file_info['id'])
        except Exception as e:
            logger.error(f"‚ùå Error: {str(e)}")
            return None
    
    def _download_csv_content(self, file_id):
        try:
            file_content = self.service.files().get_media(fileId=file_id, supportsAllDrives=True).execute()
            return file_content.decode('utf-8')
        except Exception as e:
            logger.error(f"‚ùå Error downloading: {str(e)}")
            return None

    def _upload_csv_content(self, csv_content, file_id=None):
        try:
            media_body = MediaIoBaseUpload(io.BytesIO(csv_content.encode('utf-8')), mimetype='text/csv', resumable=True)
            if file_id:
                return self.service.files().update(fileId=file_id, media_body=media_body, supportsAllDrives=True).execute()
            else:
                meta = {'name': self.csv_filename, 'parents': [self.folder_id]}
                return self.service.files().create(body=meta, media_body=media_body, fields='id,name', supportsAllDrives=True).execute()
        except Exception as e:
            logger.error(f"‚ùå Error uploading: {str(e)}")
            return None

    def _csv_to_database_format(self, csv_content):
        try:
            if not csv_content or len(csv_content.strip()) < 10: return []
            df = pd.read_csv(io.StringIO(csv_content), dtype=str)
            if df.empty: return []
            df.columns = df.columns.str.strip()
            required = ['STT', 'question', 'answer']
            if not all(col in df.columns for col in required): return []
            df = df.fillna('')
            df = df[(df['STT'].str.strip() != '') | (df['question'].str.strip() != '')]
            entries = []
            for _, row in df.iterrows():
                entries.append({
                    'STT': str(row['STT']).strip(),
                    'question': str(row['question']).strip(),
                    'answer': str(row['answer']).strip(),
                    'category': str(row.get('category', 'Gi·∫£ng vi√™n')).strip()
                })
            return entries
        except Exception as e:
            logger.error(f"‚ùå Error parsing: {str(e)}")
            return []

    def sync_batch_entries(self, entries_list):
        try:
            entries = list(entries_list) if hasattr(entries_list, '__iter__') else [entries_list]
            count = len(entries)
            logger.info(f"üîÑ Starting batch sync for {count} entries...")

            # 1. T√¨m v√† t·∫£i file hi·ªán t·∫°i
            file_info = self._find_csv_file()
            existing_entries = []
            file_id = None

            if file_info:
                file_id = file_info['id']
                content = self._download_csv_content(file_id)
                if content:
                    existing_entries = self._csv_to_database_format(content)
            
            # 2. Backup n·∫øu d·ªØ li·ªáu b·∫•t th∆∞·ªùng (optional logic)
            if len(existing_entries) > 100 and len(existing_entries) > count * 10:
                 pass

            # 3. T·∫°o Map ƒë·ªÉ tra c·ª©u nhanh.
            # ‚ö†Ô∏è QUAN TR·ªåNG: D√πng (STT + Question) l√†m key ƒë·ªÉ ph√¢n bi·ªát c√°c c√¢u h·ªèi kh√°c nhau trong c√πng 1 STT
            entry_map = {}
            for i, item in enumerate(existing_entries):
                # T·∫°o composite key: (STT chu·∫©n h√≥a, C√¢u h·ªèi chu·∫©n h√≥a)
                key = (str(item['STT']).strip(), str(item['question']).strip())
                entry_map[key] = i
            
            merged_entries = existing_entries.copy()
            
            # 4. C·∫≠p nh·∫≠t ho·∫∑c th√™m m·ªõi
            for entry in entries:
                entry_stt = str(entry.stt).strip()
                entry_question = str(entry.question).strip()
                
                # Key ƒë·ªÉ t√¨m ki·∫øm
                key = (entry_stt, entry_question)
                
                new_data = {
                    'STT': entry_stt,
                    'question': entry_question,
                    'answer': entry.answer,
                    'category': getattr(entry, 'category', 'Gi·∫£ng vi√™n'),
                }

                if key in entry_map:
                    # ‚úÖ Case 1: Tr√πng c·∫£ STT v√† C√¢u h·ªèi -> C·∫≠p nh·∫≠t (S·ª≠a c√¢u tr·∫£ l·ªùi)
                    idx = entry_map[key]
                    merged_entries[idx] = new_data
                    logger.info(f"‚úèÔ∏è Updated existing entry: {entry_stt} - {entry_question[:20]}...")
                else:
                    # ‚úÖ Case 2: C√πng STT nh∆∞ng C√¢u h·ªèi kh√°c (ho·∫∑c STT m·ªõi) -> Th√™m m·ªõi (Append)
                    merged_entries.append(new_data)
                    # Update map lu√¥n ƒë·ªÉ n·∫øu trong batch c√≥ 2 c√¢u gi·ªëng h·ªát nhau th√¨ c√¢u sau ƒë√® c√¢u tr∆∞·ªõc
                    entry_map[key] = len(merged_entries) - 1
                    logger.info(f"‚ûï Appended new entry: {entry_stt} - {entry_question[:20]}...")
            
            # 5. T·∫°o n·ªôi dung CSV m·ªõi
            merged_csv_content = self._create_csv_from_entries(merged_entries)

            # 6. Upload l√™n Drive
            result_file = self._upload_csv_content(merged_csv_content, file_id)
            
            if result_file:
                # C·∫≠p nh·∫≠t tr·∫°ng th√°i DB
                now = timezone.now()
                QAEntry.objects.filter(pk__in=[e.pk for e in entries]).update(
                    sync_status='synced',
                    last_synced_to_drive=now
                )
                logger.info(f"‚úÖ Batch sync completed. Total entries on Drive: {len(merged_entries)}")
                return {'success': True, 'count': len(entries)}
            else:
                raise Exception("Failed to upload merged CSV content")

        except Exception as e:
            logger.error(f"‚ùå Batch sync failed: {str(e)}")
            if isinstance(entries_list, (list, tuple)) or hasattr(entries_list, 'update'):
                 QAEntry.objects.filter(pk__in=[e.pk for e in entries]).update(sync_status='error')
            return {'success': False, 'error': str(e)}

    def sync_single_entry(self, entry):
        try:
            result = self.sync_batch_entries([entry])
            return result['success']
        except: return False

    def get_drive_status(self):
        try:
            if not self.service: return {'connected': False}
            info = self._find_csv_file()
            if info: return {'connected': True, 'file_exists': True, 'file_name': info['name']}
            return {'connected': True, 'file_exists': False}
        except Exception as e: return {'connected': False, 'error': str(e)}

    def _download_and_parse(self):
        info = self._find_csv_file()
        if not info: return []
        content = self._download_csv_content(info['id'])
        return self._csv_to_database_format(content) if content else []

    def _load_fallback_csv(self):
        try:
            path = settings.BASE_DIR / 'data' / 'QA.csv'
            if path.exists():
                df = pd.read_csv(path, encoding='utf-8', dtype=str).fillna('')
                if 'category' not in df.columns: df['category'] = 'Gi·∫£ng vi√™n'
                return df.to_dict('records')
            return []
        except: return []

    def get_csv_data(self, force_refresh=False):
        try:
            info = self._find_csv_file()
            if not info: return self._load_fallback_csv()
            content = self._download_csv_content(info['id'])
            return self._csv_to_database_format(content) if content else self._load_fallback_csv()
        except: return self._load_fallback_csv()

    def _create_csv_from_entries(self, entries_dicts):
        """Helper method to create CSV content from list of dicts"""
        output = io.StringIO()
        writer = csv.writer(output)
        # Header chu·∫©n
        writer.writerow(['STT', 'question', 'answer', 'category'])
        
        for entry in entries_dicts:
            writer.writerow([
                entry.get('STT', ''), # Ch√∫ √Ω key map 'STT' vi·∫øt hoa
                entry.get('question', ''),
                entry.get('answer', ''),
                entry.get('category', 'Gi·∫£ng vi√™n')
            ])
        
        csv_content = output.getvalue()
        output.close()
        logger.info(f"üîÑ Created CSV content with {len(entries_dicts)} entries")
        return csv_content
    
    def _create_csv_from_entries(self, entries):
        out = io.StringIO()
        writer = csv.writer(out)
        writer.writerow(['STT', 'question', 'answer', 'category'])
        for e in entries:
            writer.writerow([e.get('STT',''), e.get('question',''), e.get('answer',''), e.get('category','Gi·∫£ng vi√™n')])
        return out.getvalue()
    
    def _database_to_csv_format(self, entries=None):
        if entries is None: entries = QAEntry.objects.filter(is_active=True).order_by('stt')
        return self._create_csv_from_entries([{'STT':str(e.stt), 'question':e.question, 'answer':e.answer, 'category':getattr(e,'category','Gi·∫£ng vi√™n')} for e in entries])

    def import_from_drive(self):
        sync_log = QASyncLog.objects.create(operation='import_from_drive', status='partial')
        
        # üõë 1. Ng·∫Øt t√≠n hi·ªáu ƒë·ªÉ tr√°nh b√£o log
        post_save.disconnect(qa_entry_post_save_handler, sender=QAEntry)
        post_delete.disconnect(qa_entry_post_delete_handler, sender=QAEntry)
        
        try:
            logger.info("üîÑ Importing data from Google Drive (Smart Sync Mode)...")
            drive_data = self._download_and_parse()
            
            if not drive_data:
                raise Exception("No valid data found in Drive CSV")
            db_map = defaultdict(list)
            all_db_entries = QAEntry.objects.all()
            for entry in all_db_entries:
                # Key chu·∫©n h√≥a: x√≥a kho·∫£ng tr·∫Øng th·ª´a
                key = (entry.question.strip(), entry.answer.strip())
                db_map[key].append(entry)
            
            initial_count = len(all_db_entries)
            to_create = []
            to_update = []
            reused_ids = set() # Nh·ªØng ID ƒë∆∞·ª£c gi·ªØ l·∫°i
            now = timezone.now()

            for item in drive_data:
                q_raw = str(item.get('question', '')).strip()
                a_raw = str(item.get('answer', '')).strip()
                stt_raw = str(item.get('STT', '')).strip()
                cat_raw = item.get('category', 'Gi·∫£ng vi√™n')
                
                if not q_raw or not a_raw: continue

                key = (q_raw, a_raw)
                
                if db_map[key]:
                    # ‚úÖ T√åM TH·∫§Y: T√°i s·ª≠ d·ª•ng entry c≈©
                    entry = db_map[key].pop(0) # L·∫•y ra v√† x√≥a kh·ªèi list ch·ªù ƒë·ªÉ kh√¥ng d√πng l·∫°i cho d√≤ng kh√°c
                    reused_ids.add(entry.id)
                    
                    # Ki·ªÉm tra xem c√≥ c·∫ßn update th√¥ng tin ph·ª• (STT, Category) kh√¥ng
                    if entry.stt != stt_raw or entry.category != cat_raw:
                        entry.stt = stt_raw
                        entry.category = cat_raw
                        entry.sync_status = 'synced'
                        entry.last_synced_to_drive = now
                        to_update.append(entry)
                    else:
                        # N·∫øu y h·ªát 100% th√¨ ch·ªâ c·∫ßn update timestamp sync (ho·∫∑c b·ªè qua ƒë·ªÉ t·ªëi ∆∞u)
                        pass 
                else:
                    # üÜï KH√îNG TH·∫§Y: T·∫°o m·ªõi
                    to_create.append(QAEntry(
                        stt=stt_raw,
                        question=q_raw,
                        answer=a_raw,
                        category=cat_raw,
                        sync_status='synced',
                        last_synced_to_drive=now,
                        is_active=True
                    ))
            
            # Nh·ªØng entry c√≤n s√≥t l·∫°i trong db_map l√† nh·ªØng c√°i kh√¥ng c√≥ trong Drive -> C·∫ßn X√≥a
            to_delete_ids = []
            for key, entries in db_map.items():
                for entry in entries:
                    to_delete_ids.append(entry.id)

            # --- GIAI ƒêO·∫†N 3: Th·ª±c thi Database ---
            with transaction.atomic():
                # 1. X√≥a th·ª´a
                if to_delete_ids:
                    QAEntry.objects.filter(id__in=to_delete_ids).delete()
                    logger.info(f"üóëÔ∏è Smart delete: {len(to_delete_ids)} old entries removed.")
                
                # 2. Th√™m m·ªõi
                if to_create:
                    QAEntry.objects.bulk_create(to_create, batch_size=1000)
                    logger.info(f"‚ú® Smart create: {len(to_create)} new entries added.")
                
                # 3. C·∫≠p nh·∫≠t (nh·ªØng c√°i ƒë·ªïi STT/Category)
                if to_update:
                    QAEntry.objects.bulk_update(
                        to_update, 
                        ['stt', 'category', 'sync_status', 'last_synced_to_drive'],
                        batch_size=1000
                    )
                    logger.info(f"üìù Smart update: {len(to_update)} entries metadata updated.")

            # K·∫øt qu·∫£
            final_count = len(reused_ids) + len(to_create)
            sync_log.status = 'success'
            sync_log.entries_processed = len(drive_data)
            sync_log.entries_success = final_count
            
            msg = f"ƒê·ªìng b·ªô xong: {len(to_create)} m·ªõi, {len(to_update)} c·∫≠p nh·∫≠t, {len(to_delete_ids)} x√≥a. (T·ªïng: {final_count})"
            logger.info(f"‚úÖ {msg}")
            
            return {
                'success': True, 
                'imported': len(to_create), 
                'updated': len(to_update),
                'deleted': len(to_delete_ids),
                'message': msg
            }

        except Exception as e:
            sync_log.status = 'failed'
            sync_log.error_message = str(e)
            logger.error(f"‚ùå Import failed: {str(e)}")
            return {'success': False, 'error': str(e)}
        finally:
            # üîå B·∫≠t l·∫°i t√≠n hi·ªáu
            post_save.connect(qa_entry_post_save_handler, sender=QAEntry)
            post_delete.connect(qa_entry_post_delete_handler, sender=QAEntry)
            
            sync_log.completed_at = timezone.now()
            sync_log.save()

    def export_all_to_drive(self):
        sync_log = QASyncLog.objects.create(operation='export_to_drive', status='partial')
        try:
            entries = QAEntry.objects.filter(is_active=True).order_by('stt')
            if not entries.exists():
                raise Exception('No active entries to export')
            csv_content = self._database_to_csv_format(entries)
            file_info = self._find_csv_file()
            file_id = file_info['id'] if file_info else None
            
            upload_result = self._upload_csv_content(csv_content, file_id)
            if not upload_result:
                raise Exception('Failed to upload CSV to Drive')

            updated_count = entries.update(sync_status='synced', last_synced_to_drive=timezone.now())
            sync_log.status = 'success'
            sync_log.entries_success = updated_count
            logger.info(f"‚úÖ Export completed: {updated_count} entries synced to Drive")
            return {'success': True, 'total_entries': updated_count}

        except Exception as e:
            sync_log.status = 'failed'
            sync_log.error_message = str(e)
            logger.error(f"‚ùå Export failed: {str(e)}")
            return {'success': False, 'error': str(e)}
        finally:
            sync_log.completed_at = timezone.now()
            sync_log.save()

    def backup_current_data(self):
        try:
            entries = QAEntry.objects.filter(is_active=True).order_by('stt')
            csv_content = self._database_to_csv_format(entries)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_filename = f"QA_backup_{timestamp}.csv"
            
            media_body = MediaIoBaseUpload(io.BytesIO(csv_content.encode('utf-8')), mimetype='text/csv', resumable=True)
            file_metadata = {'name': backup_filename, 'parents': [self.folder_id]}
            
            backup_file = self.service.files().create(
                body=file_metadata, 
                media_body=media_body, 
                fields='id,name',
                supportsAllDrives=True
            ).execute()
            
            logger.info(f"‚úÖ Backup created: {backup_filename}")
            return {'success': True, 'backup_filename': backup_filename}
        except Exception as e:
            logger.error(f"‚ùå Error creating backup: {str(e)}")
            return {'success': False, 'error': str(e)}

    def clear_cache(self):
        try:
            if hasattr(self, '_cached_data'):
                self._cached_data = None
            if hasattr(self, '_cache_timestamp'):
                self._cache_timestamp = 0
            logger.info("üóëÔ∏è Google Drive service cache cleared")
        except Exception as e:
            logger.error(f"‚ùå Error clearing cache: {str(e)}")
    
    def get_system_status(self):
        return {
            'service_name': 'GoogleDriveService',
            'authenticated': self.service is not None,
            'shared_drive_id': self.drive_id
        }
    
drive_service = GoogleDriveService()